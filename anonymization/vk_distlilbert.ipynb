{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú–æ–∂–µ—Ç –ø–æ–ª—É—á–∏—Ç—å—Å—è –Ω–∞–π—Ç–∏ –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —É–º–µ–µ—Ç –ª—É—á—à–µ –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å –∞–≤—Ç–æ—Ä–∞ —Ç–µ–∫—Å—Ç–∞ (0.26 —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ) | Done\n",
    "\n",
    "–•–µ—à—Ç–µ–≥–∏:\n",
    "–ë–µ—Ä–µ–º –±–µ—Ä—Ç, –≤—ã—á–∏—Å–ª—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, —É—Å—Ä–µ–¥–Ω—è–µ–º –ø–æ —Ö–µ—à—Ç–µ–≥–∞–º. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–∞—Ö–æ–¥–∏–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞. –ó–∞–º–µ–Ω—É –∏—â–µ–º –∏–∑ –±–ª–∏–∂–∞–π—à–∏—Ö –ø–æ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–º—É —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—é, –Ω–æ —Ç–∞–∫–∂–µ –Ω–µ –±–ª–∏–∑–∫–∏—Ö –ø–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—é –ª–µ–≤–µ–Ω—à—Ç–∞–π–Ω–∞.\n",
    "\n",
    "–ü–æ—Å—á–∏—Ç–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –±–µ—Ä—Ç–∞ –Ω–∞ –∑–∞–¥–∞—á–∞—Ö MLM –∏ NSP –¥–ª—è –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –∏ –∞–Ω–æ–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö.\n",
    "–ü–æ—Å—á–∏—Ç–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –∑–∞–¥–∞—á–µ –æ—Ü–µ–Ω–∫–∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏. –î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –∏–Ω—Å—Ç—ã –º–æ–∂–µ—Ç –ú–∏—à–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å, –µ—Å—Ç—å –≤ –∏–Ω–µ—Ç–µ —Ç–∞–∫–∂–µ –ø–æ —Ç–≤–∏—Ç—Ç–µ—Ä—É.\n",
    "\n",
    "–ö–∞–∫ –æ—Å—Ç–∞–≤–∏—Ç—å —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –¥—Ä—É–≥–∏—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π? –ó–∞–º–µ–Ω–∏—Ç—å –∏—Ö –≤ —Ç–µ–∫—Å—Ç–µ –Ω–∞ <MASK> –Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ—Ç–æ–º –≤—Å—Ç–∞–≤–∏—Ç—å –Ω–∞ –º–µ—Å—Ç–æ —Ç–æ–∫–µ–Ω–∞ —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "eng_stopwords = stopwords.words('english')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from gensim.models import fasttext as ft, Word2Vec\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "import fasttext\n",
    "\n",
    "from cleantext import clean\n",
    "\n",
    "from transformers import (\n",
    "    AutoModel, \n",
    "    AutoModelForMaskedLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from transformers import pipeline\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, Embedding\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint \n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import re\n",
    "from itertools import chain, islice\n",
    "import logging\n",
    "import os\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "from utils import apply_clean, get_text_and_hashtags\n",
    "\n",
    "CORES = 10\n",
    "\n",
    "SEED = 42\n",
    "TRAIN_DOC_COUNT = 10000\n",
    "TEST_DOC_COUNT = 1000\n",
    "AUTHOR_COUNT = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>post_id</th>\n",
       "      <th>owner_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-02 18:58:00</td>\n",
       "      <td>11205</td>\n",
       "      <td>84</td>\n",
       "      <td>–ö—Ä—É—Ç–µ–π—à–∏–µ –ù–µ–æ–Ω–æ–≤—ã–µ –º–∞—Å–∫–∏. –£—Å–ø–µ–π –∑–∞–∫–∞–∑–∞—Ç—å –ø–µ—Ä–≤—ã...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-02-03 20:55:00</td>\n",
       "      <td>12084</td>\n",
       "      <td>84</td>\n",
       "      <td>–†–µ–∫–æ–º–µ–Ω–¥—É—é! –û—Ç–ª–∏—á–Ω–∞—è –≤–µ—â—å. –í–∏–¥–µ–æ—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ç–æ—Ä Xi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-02-04 15:57:00</td>\n",
       "      <td>12086</td>\n",
       "      <td>84</td>\n",
       "      <td>–ö—Ä—É—Ç–µ–π—à–∏–π –Ω–∞–±–æ—Ä –¥–ª—è —Ä—É–∫–æ–¥–µ–ª–∏—è.     &lt;a href=\"/a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-02-05 11:16:00</td>\n",
       "      <td>12087</td>\n",
       "      <td>84</td>\n",
       "      <td>–ö–æ—à–µ–ª–µ–∫ –∏–≥—Ä–æ–≤–∞—è –∫–æ–Ω—Å–æ–ª—å PlayStation —Ä–µ—Ç—Ä–æ.  &lt;i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-02-06 19:17:14</td>\n",
       "      <td>12091</td>\n",
       "      <td>84</td>\n",
       "      <td>–ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—é –æ—Ç—á—ë—Ç –ø–æ –ì–æ—Å –Ω–æ–º–µ—Ä—É –∞–≤—Ç–æ–º–æ–±–∏–ª—è: –î–¢...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date  post_id  owner_id  \\\n",
       "0  2019-01-02 18:58:00    11205        84   \n",
       "1  2019-02-03 20:55:00    12084        84   \n",
       "2  2019-02-04 15:57:00    12086        84   \n",
       "3  2019-02-05 11:16:00    12087        84   \n",
       "4  2019-02-06 19:17:14    12091        84   \n",
       "\n",
       "                                                text  \n",
       "0  –ö—Ä—É—Ç–µ–π—à–∏–µ –ù–µ–æ–Ω–æ–≤—ã–µ –º–∞—Å–∫–∏. –£—Å–ø–µ–π –∑–∞–∫–∞–∑–∞—Ç—å –ø–µ—Ä–≤—ã...  \n",
       "1  –†–µ–∫–æ–º–µ–Ω–¥—É—é! –û—Ç–ª–∏—á–Ω–∞—è –≤–µ—â—å. –í–∏–¥–µ–æ—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ç–æ—Ä Xi...  \n",
       "2  –ö—Ä—É—Ç–µ–π—à–∏–π –Ω–∞–±–æ—Ä –¥–ª—è —Ä—É–∫–æ–¥–µ–ª–∏—è.     <a href=\"/a...  \n",
       "3  –ö–æ—à–µ–ª–µ–∫ –∏–≥—Ä–æ–≤–∞—è –∫–æ–Ω—Å–æ–ª—å PlayStation —Ä–µ—Ç—Ä–æ.  <i...  \n",
       "4  –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—é –æ—Ç—á—ë—Ç –ø–æ –ì–æ—Å –Ω–æ–º–µ—Ä—É –∞–≤—Ç–æ–º–æ–±–∏–ª—è: –î–¢...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_df = pd.read_csv('/mnt/ess_storage/DN_1/storage/home/vpanov/vk/vk_spb_posts_2019.csv')\n",
    "posts_df = posts_df.dropna(axis=0)\n",
    "posts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "482067"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(posts_df['owner_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227620       1175\n",
       "371727932    1174\n",
       "181072430    1172\n",
       "1378978      1171\n",
       "36010941     1170\n",
       "             ... \n",
       "309131878    1031\n",
       "478126947    1030\n",
       "176091683    1026\n",
       "289137251    1026\n",
       "393151306    1024\n",
       "Name: owner_id, Length: 100, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offset = 500\n",
    "posts_df['owner_id'].value_counts()[offset:offset+AUTHOR_COUNT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>post_id</th>\n",
       "      <th>owner_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>989187</th>\n",
       "      <td>2019-10-28 00:00:00</td>\n",
       "      <td>49664</td>\n",
       "      <td>134312775</td>\n",
       "      <td>–¢—ã –≥–æ–≤–æ—Ä–∏—à—å –ª–µ—Ç–µ—Ç—å –∑–∞ –º—ë–¥–æ–º \\n–≤ –¥–∞–ª—ë–∫–∏–π –∫–æ—Å–º–æ—Å...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1287802</th>\n",
       "      <td>2019-03-23 00:00:00</td>\n",
       "      <td>26829</td>\n",
       "      <td>161384085</td>\n",
       "      <td>–°–º–æ—Ç—Ä–∏—Ç–µ \"–ù–∞—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∑–∞ —à–∞–≥ –æ—Ç —Å–º–µ—Ä—Ç–∏! –°–æ—Ö—Ä–∞–Ω...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8066156</th>\n",
       "      <td>2019-04-13 05:31:00</td>\n",
       "      <td>4000</td>\n",
       "      <td>106327572</td>\n",
       "      <td>–Ø –Ω–∞–±—Ä–∞–ª 141800 –æ—á–∫–æ–≤ –Ω–∞ 2329 —É—Ä–æ–≤–Ω–µ. –ü—Ä–∏—Å–æ–µ–¥–∏...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1777838</th>\n",
       "      <td>2019-03-23 00:00:00</td>\n",
       "      <td>31216</td>\n",
       "      <td>338518394</td>\n",
       "      <td>–ü–æ—Å–ª–µ —ç—Ç–æ–≥–æ –†–µ—Ü–µ–ø—Ç–∞ –í—ã –ü–æ–ª—é–±–∏—Ç–µ –ü–µ—á–µ–Ω—å. –ö—É—Ä–∏–Ω–∞...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502411</th>\n",
       "      <td>2019-09-09 00:00:00</td>\n",
       "      <td>5862</td>\n",
       "      <td>36010941</td>\n",
       "      <td>–†–∞–∫–µ—Ç–∞ –ù–û–õ–¨ 2609 –°–°–°–†\\nVintage soviet USSR wat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        date  post_id   owner_id  \\\n",
       "989187   2019-10-28 00:00:00    49664  134312775   \n",
       "1287802  2019-03-23 00:00:00    26829  161384085   \n",
       "8066156  2019-04-13 05:31:00     4000  106327572   \n",
       "1777838  2019-03-23 00:00:00    31216  338518394   \n",
       "502411   2019-09-09 00:00:00     5862   36010941   \n",
       "\n",
       "                                                      text  \n",
       "989187   –¢—ã –≥–æ–≤–æ—Ä–∏—à—å –ª–µ—Ç–µ—Ç—å –∑–∞ –º—ë–¥–æ–º \\n–≤ –¥–∞–ª—ë–∫–∏–π –∫–æ—Å–º–æ—Å...  \n",
       "1287802  –°–º–æ—Ç—Ä–∏—Ç–µ \"–ù–∞—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∑–∞ —à–∞–≥ –æ—Ç —Å–º–µ—Ä—Ç–∏! –°–æ—Ö—Ä–∞–Ω...  \n",
       "8066156  –Ø –Ω–∞–±—Ä–∞–ª 141800 –æ—á–∫–æ–≤ –Ω–∞ 2329 —É—Ä–æ–≤–Ω–µ. –ü—Ä–∏—Å–æ–µ–¥–∏...  \n",
       "1777838  –ü–æ—Å–ª–µ —ç—Ç–æ–≥–æ –†–µ—Ü–µ–ø—Ç–∞ –í—ã –ü–æ–ª—é–±–∏—Ç–µ –ü–µ—á–µ–Ω—å. –ö—É—Ä–∏–Ω–∞...  \n",
       "502411   –†–∞–∫–µ—Ç–∞ –ù–û–õ–¨ 2609 –°–°–°–†\\nVintage soviet USSR wat...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_productive_authors = posts_df['owner_id'].value_counts()[offset:offset+AUTHOR_COUNT].index.values\n",
    "posts_authors_df = posts_df[posts_df.owner_id.isin(most_productive_authors)].sample(frac=1.0, random_state=SEED)\n",
    "posts_authors_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_authors_df[['only_text', 'hashtags']] = posts_authors_df.apply(get_text_and_hashtags, axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_authors_df['text_clean'] = posts_authors_df.only_text.apply(lambda x: apply_clean(x, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_space(text):\n",
    "    return re.sub(r'([,.!\\'\\\"@<>\\\\\\[\\]\\(\\)#\\$%^&\\*;:])', ' \\1', text)\n",
    "\n",
    "posts_authors_df['text2'] = posts_authors_df['text_clean'].apply(input_space)\n",
    "\n",
    "count_threshold = 5\n",
    "\n",
    "v = posts_authors_df['text2'].str.split().tolist()\n",
    "c = Counter(chain.from_iterable(v))\n",
    "posts_authors_df['text_clean'] = [' '.join([j for j in i if c[j] > count_threshold]) for i in v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_authors = 50\n",
    "least_long = 50\n",
    "max_posts_per_user = 800\n",
    "\n",
    "long_posts = posts_authors_df[posts_authors_df.text_clean.str.len() > least_long]\n",
    "authors_posts_count = long_posts.owner_id.value_counts()\n",
    "authors = authors_posts_count[:max_authors].index.tolist()\n",
    "min_posts = authors_posts_count.values[max_authors - 1]\n",
    "median_posts = int(authors_posts_count.median())\n",
    "\n",
    "train_posts = []\n",
    "test_posts = []\n",
    "\n",
    "for i in authors:\n",
    "    author_i_posts = posts_authors_df[(posts_authors_df.text_clean.str.len() > least_long) & (posts_authors_df.owner_id == i)]\n",
    "    l = len(author_i_posts)\n",
    "    train_posts.append(author_i_posts[:min(max_posts_per_user, l - 20)])\n",
    "    test_posts.append(author_i_posts[l - 20:])\n",
    "\n",
    "train_posts = pd.concat(train_posts).sample(frac=1.0, random_state=SEED)\n",
    "test_posts = pd.concat(test_posts).sample(frac=1.0, random_state=SEED)\n",
    "\n",
    "train_posts.to_csv('/home/jovyan/notebooks/vk/train_posts.csv', index=None)\n",
    "test_posts.to_csv('/home/jovyan/notebooks/vk/test_posts.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_posts = pd.read_csv('/home/jovyan/notebooks/vk/train_posts.csv')\n",
    "test_posts = pd.read_csv('/home/jovyan/notebooks/vk/test_posts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text anonymization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SynTF method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_SYNSETS = False\n",
    "TEXT_LENGTH = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3620\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3621\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3622\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-09737ae7c995>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moriginal_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnyc_posts_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnyc_posts_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3503\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3504\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3505\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3506\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3507\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3621\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3622\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3623\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3624\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3625\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "original_docs = nyc_posts_df.loc[nyc_posts_df['text'].str.len() > 100, 'text'][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'@\\w+', ' ', text.lower())\n",
    "    text = re.sub(r'[^a-zA-Z0-9]', ' ', text)\n",
    "    words = text.split()\n",
    "    words = filter(lambda x: x not in eng_stopwords, words)\n",
    "    return ' '.join(lemmatizer.lemmatize(x) for x in words)\n",
    "\n",
    "def get_synonyms(uniq_words):\n",
    "    all_synonyms = set()\n",
    "    for word in uniq_words:\n",
    "        synonyms = wordnet.synsets(word)\n",
    "        all_synonyms.update(chain.from_iterable([word.lemma_names() for word in synonyms]))\n",
    "    return all_synonyms\n",
    "\n",
    "docs = original_docs.apply(preprocess).tolist()\n",
    "if USE_SYNSETS:\n",
    "    uniq_words = set(chain.from_iterable([doc.split() for doc in docs]))\n",
    "    synonyms = ' '.join(get_synonyms(uniq_words))\n",
    "    docs_with_synonyms = [*docs, synonyms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "if USE_SYNSETS:\n",
    "    tfidf.fit(docs_with_synonyms)\n",
    "else:\n",
    "    tfidf.fit(docs)\n",
    "doc_vecs = tfidf.transform(docs)\n",
    "doc_vecs = normalize(doc_vecs, norm='l1')\n",
    "words = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7458"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = fasttext.load_facebook_model(datapath('/mnt/ess_storage/DN_1/storage/home/vpanov/cc.en.300.bin.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7458, 7458)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vecs = [ft.wv[word] for word in words]\n",
    "word_similarities = cosine_similarity(word_vecs, word_vecs)\n",
    "word_similarities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kgram_overlap(word1, word2, k):\n",
    "    a = set([word1[i:i+k] for i in range(0, len(word1) - k + 1)])\n",
    "    b = set([word2[i:i+k] for i in range(0, len(word2) - k + 1)])\n",
    "    inter = len(a.intersection(b))\n",
    "    return inter / (len(a) + len(b) - inter)\n",
    "\n",
    "def score(word1, word2):\n",
    "    idx1, idx2 = tfidf.vocabulary_[word1], tfidf.vocabulary_[word2]\n",
    "    return word_similarities[idx1, idx2] - 0.3 * kgram_overlap(word1, word2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://programming-dp.com/notebooks/ch9.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unforgettable ['unedited']\n"
     ]
    }
   ],
   "source": [
    "def exponential_gen(x, R, u, sensitivity=1, epsilon=25.4):\n",
    "    # Calculate the score for each element of R\n",
    "    scores = [u(x, r) for r in R]\n",
    "    \n",
    "    # Calculate the probability for each element, based on its score\n",
    "    probabilities = [np.exp(epsilon * score / (2 * sensitivity)) for score in scores]\n",
    "    \n",
    "    # Normalize the probabilties so they sum to 1\n",
    "    probabilities = probabilities / np.linalg.norm(probabilities, ord=1)\n",
    "\n",
    "    # Choose an element from R based on the probabilities\n",
    "    return np.random.choice(R, 1, p=probabilities)\n",
    "\n",
    "num = 7000\n",
    "print(words[num], exponential_gen(words[num], words, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3342ecb536a0454d9452dbfbf1e3cd3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=7458.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7458, 7458)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def exponential(x, R, u, sensitivity=1, epsilon=25.4):\n",
    "    # Calculate the score for each element of R\n",
    "    scores = [u(x, r) for r in R]\n",
    "    \n",
    "    # Calculate the probability for each element, based on its score\n",
    "    probabilities = [np.exp(epsilon * score / (2 * sensitivity)) for score in scores]\n",
    "    \n",
    "    # Normalize the probabilties so they sum to 1\n",
    "    probabilities = probabilities / np.linalg.norm(probabilities, ord=1)\n",
    "    \n",
    "    return probabilities\n",
    "\n",
    "word_replace_probs = []\n",
    "\n",
    "for word in tqdm(words):\n",
    "    word_replace_probs.append(exponential(word, words, score))\n",
    "\n",
    "word_replace_probs = np.array(word_replace_probs)\n",
    "word_replace_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--ORIGINAL--\n",
      "Right or Left?\n",
      "\n",
      "Buffalo Chicken slice on the right and Chicken, Bacon(beef), Ranch on the left from @saucny, all new halal pizza spot in New Hyde Park.\n",
      "--PREPROCESSED--\n",
      "right left buffalo chicken slice right chicken bacon beef ranch left new halal pizza spot new hyde park\n",
      "--GENERATED SEQUENCE (WITHOUT WORD ORDER)--\n",
      "freshwater bend fish horn lyndhurst cabbage placed rice priestley proof goat rippon bacon inside pork nearby spot though\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "--ORIGINAL--\n",
      "Poor Cyndy!  This superstar massage therapist had to endure an hour of Amy's special playlist with The Sky treatment to break a 4 day cuckoo migraine.  Wow, this treatment works!  But, Cyndy will never ever want to listen to Pearl Jam, DMB or REM again.\n",
      "--PREPROCESSED--\n",
      "poor cyndy superstar massage therapist endure hour amy special playlist sky treatment break 4 day cuckoo migraine wow treatment work cyndy never ever want listen pearl jam dmb rem\n",
      "--GENERATED SEQUENCE (WITHOUT WORD ORDER)--\n",
      "spray elizabeth falling karen occasion atlanta visited soft revamped heidi superstar else crush tech scott blue elizabeth nice folklore kitt edwin programing tmn defiantly waking rosie drummer twelve product\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "--ORIGINAL--\n",
      "he hit the booty like a drum, yumyum.üòÄ why he wanna stutter and he know he wanna butter my buns and have all of my sons. got them young and old like yang hyun suk and bang yong guk and my baby knows how to cook, red suit,  korean look.  startin all them trends in the mercedes benz playin that kpop, that jrock, i'm playin with that cüêàüêìk he likes a chick that plays wu tang that's how we bang, loves to talk slang knows how to freestyle knows how to get buckwild. he's a little bit of bruce lee doin karate, he a hottie he pulls out the shottie and we keep it sexual, also intellectual they askin what the dragon  do! üêâüî•üî•üî•                   üçéüçè          üòò‚õ≤üòúüí±üì≥üåÉüíöüí´\n",
      "--PREPROCESSED--\n",
      "hit booty like drum yumyum wanna stutter know wanna butter bun son got young old like yang hyun suk bang yong guk baby know cook red suit korean look startin trend mercedes benz playin kpop jrock playin c k like chick play wu tang bang love talk slang know freestyle know get buckwild little bit bruce lee doin karate hottie pull shottie keep sexual also intellectual askin dragon\n",
      "--GENERATED SEQUENCE (WITHOUT WORD ORDER)--\n",
      "jen strangely booty dj bruce pulling ndido guk intellectual infant whine garlic alright ssi hackensack slang let nicole heavy blvd heat samantha stutter look giant husband jummah ure bagel everywhere bjj echt enrichment second mater charming hell trans rts adversity push uch sobre new chunky true zou pointed verlo streammiter rhd conozcas sportin girl nenhum farm ah medical seriously tof little firsties sky naomi suk cheryl bit key\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "--ORIGINAL--\n",
      "First cocktail of the day ‚Äî Three Hearts ‚Äî featuring Kansas distilled spirit. And our bartender just finished writing her first play.\n",
      "--PREPROCESSED--\n",
      "first cocktail day three heart featuring kansa distilled spirit bartender finished writing first play\n",
      "--GENERATED SEQUENCE (WITHOUT WORD ORDER)--\n",
      "twice fourth summer timing kansa finishing allowing second seven includes snack clase unfinished named\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "--ORIGINAL--\n",
      "Just a week away we are welcoming all the youth from ages 14-18 !!! Let us start this year with God !\n",
      "--PREPROCESSED--\n",
      "week away welcoming youth age 14 18 let u start year god\n",
      "--GENERATED SEQUENCE (WITHOUT WORD ORDER)--\n",
      "34 23 welcoming god joyous hating pursuing adult month 30 true announcing\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for idx, doc in enumerate(docs[:5]):\n",
    "    print('--ORIGINAL--')\n",
    "    print(original_docs[original_docs.index[idx]])\n",
    "    print('--PREPROCESSED--')\n",
    "    print(doc)\n",
    "    print('--GENERATED SEQUENCE (WITHOUT WORD ORDER)--')\n",
    "    words_count = len(doc.split())\n",
    "    words_ = np.random.choice(words, words_count, p=doc_vecs[idx].todense().tolist()[0])\n",
    "    for i in range(words_count):\n",
    "        word_idx = tfidf.vocabulary_[words_[i]]\n",
    "        words_[i] = np.random.choice(words, 1, p=word_replace_probs[word_idx])[0]\n",
    "    print(' '.join(words_))\n",
    "    print('-'*150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ER-AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-multilingual-cased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = 'distilbert-base-multilingual-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "bert = AutoModel.from_pretrained(model_name, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 146, 11850, 24109, 15703, 119, 12689, 72894, 11268, 119, 102] [CLS] I like pigs. And apples. [SEP]\n",
      "[101, 177, 11850, 24109, 15703, 119, 10111, 72894, 11268, 119, 102] [CLS] i like pigs. and apples. [SEP]\n",
      "[101, 177, 11850, 24109, 15703, 119, 10111, 72894, 11268, 119, 102] [CLS] i like pigs. and apples. [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode('I like pigs. And apples.'), tokenizer.decode(tokenizer.encode('I like pigs. And apples.')))\n",
    "print(tokenizer.encode('i like pigs. and apples.'), tokenizer.decode(tokenizer.encode('i like pigs. and apples.')))\n",
    "print(tokenizer.encode('i like pigs . and apples .'), tokenizer.decode(tokenizer.encode('i like pigs . and apples .')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(doc):\n",
    "    doc = tokenizer.decode(tokenizer.encode(doc, max_length=128, padding='max_length', truncation=True))\n",
    "#     doc = tokenizer.decode(tokenizer.encode(doc))\n",
    "    doc = re.sub(r'([\\.,\\'‚Äô\\\"\\-!\\?\\(\\)])', r' \\1 ', doc)\n",
    "    doc = re.sub('\\s', ' ', doc)\n",
    "    return doc.split()\n",
    "\n",
    "unique_tokens = set(chain.from_iterable([*train_posts.text_clean.apply(get_words).tolist(),\n",
    "                                       *test_posts.text_clean.apply(get_words).tolist()]))\n",
    "\n",
    "token2idx = {token: idx for idx, token in enumerate(unique_tokens)}\n",
    "idx2token = {idx: token for idx, token in enumerate(unique_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_TOKEN_ID = token2idx['[CLS]']\n",
    "EOS_TOKEN_ID = token2idx['[SEP]']\n",
    "PAD_TOKEN_ID = token2idx['[PAD]']\n",
    "\n",
    "VOCAB_SIZE = len(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "del unique_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44463"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_idx(sent: str, word: str):\n",
    "    return sent.split(\" \").index(word)\n",
    "\n",
    "def get_hidden_states(encoded, token_ids_words, model, layers):\n",
    "    \"\"\"Push input IDs through model. Stack and sum `layers` (last four by default).\n",
    "        Select only those subword token outputs that belong to our word of interest\n",
    "        and average them.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded)\n",
    " \n",
    "    # Get all hidden states\n",
    "    states = output.hidden_states\n",
    "    # Stack and sum all requested layers\n",
    "    output = torch.stack([states[i] for i in layers]).sum(0).squeeze()\n",
    "    \n",
    "    res = []\n",
    "    labels_count = []\n",
    "    \n",
    "    for idx, (outp, label) in enumerate(zip(output, token_ids_words)):\n",
    "        if label is None or token_ids_words[idx - 1] is None or token_ids_words[idx - 1] != token_ids_words[idx]:\n",
    "            res.append(outp)\n",
    "            labels_count.append(1)\n",
    "        else: \n",
    "            res[-1] += outp\n",
    "            labels_count[-1] += 1\n",
    "    \n",
    "    res = torch.vstack(res)\n",
    "    res = res / torch.tensor(labels_count).float().unsqueeze(1)\n",
    " \n",
    "    return res\n",
    "\n",
    "\n",
    "def get_word_vectors(sent, tokenizer, model, layers):\n",
    "    \"\"\"Get a word vector by first tokenizing the input sentence, getting all token idxs\n",
    "        that make up the word of interest, and then `get_hidden_states`.\"\"\"\n",
    "    encoded = tokenizer.encode_plus(sent, return_tensors=\"pt\", max_length=128, padding='max_length', truncation=True)\n",
    "    return get_hidden_states(encoded, encoded.word_ids(), model, layers)\n",
    "\n",
    "\n",
    "def exmpl(layers=None):\n",
    "    # Use last four layers by default\n",
    "    layers = [-4, -3, -2, -1] if layers is None else layers\n",
    "\n",
    "    sent = train_posts.text_clean[train_posts.index[23]]\n",
    "\n",
    "    word_embedding = get_word_vectors(sent, tokenizer, bert, layers)\n",
    "\n",
    "    return word_embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([121, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exmpl().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vectors(sent, tokenizer, model, layers):\n",
    "    \"\"\"Get a word vector by first tokenizing the input sentence, getting all token idxs\n",
    "        that make up the word of interest, and then `get_hidden_states`.\"\"\"\n",
    "    \n",
    "    encoded = tokenizer.encode_plus(sent, return_tensors=\"pt\", max_length=128, padding='max_length', truncation=True)\n",
    "    \n",
    "    input_ids = list(map(lambda x: token2idx[x], get_words(sent)))\n",
    "    input_ids = torch.tensor(input_ids).unsqueeze(0)\n",
    "    return get_hidden_states(encoded, encoded.word_ids(), model, layers).cpu(), input_ids\n",
    "\n",
    "def get_embedding(doc, model=bert):\n",
    "    \"Get embedding for each word\"\n",
    "    layers = [-4, -3, -2, -1]\n",
    "    return get_word_vectors(doc, tokenizer, model, layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "input_ids = []\n",
    "\n",
    "token_embeddings = torch.zeros((VOCAB_SIZE, 768))\n",
    "token_count = torch.zeros((VOCAB_SIZE,))\n",
    "\n",
    "for idx, doc in enumerate(train_posts.text_clean):\n",
    "#     try:\n",
    "    embedding, ids = get_embedding(doc)\n",
    "    embeddings.append(embedding)\n",
    "    input_ids.append(ids)\n",
    "    token_embeddings.scatter_add_(0, ids[0].unsqueeze(1).expand(embedding.shape), embedding)\n",
    "    token_count.scatter_add_(0, ids[0], torch.ones_like(ids[0], dtype=torch.float))\n",
    "#     except Exception as e:\n",
    "#         print(idx)\n",
    "#         print(get_words(doc))\n",
    "#         print(e)\n",
    "\n",
    "token_embeddings = torch.nan_to_num(torch.div(token_embeddings, token_count.unsqueeze(1).expand(token_embeddings.shape)))\n",
    "F.normalize(token_embeddings, dim=-1, out=token_embeddings)\n",
    "del token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([44463, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del token_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n"
     ]
    }
   ],
   "source": [
    "token_similarities = torch.zeros((VOCAB_SIZE, VOCAB_SIZE), dtype=torch.float16)\n",
    "\n",
    "batch_size = 100000\n",
    "for i in range(VOCAB_SIZE):\n",
    "    if i % 10000 == 0:\n",
    "        print(i)\n",
    "        \n",
    "    vals = torch.matmul(token_embeddings[i].cuda(), token_embeddings[0:i+1].T.cuda()).cpu()\n",
    "    torch.clip(vals, max=0.85, out=vals)\n",
    "    vals = vals.to(torch.float16)\n",
    "    token_similarities[i, 0:i+1] = vals\n",
    "    token_similarities[0:i+1, i] = vals\n",
    "\n",
    "def get_token_sim(i, j, token_similarities=token_similarities):\n",
    "    return token_similarities[i, j].to(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.utilities.memory.garbage_collection_cuda()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('checkpoints_vk_distilbert/log.txt', 'a', encoding='utf-8') as f:\n",
    "    print('token_similarities ok', file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.clip(token_similarities, max=0.85, out=token_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([44463, 44463])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_similarities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NYDataset(Dataset):\n",
    "    def __init__(self, embeddings, input_ids):\n",
    "        self.embeddings = embeddings\n",
    "        self.input_ids = list(map(lambda x: x.squeeze(), input_ids))\n",
    "        \n",
    "        self.embeddings = nn.utils.rnn.pad_sequence(self.embeddings, batch_first=True, padding_value=0)\n",
    "        self.input_ids = nn.utils.rnn.pad_sequence(self.input_ids, batch_first=True, padding_value=PAD_TOKEN_ID)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.input_ids[idx].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 44463])\n"
     ]
    }
   ],
   "source": [
    "gen = torch.randn(128, 64, VOCAB_SIZE)\n",
    "print(gen[:, 0, :].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(900735.2500)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = F.log_softmax(torch.randn(128, 64, VOCAB_SIZE), -1)\n",
    "orig = torch.randint(0, VOCAB_SIZE, (64, 128))\n",
    "\n",
    "gen_doc = gen[:, 0, :]\n",
    "orig_doc = orig[0, :]\n",
    "\n",
    "def doc_embed_loss(gen_doc, orig_doc, k=5):\n",
    "    topk_values, topk_indices = torch.topk(gen_doc, k, dim=-1)\n",
    "    rand_indices = torch.randint(0, VOCAB_SIZE, (gen_doc.shape[0], k))\n",
    "    doc_loss = 0\n",
    "    for i in range(gen_doc.shape[0]):\n",
    "        rand_values = torch.index_select(gen_doc, -1, rand_indices[i])\n",
    "        doc_loss += (topk_values * get_token_sim(orig_doc[i].item(), topk_indices[i])).sum()\n",
    "        doc_loss += (rand_values * get_token_sim(orig_doc[i].item(), rand_indices[i])).sum()\n",
    "    \n",
    "    return -doc_loss\n",
    "\n",
    "doc_embed_loss(gen_doc, orig_doc)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5218"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_doc[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.2026)\n",
      "tensor(58259076.)\n",
      "tensor(29129340.)\n"
     ]
    }
   ],
   "source": [
    "gen = torch.randn(64, 128, VOCAB_SIZE)\n",
    "orig = torch.randint(0, VOCAB_SIZE, (64, 1, 128))\n",
    "\n",
    "# LOSS FUNCTIONS\n",
    "\n",
    "def recon_loss(inp, targ):\n",
    "    \"Loss of first stage\"\n",
    "    return F.cross_entropy(inp.view(-1, VOCAB_SIZE), targ.reshape(-1)) # forgot set ignore_index as PAD_TOKEN_ID\n",
    "\n",
    "def doc_embed_loss(gen_doc, orig_doc, k=5):\n",
    "    topk_values, topk_indices = torch.topk(gen_doc, k, dim=-1)\n",
    "    rand_indices = torch.randint(0, VOCAB_SIZE, (gen_doc.shape[0], k))\n",
    "    doc_loss = 0\n",
    "    \n",
    "    for i in range(gen_doc.shape[0]):\n",
    "        rand_values = torch.index_select(gen_doc, -1, rand_indices[i])\n",
    "        doc_loss += (topk_values * get_token_sim(orig_doc[i].item(), topk_indices[i])).sum()\n",
    "        doc_loss += (rand_values * get_token_sim(orig_doc[i].item(), rand_indices[i])).sum()\n",
    "\n",
    "    return doc_loss.to(torch.float)\n",
    "\n",
    "def embed_loss(inp, targ, k=5):\n",
    "    loss = 0\n",
    "    inp = F.log_softmax(inp, dim=-1)\n",
    "    for i in range(inp.shape[0]):\n",
    "        loss += doc_embed_loss(inp[i], targ[i][0], k=k)\n",
    "    return -loss\n",
    "\n",
    "def total_loss(inp, targ, alpha=1, beta=0.5, k=5):\n",
    "    \"Loss of second stage\"\n",
    "    return alpha * recon_loss(inp, targ) + beta * embed_loss(inp, targ, k)\n",
    "\n",
    "print(recon_loss(gen, orig))\n",
    "print(embed_loss(gen, orig))\n",
    "print(total_loss(gen, orig))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch-lightning.readthedocs.io/en/latest/notebooks/lightning_examples/datamodules.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(samples):\n",
    "    x = [sample[0] for sample in samples]\n",
    "    y = [sample[1].squeeze() for sample in samples]\n",
    "    \n",
    "    x = nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value=0.0)\n",
    "    y = nn.utils.rnn.pad_sequence(y, batch_first=True, padding_value=PAD_TOKEN_ID)\n",
    "    \n",
    "    if x.shape[1] < y.shape[1]:\n",
    "        y = y[:, :x.shape[1]]\n",
    "    \n",
    "#     print(x.shape, y.shape)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "class LitERAE(pl.LightningModule):\n",
    "    def __init__(self, data, emb_size=768, hidden_size=512, num_layers=2, act_type=None, learning_rate=1e-3, batch_size=64):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # We hardcode dataset specific stuff here.\n",
    "        self.data = data\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Build model\n",
    "        self.gru_1 = nn.GRU(emb_size, hidden_size, num_layers, bidirectional=True, batch_first=True)\n",
    "        if act_type == None:\n",
    "            self.act_1 = nn.Identity()\n",
    "        if act_type == 'ReLU':\n",
    "            self.act_1 = nn.ReLU()\n",
    "        self.linear_1 = nn.Linear(hidden_size * 2, emb_size)\n",
    "        self.gru_2 = nn.GRU(emb_size, hidden_size, num_layers, bidirectional=True, batch_first=True)\n",
    "        self.linear_2 = nn.Linear(hidden_size * 2, VOCAB_SIZE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, hidden = self.gru_1(x)\n",
    "        x = self.act_1(x)\n",
    "        x = self.linear_1(x)\n",
    "        x, _ = self.gru_2(x, hidden)\n",
    "        x = self.act_1(x)\n",
    "        x = self.linear_2(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_func(logits, y)\n",
    "        \n",
    "        self.log(f'train_loss', loss)\n",
    "        self.log(f'avg_train_loss', loss, on_step=False, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_func(logits, y)\n",
    "        \n",
    "        self.log(f'val_loss', loss)\n",
    "        self.log(f'avg_val_loss', loss, on_step=False, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        lr_scheduler = {\"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2, verbose=True), \"monitor\": \"avg_val_loss\"}\n",
    "        return {'optimizer': optimizer, 'lr_shceduler': lr_scheduler}\n",
    "\n",
    "    def on_train_batch_end(self, outputs, batch, batch_idx):\n",
    "        metrics = self.trainer.callback_metrics\n",
    "#         logger.info(f'Batch train loss {metrics}')\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        metrics = self.trainer.callback_metrics\n",
    "        print(f'Train loss: {metrics[\"avg_train_loss\"]}')\n",
    "\n",
    "    def on_validation_batch_end(self, outputs, batch, batch_idx):\n",
    "        metrics = self.trainer.callback_metrics\n",
    "#         logger.info(f'Batch validation loss {metrics}')\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        metrics = self.trainer.callback_metrics\n",
    "        print(f'Val loss: {metrics[\"avg_val_loss\"]}')\n",
    "\n",
    "    ####################\n",
    "    # DATA RELATED HOOKS\n",
    "    ####################\n",
    "\n",
    "#     def prepare_data(self):\n",
    "#         self.data = nn.utils.rnn.pad_sequence(self.data)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            train_size = int(0.9 * len(self.data))\n",
    "            val_size = len(self.data) - train_size\n",
    "            self.data_train, self.data_val = random_split(self.data, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "            self.loss_func = recon_loss\n",
    "        \n",
    "        if stage == 'fit_2':\n",
    "            train_size = int(0.9 * self.data.shape[1])\n",
    "            val_size = self.data.shape[1] - train_size\n",
    "            self.data_train, self.data_val = random_split(self.data, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "            self.loss_func = total_loss\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "#         if stage == \"test\" or stage is None:\n",
    "#             self.data_test\n",
    "#             self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.data_train, batch_size=self.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.data_val, batch_size=self.batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "#     def test_dataloader(self):\n",
    "#         return DataLoader(self.mnist_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "chechpoint_path = \"checkpoints_vk_distilbert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "params = {\n",
    "    'hidden_size': 700,\n",
    "    'num_layers': 2,\n",
    "    'learning_rate': 1e-3,\n",
    "    'act_type': 'ReLU'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type   | Params\n",
      "------------------------------------\n",
      "0 | gru_1    | GRU    | 15.0 M\n",
      "1 | act_1    | ReLU   | 0     \n",
      "2 | linear_1 | Linear | 1.1 M \n",
      "3 | gru_2    | GRU    | 15.0 M\n",
      "4 | linear_2 | Linear | 62.3 M\n",
      "------------------------------------\n",
      "93.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "93.4 M    Total params\n",
      "373.494   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5658c18ebab426a8cf6fdf0b6c70e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 1.845604658126831\n",
      "Train loss: 2.1288845539093018\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 1.2394801378250122\n",
      "Train loss: 1.517362356185913\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50313128abbb43afb7eeb1f4b7c32888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5857846736907959\n",
      "Train loss: 0.5560625791549683\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.499859094619751\n",
      "Train loss: 0.36761829257011414\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.45253172516822815\n",
      "Train loss: 0.2541213631629944\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.4258239269256592\n",
      "Train loss: 0.17898763716220856\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.4147576093673706\n",
      "Train loss: 0.12884068489074707\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.3999043405056\n",
      "Train loss: 0.09576758742332458\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.4012964367866516\n",
      "Train loss: 0.07497640699148178\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.4011717438697815\n",
      "Train loss: 0.06294993311166763\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.4036440849304199\n",
      "Train loss: 0.05500193312764168\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.4000810384750366\n",
      "Train loss: 0.049020130187273026\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.4064443111419678\n",
      "Train loss: 0.04413075000047684\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=15` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.4098970592021942\n",
      "Train loss: 0.042632874101400375\n"
     ]
    }
   ],
   "source": [
    "data = NYDataset(embeddings, input_ids)\n",
    "model = LitERAE(data, batch_size=BATCH_SIZE, **params)\n",
    "model.train()\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(dirpath=chechpoint_path, save_top_k=2, monitor=\"val_loss\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=15,\n",
    "    num_nodes=1,\n",
    "    num_sanity_val_steps=0,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = checkpoint_callback.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{chechpoint_path}/best1.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data\n",
    "del model\n",
    "del trainer\n",
    "pl.utilities.memory.garbage_collection_cuda()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun 14 14:37:14 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.64.00    Driver Version: 440.64.00    CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:5E:00.0 Off |                    0 |\n",
      "| N/A   64C    P0    38W /  70W |   3078MiB / 15109MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = NYDataset(embeddings, input_ids)\n",
    "model = LitERAE.load_from_checkpoint(checkpoint_path=best_model_path, data=data, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type   | Params\n",
      "------------------------------------\n",
      "0 | gru_1    | GRU    | 15.0 M\n",
      "1 | act_1    | ReLU   | 0     \n",
      "2 | linear_1 | Linear | 1.1 M \n",
      "3 | gru_2    | GRU    | 15.0 M\n",
      "4 | linear_2 | Linear | 62.3 M\n",
      "------------------------------------\n",
      "93.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "93.4 M    Total params\n",
      "373.494   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "087e6fa59dc94804a26729118ec4f5ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.369366854429245\n",
      "Train loss: 0.06973734498023987\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.37771719694137573\n",
      "Train loss: 0.042418014258146286\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.3689057528972626\n",
      "Train loss: 0.03361129015684128\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.3715161681175232\n",
      "Train loss: 0.028621839359402657\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.37552621960639954\n",
      "Train loss: 0.025716310366988182\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.3814913332462311\n",
      "Train loss: 0.023617105558514595\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.3801876902580261\n",
      "Train loss: 0.021150799468159676\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.3817344307899475\n",
      "Train loss: 0.020335646346211433\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.38352853059768677\n",
      "Train loss: 0.01897362992167473\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.3885502815246582\n",
      "Train loss: 0.017361585050821304\n"
     ]
    }
   ],
   "source": [
    "model.stage = 'fit_2'\n",
    "model.train()\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(dirpath=chechpoint_path, save_top_k=2, monitor=\"val_loss\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    num_nodes=1,\n",
    "    num_sanity_val_steps=0,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = checkpoint_callback.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{chechpoint_path}/best2.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data\n",
    "del model\n",
    "del trainer\n",
    "# torch.cuda.empty_cache()\n",
    "pl.utilities.memory.garbage_collection_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('checkpoints_vk_distilbert/best2.txt', 'r', encoding='utf-8') as f:\n",
    "    best_model_path = f.readlines()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = NYDataset(embeddings, input_ids)\n",
    "model = LitERAE.load_from_checkpoint(checkpoint_path=best_model_path, data=data, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_mechanism(pho, epsilon, delta):\n",
    "    pho = np.array(pho)\n",
    "    temp = np.exp(epsilon / (2 * delta) * pho)\n",
    "    return temp / np.sum(temp)\n",
    "\n",
    "\n",
    "def predict(model, sent, seed=None):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    \n",
    "    inp = get_embedding(sent)[0].unsqueeze(0).cuda()\n",
    "    with torch.no_grad():\n",
    "        logits = model(inp)[0].cpu()\n",
    "    \n",
    "    predicted_probs = torch.softmax(logits, dim=-1)\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    def build_two_sets(probs, k=5):\n",
    "        # return lexical set and semantic set\n",
    "        probs = np.array(probs)\n",
    "        l_set = rng.choice(probs.shape[0], k, p=probs, replace=True)\n",
    "        l_set_probs = probs[l_set]\n",
    "\n",
    "        marks = np.ones(probs.shape[0], dtype=bool)\n",
    "        marks[l_set] = False\n",
    "\n",
    "        whole_idxs = np.arange(probs.shape[0])\n",
    "        s_set = whole_idxs[marks]\n",
    "        s_set_probs = probs[marks]\n",
    "\n",
    "        return l_set, s_set, l_set_probs, s_set_probs\n",
    "\n",
    "    def choose_set(l_set, s_set, l_set_probs, s_set_probs, eps=180):\n",
    "        probs = [0, 0]\n",
    "        probs[0] = np.sum(l_set_probs) / (np.sum(l_set_probs) + np.sum(s_set_probs))\n",
    "        probs[1] = 1 - probs[0]\n",
    "        probs = exponential_mechanism(probs, eps, 1)\n",
    "        po = [(l_set, l_set_probs), (s_set, s_set_probs)]\n",
    "        indxs = [0, 1]\n",
    "        indx = int(rng.choice(indxs, 1, p=probs))\n",
    "        return po[indx]\n",
    "\n",
    "    for probs in predicted_probs:\n",
    "        # build set\n",
    "        l_set, s_set, l_set_probs, s_set_probs = build_two_sets(probs, k=5)\n",
    "\n",
    "        # choose set\n",
    "        c_set, c_set_probs = choose_set(l_set, s_set, l_set_probs, s_set_probs)\n",
    "\n",
    "        # choose token\n",
    "        token_eps = 0.1\n",
    "        c_set_probs = exponential_mechanism(c_set_probs, token_eps, 1)\n",
    "        token_idx = int(rng.choice(c_set, 1, p=c_set_probs))\n",
    "\n",
    "        if token_idx == EOS_TOKEN_ID:\n",
    "            break\n",
    "        res.append(token_idx)\n",
    "#     o_pred = ' '.join([self.params['idx2word'][idx] for idx in res])\n",
    "#     o_pred = tokenizer.decode(res, skip_special_tokens=True)\n",
    "    o_pred = ' '.join(idx2token[idx] for idx in res[1:])\n",
    "    return o_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Original>:\n",
      "—É—Ä–∞ \u0001 —É—Ä–∞ \u0001 –ø–æ–ª—É—á–∏–ª–∏ –Ω–æ–≤–∏–Ω–∫–∏ –ª–∏–Ω–∏—è \u0001 22 112 —Ä–∞–∑–º–µ—Ä \u0001 50 52 54 56 58 60 —Ü–µ–Ω–∞ \u0001 1100 —Ä\n",
      "--------------------------------->\n",
      "<Transformed>:\n",
      "—É—Ä–∞ —É—Ä–∞ –ø–æ–ª—É—á–∏–ª–∏ –Ω–æ–≤–∏–Ω–∫–∏ –∫–∞—Ä—Ç–∞ 22 150 —Ä–∞–∑–º–µ—Ä 50 52 54 58 58 60 —Ü–µ–Ω–∞ 500 —Ä\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "text = test_posts.text_clean[test_posts.index[1]]\n",
    "print('<Original>:')\n",
    "print(text)\n",
    "print('--------------------------------->')\n",
    "print('<Transformed>:')\n",
    "print(predict(model, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Original>:\n",
      "–¥–æ–±—Ä–æ–µ —É—Ç—Ä–æ \u0001 –ª–µ–≥–∫–æ–≥–æ –ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫–∞ \u0001 –æ—Å–µ–Ω—å \u0001 —É—Ç—Ä–æ \u0001 –¥–æ–º \u0001 \u0001 –≥–æ—Ä–µ—á—å \u0001 –∫–æ—Ñ–µ \u0001 \u0001 –¥–æ–∂–¥—å \u0001 —Ç–æ—Å–∫–∞ \u0001 –¥—É—à–∞ \u0001 –≤–∑–≥–ª—è–¥ \u0001 –∫–æ–º–ø—å—é—Ç–µ—Ä \u0001 –≤–∑–¥–æ—Ö \u0001 –Ω–∞–¥–µ–∂–¥–∞ \u0001 –≥—Ä—É—Å—Ç—å \u0001 —Å–æ–º–Ω–µ–Ω—å–µ \u0001 –∏–Ω—Ç–µ—Ä–Ω–µ—Ç \u0001 –∫–Ω–æ–ø–∫–∞ \u0001 \u0001 —ç–∫—Ä–∞–Ω \u0001 –ø—Ä–∏–≤–µ—Ç \u0001 –≤–µ—Ä–∞ \u0001 —Å–µ—Ä–¥—Ü–µ \u0001 —Å—Ç—É–∫ \u0001 –º–≥–Ω–æ–≤–µ–Ω—å–µ \u0001 –±–ª–µ—Å–∫ \u0001 –≥–ª–∞–∑–∞ \u0001 –º–∏–≥ \u0001 —Å–æ–ª–Ω—Ü–µ \u0001 –ª—É—á–∏–∫ \u0001 –æ–¥–Ω–æ—á–∞—Å—å–µ \u0001 —Ä–∞–¥–æ—Å—Ç—å \u0001 —Å–º–µ—Ö \u0001 —É–ª—ã–±–∫–∞ \u0001 —Å—á–∞—Å—Ç—å–µ \u0001 –Ω–∏–∫–∞\n",
      "------------------------>\n",
      "<Transformed>:\n",
      "–¥–æ–±—Ä–æ–µ —É—Ç—Ä–æ –ª–µ–≥–∫–æ–≥–æ —Ä–∞—Å–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è –æ—Å–µ–Ω—å —É—Ç—Ä–æ –º–æ—Ä–µ –ø—Ä–æ—Ä—É–±–∏ –∫–æ—Ñ–µ –¥–æ–∂–¥—å –ø–æ–ª—è–Ω–∞ blue –≤–∑–≥–ª—è–¥ –æ–ª—å–≥–∞ –≤–ø–µ—á–∞—Ç–ª–µ–Ω–∏–π –Ω–∞–¥–µ–∂–¥—É —Å–º–µ—Ö —Å–æ–ª–Ω–µ—á–Ω–æ–≥–æ —Å–∞–¥ dial –ø–æ–¥–∞—Ä–æ–∫ –ø—Ä–∏–≤–µ—Ç –≤–µ—Ä—É –¥–º–∏—Ç—Ä–∏–π –∞—Ç–º–æ—Å—Ñ–µ—Ä–∞ –∞—Ä–æ–º–∞—Ç –±–ª–µ—Å–∫ –≥–ª–∞–∑–∞ —Å—Ç —Å–æ–ª–Ω—Ü–µ dial edition —Ä–∞–¥–æ—Å—Ç–∏ —Å—Ç—Ä–∞—Å—Ç—å [PAD] —Å—á–∞—Å—Ç—å–µ edition\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "<Original>:\n",
      "estel - —Å–∞–ª–æ–Ω –∫—Ä–∞—Å–æ—Ç—ã —É –≤–∞—Å –¥–æ–º–∞ –≤—ã–±—Ä–∞—Ç—å —Å–æ–≤–º–µ—Å—Ç–Ω—ã–µ –ø–æ–∫—É–ø–∫–∏ —Å–ø–± \u0001–∫–ª—é–∫v–∞ \u0001 –Ω–∞—à–∏ —Ä–∞–∑–¥–∞—á–∏\n",
      "------------------------>\n",
      "<Transformed>:\n",
      "estel - —Å–∞–ª–∞—Ç –∫—Ä–∞—Å–æ—Ç—É —É –≤–∞—Å 13 –≤—ã–±—Ä–∞—Ç—å —Å–æ–≤–º–µ—Å—Ç–Ω—ã–µ –ø–æ–∫—É–ø–∫–∏ —Å–ø–± –∫–ª—é–∫v–∞ –Ω–∞—à–∏ —Ä–∞–∑–¥–∞—á–∏\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "<Original>:\n",
      "my november \u0001 \u0001 \u0001–º–Ω–µ –Ω–µ –¥–∞–Ω–æ –∂–∏—Ç—å –Ω–æ—Ä–º–∞–ª—å–Ω–æ–π –∂–∏–∑–Ω—å—é \u0001 —Ç–æ–≥–¥–∞ –ø—Ä–∏—Ö–æ–¥–∏—Ç—Å—è –ø—Ä–∏–¥—É–º–∞—Ç—å —Å–≤–æ—é \u0001 \u0001 \u0001 —Å–∞—Ä–∞ \u0001\n",
      "------------------------>\n",
      "<Transformed>:\n",
      "my –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–æ—á–Ω–æ –º–Ω–µ –Ω–µ –¥–∞–µ—Ç –∂–∏—Ç—å –Ω–æ—Ä–º–∞–ª—å–Ω–æ –∂–∏–∑–Ω—å—é —Ç–æ–≥–¥–∞ –ø—Ä–∞–≤–¥–∞ –ø—Ä–∏–∫–ª—é—á–µ–Ω–∏–π —Å–≤–æ—é eyes\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "<Original>:\n",
      "–∏—Å–ø—ã—Ç–∞–π —É–¥–∞—á—É –∏ –≤—ã–∏–≥—Ä–∞–π –≤ –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–π –ª–æ—Ç–µ—Ä–µ–µ \u0001–∑–ª–æ–π —Å–∫–∞–∑–∫–∏ \u0001 –ø–µ—Ä–≤—ã–π –∫–ª–∏–∫–Ω—É–≤—à–∏–π –ø–æ–ª—É—á–∏—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –ø–æ–ø—ã—Ç–∫—É\n",
      "------------------------>\n",
      "<Transformed>:\n",
      "–∏—Å–ø—ã—Ç–∞–π —É–¥–∞—á—É –∏ –ø–æ–¥–ø–∏—Å—ã–≤–∞–π—Å—è –≤ –ø—Ä–∞–∑–¥–Ω–∏–∫ —Ä–æ—Å—Å–∏–∏ –∂–¥–∞—Ç—å —Å–∫–∞–∑–∫–∏ –ø–µ—Ä–≤—ã–π –∫–ª–∏–∫–Ω—É–≤—à–∏–π –ø–æ–ª—É—á–∏—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –ø–æ–ø—ã—Ç–∫—É\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "<Original>:\n",
      "–æ–¥–Ω–∞–∂–¥—ã —Ö–µ–º–∏–Ω–≥—É—ç–π –∑–∞–∫–ª—é—á–∏–ª —Å–ø–æ—Ä \u0001 —á—Ç–æ –Ω–∞–ø–∏—à–µ—Ç —Ä–∞—Å—Å–∫–∞–∑ \u0001 —Å–æ—Å—Ç–æ—è—â–∏–π –≤—Å–µ–≥–æ –∏–∑ —à–µ—Å—Ç–∏ —Å–ª–æ–≤ \u0001 —Å–ø–æ—Å–æ–±–Ω—ã–π –ª—é–±–æ–≥–æ —á–∏—Ç–∞—Ç–µ–ª—è \u0001 –ø–∏—Å–∞—Ç–µ–ª—é —É–¥–∞–ª–æ—Å—å –≤—ã–∏–≥—Ä–∞—Ç—å —Å–ø–æ—Ä \u0001 –¥–µ—Ç—Å–∫–∏–µ –±–æ—Ç–∏–Ω–æ—á–∫–∏ \u0001 sale \u0001 baby \u0001 never \u0001 \u0001 \u0001 —Ñ—Ä–µ–¥–µ—Ä–∏–∫ –±—Ä–∞—É–Ω —Å–æ—á–∏–Ω–∏–ª —Å—Ç—Ä–∞—à–Ω—É—é –∏—Å—Ç–æ—Ä–∏—é –∏–∑ –∫–æ–≥–¥–∞-–ª–∏–±–æ –Ω–∞–ø–∏—Å–∞–Ω–Ω—ã—Ö \u0001 ¬´–ø–æ—Å–ª–µ–¥–Ω–∏–π —á–µ–ª–æ–≤–µ–∫ –Ω–∞ –∑–µ–º–ª–µ —Å–∏–¥–µ–ª –≤ –∫–æ–º–Ω–∞—Ç–µ \u0001 –≤ –¥–≤–µ—Ä—å \u0001 \u0001 \u0001¬ª \u0001 \u0001 \u0001 –∞–º–µ—Ä–∏–∫–∞–Ω—Å–∫–∏–π –ø–∏—Å–∞—Ç–µ–ª—å –æ \u0001–≥–µ–Ω—Ä–∏ –≤—ã–∏–≥—Ä–∞–ª –∫–æ–Ω–∫—É—Ä—Å –Ω–∞ —Å–∞–º—ã–π –∫–æ—Ä–æ—Ç–∫–∏–π —Ä–∞—Å—Å–∫–∞–∑ \u0001 –∫–æ—Ç–æ—Ä—ã–π –∏–º–µ–µ—Ç –≤—Å–µ —Å–æ—Å—Ç–∞–≤–ª—è—é—â–∏–µ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ä–∞—Å—Å–∫–∞–∑–∞ ‚Äî \u0001 –∏ \u0001 –∏ –Ω–∞–¥ \u0001 –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –º–Ω–æ–≥–æ –ª–∏ –æ—Å—Ç–∞–ª–æ—Å—å –±–µ–Ω–∑–∏–Ω–∞ \u0001 –±—ã–ª–æ –¥–≤–∞–¥—Ü–∞—Ç—å —Ç—Ä–∏ –≥–æ–¥–∞¬ª \u0001 \u0001 \u0001 \u0001 –∞–Ω–≥–ª–∏—á–∞–Ω–µ —Ç–æ–∂–µ –∫–æ–Ω–∫—É—Ä—Å –Ω–∞ —Å–∞–º—ã–π –∫—Ä–∞—Ç–∫–∏–π —Ä–∞—Å—Å–∫–∞–∑ \u0001 –Ω–æ –ø–æ —É—Å–ª–æ–≤–∏—è–º –∫–æ–Ω–∫—É—Ä—Å–∞ \u0001 –≤ –Ω–µ–º –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∫–æ—Ä–æ–ª–µ–≤–∞ \u0001 –±–æ–≥ \u0001 —Å–µ–∫—Å \u0001 —Ç–∞–π–Ω–∞ \u0001 –ø–µ—Ä–≤–æ–µ –º–µ—Å—Ç–æ –ø—Ä–∏—Å—É–¥–∏–ª–∏ –∞–≤—Ç–æ—Ä—É —Ç–∞–∫–æ–≥–æ —Ä–∞—Å—Å–∫–∞–∑–∞ \u0001 ¬´–æ \u0001 –±–æ–∂–µ \u0001 ‚Äî –≤–æ—Å–∫–ª–∏–∫–Ω—É–ª–∞ –∫–æ—Ä–æ–ª–µ–≤–∞ \u0001 ‚Äî —è –±–µ—Ä–µ–º–µ–Ω–Ω–∞ –∏ –Ω–µ –∑–Ω–∞—é –æ—Ç –∫–æ–≥–æ \u0001¬ª \u0001 \u0001 \u0001 –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π –ø—Ä–∏–º–µ—Ä —Å–ø–∞—Ä—Ç–∞–Ω—Ü–µ–≤ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ —Ü–∞—Ä—è —Ñ–∏–ª–∏–ø–ø–∞ ii \u0001 –º–Ω–æ–≥–∏–µ –≥—Ä–µ—á–µ—Å–∫–∏–µ –≥–æ—Ä–æ–¥–∞ \u0001 –≤–∞–º —Å–¥–∞—Ç—å—Å—è –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ \u0001 –ø–æ—Ç–æ–º—É —á—Ç–æ –µ—Å–ª–∏ –º–æ—è –∞—Ä–º–∏—è –≤–æ–π–¥—ë—Ç –≤ –≤–∞—à–∏ –∑–µ–º–ª–∏ \u0001 —è –≤–∞—à–∏ —Å–∞–¥—ã \u0001 –ª—é–¥–µ–π –∏ –≥–æ—Ä–æ–¥¬ª \u0001 –Ω–∞ —ç—Ç–æ –æ—Ç–≤–µ—Ç–∏–ª–∏ –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º \u0001 \u0001 \u0001 \u0001 \u0001 –≤–∏–∫—Ç–æ—Ä –≥—é–≥–æ –æ—Ç–ø—Ä–∞–≤–∏–ª —Ä—É–∫–æ–ø–∏—Å—å —Ä–æ–º–∞–Ω–∞ —Å –ø–∏—Å—å–º–æ–º \u0001 –æ—Ç–≤–µ—Ç –±—ã–ª –Ω–µ –º–µ–Ω–µ–µ \u0001 ¬´ \u0001¬ª \u0001 \u0001 \u0001 b –Ω–∞ —Å–∞–º—É—é –∫–æ—Ä–æ—Ç–∫—É—é –ø–æ–±–µ–¥–∏–ª–∞ –æ–¥–Ω–∞ –ø–æ–∂–∏–ª–∞—è —Ñ—Ä–∞–Ω—Ü—É–∂–µ–Ω–∫–∞ \u0001 –∫–æ—Ç–æ—Ä–∞—è –Ω–∞–ø–∏—Å–∞–ª–∞ \u0001 —É –º–µ–Ω—è –±—ã–ª–æ –ª–∏—Ü–æ –∏ —é–±–∫–∞ \u0001 –∞ —Ç–µ–ø–µ—Ä—å ‚Äî\n",
      "------------------------>\n",
      "<Transformed>:\n",
      "–ø–µ—Ä–≤—ã–π –µ–∑–¥–∏—Ç –Ω–∞–ø–æ–º–∏–Ω–∞—é —Ä–∞–∑–≥–æ–≤–æ—Ä —á—Ç–æ –∞–≤—Ç–æ—Ä—É —Ä–µ—á—å —Å–æ–∑–¥–∞–Ω–Ω—ã–π –≤—Å–µ–≥–æ –∏–∑ –º–æ–Ω—Å–µ—Ä—Ä–∞—Ç —Å–ª–æ–≤ —Å—ç—Å –ª—é–±—ã–µ –ø–æ–¥—á–µ—Ä–∫–Ω—É–ª –ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–º —É–¥–∞–ª–æ—Å—å –ºi—Ä —Å–ø–æ—Ä –¥–µ—Ç—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã man –ø—Ä–æ—Ü–µ—Å—Å never –∞–ª–µ–∫—Å–µ–π –ª–∞–≤—Ä–æ–≤ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –∫–æ–Ω—Ü–µ—Ä—Ç–∞ –≤–∏–¥–æ–≤ –∏–∑ –∫–æ–≥–¥–∞ - –ª–∏–±–æ –æ—Ç—ã–≥—Ä–∞–ª ¬´ –Ω–µ–º–Ω–æ–≥–æ —á–µ–ª–æ–≤–µ–∫ –Ω–∞ –∑–µ–º–ª–µ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –≤ –∞–ª–∏—Å–∞ –≤ 2010 ¬ª –ª—É—á—à–∏—Ö –º–µ–¥–∞–ª—å –æ —Ñ—É—Ç–±–æ–ª—å–Ω—ã–µ –±–ª–∞–≥–æ–¥–∞—Ä—è –ø—Ä–æ—Ü–µ—Å—Å –Ω–∞ —Å–∞–º—ã–π 1974 —Ä–µ—á—å –∫–æ—Ç–æ—Ä—ã–π –∏–º–µ–µ—Ç –≤—Å–µ —É–±–∏–π—Ü—ã —Å–ª–µ–¥—ã –ª–∞–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ [UNK] –∏ –∏ –Ω–∞–¥ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –º–Ω–æ–≥–æ –ª–∏\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "<Original>:\n",
      "—è –Ω–∞–±—Ä–∞–ª–∞ 92000 –æ—á–∫–æ–≤ –Ω–∞ —É—Ä–æ–≤–Ω–µ \u0001 –∞ —Å–∫–æ–ª—å–∫–æ —Å–º–æ–∂–µ—à—å –Ω–∞–±—Ä–∞—Ç—å —Ç—ã?\n",
      "------------------------>\n",
      "<Transformed>:\n",
      "—è –Ω–∞–±—Ä–∞–ª–∞ 92000 –æ—á–∫–æ–≤ –Ω–∞ —É—Ä–æ–≤–Ω–µ –∞ —Å–∫–æ–ª—å–∫–æ —Å–º–æ–∂–µ—à—å –Ω–∞–±—Ä–∞—Ç—å —Ç—ã ?\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "<Original>:\n",
      "–∞–ª–µ–∫—Å–∞–Ω–¥—Ä 1 –º–∞—Ä—Ç–∞ 1909 - 9 –∞–≤–≥—É—Å—Ç–∞ 1966 \u0001 \u0001 \u0001 –ø—É—Å—Ç—å –≤—Å–µ —Å—Ç–∏—Ö–∏ \u0001 —á—Ç–æ –Ω–∞–ø–∏—Å–∞–ª —è –∑–∞ –≥–æ–¥ \u0001 –ø–µ—Ä–µ–¥ —Ç–æ–±–æ–π —Å–µ–π—á–∞—Å –ø–æ–∫–æ—Ä–Ω–æ –ª—è–≥—É—Ç \u0001 –≤–∑–∞–º–µ–Ω –∫–æ—Ä–∑–∏–Ω—ã —Ä–∞–∑–Ω–æ—Ü–≤–µ—Ç–Ω—ã—Ö —è–≥–æ–¥ \u0001 –∫–æ—Ç–æ—Ä—ã—Ö —è —Å–æ–±—Ä–∞—Ç—å —Ç–µ–±–µ –Ω–µ —Å–º–æ–≥ \u0001 –∏ –≤—Å–µ \u0001 –º–Ω–æ—é \u0001 –ø—É—Å—Ç—å —Å–≤–æ–µ –≤–∏–Ω–æ \u0001 —á—Ç–æ–±—ã –æ–Ω–æ –≤–æ–ª–Ω–æ—é –ø—ã–ª—å —Å —Ç–≤–æ–∏—Ö –Ω–æ–≥ \u0001 1958 \u0001 \u0001 \u0001 –¥–µ—Ä–µ–≤—å—è \u0001–∫–æ—Ç–æ—Ä—ã–π –≥–æ–¥ —É –º–æ–µ–≥–æ –æ–∫–Ω–∞ —Å —Å–æ—Å–Ω–∞ \u0001 –Ω–µ –∑–∞–º–µ—á–∞—è –Ω–∏—á–µ–≥–æ –≤–æ–∫—Ä—É–≥ \u0001 –∫–∞–∫ –º—ã —Å —Ç–æ–±–æ—é \u0001 –º–æ–π –¥—Ä—É–≥ \u0001 \u0001 \u0001 \u0001 –ø–æ–∂–µ–ª–∞–Ω–∏–µ —Ö–æ—Ç–µ–ª –±—ã –ø–æ–∂–µ–ª–∞—Ç—å –≤ —Ä–∞–¥–æ—Å—Ç–∏ \u0001 –≤ —Ä–∞–∑–ª—É–∫–∏ –∏–ª–∏ –≥–æ—Ä–µ –≤–µ—á–Ω–æ –ø–æ–º–Ω–∏—Ç—å –ø–µ—Ä–≤–æ–µ \u0001 –∑–∞–±—ã–≤–∞—è –æ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å—Å–æ—Ä–µ \u0001 \u0001 1958\n",
      "------------------------>\n",
      "<Transformed>:\n",
      "–∞–ª–µ–∫—Å–∞–Ω–¥—Ä 2 –º–∞—Ä—Ç–∞ —Å—Ç–æ–π–∫ - 9 –∞–≤–≥—É—Å—Ç–∞ –∑–∞–ø—è—Ç—ã–µ –ø—É—Å—Ç—å –≤—Å–µ –≥–∞—Ä–º–æ–Ω–∏–∏ —á—Ç–æ –Ω–∞–ø–∏—Å–∞–ª —è –∑–∞ –≥–æ–¥ –ø–µ—Ä–µ–¥ –≤–µ–ª–∏–∫–∏–º —Å–µ–≥–æ–¥–Ω—è —Ä–∞–∑–≤–µ —Ä–æ—Å—Å–∏–∏ –≤–µ—Ä—Ö —Å–∞–Ω–∞—Ç–æ—Ä–∏–π –ø—Ä–æ—â–µ–Ω–∏—è –∂–¥–∞–ª–∞ –∏–±–æ —è –ø–æ–∫—É–ø–∞—è —Ç–µ–±–µ –Ω–µ —Å–º–æ–≥ –∏ –≤—Å–µ —Ç–µ–±—è –ø–ª–æ—Ö–æ —Å–≤–æ–µ –ø–æ–ª–Ω—ã–π —á—Ç–æ–±—ã –æ–Ω–æ –∏—Å–ø–æ–ª–Ω—è–µ—Ç –ø–∞–º—è—Ç–Ω–∏–∫–∞ —Å –Ω—É–∂–Ω–∞ 1960 1958 —É–ª—ã–±–∫–∞ –∫–æ—Ç–æ—Ä—ã–π –≥–æ–¥ —É –º–æ–µ–π –æ–∫–Ω–∞ —Å –≤—Ä–µ–º–µ–Ω –Ω–µ –æ—Ç–º–µ—á–∞–ª–∏ –µ—â–µ –≤–æ–∫—Ä—É–≥ –∫–∞–∫ –º—ã —Å –∫–∞–∂–¥—É—é –º–æ–π –¥—Ä—É–≥ –º–æ–∑–≥ —Å–∫–∞–∑–∞–ª –±—ã –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –≤ —Ä–∞–¥–æ—Å—Ç–∏ –≤ —Å–∫–æ–≤–æ—Ä–æ–¥–µ –∏–ª–∏ –±–µ–ª—ã—Ö –≤\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "<Original>:\n",
      "–≤–∞—à–∞ –∏–¥–µ–∞–ª—å–Ω–∞—è —é–±–∫–∞ \u0001 –∫—Ä–∞—Å–Ω–∞—è –∏–ª–∏ –Ω–∞–π–¥–µ—Ç–µ —Å–≤–æ–π —É–Ω–∏–∫–∞–ª—å–Ω—ã–π —Å—Ç–∏–ª—å –≤–º–µ—Å—Ç–µ —Å –ø–æ–¥–ø–∏—Å—ã–≤–∞–π—Å—è –≤ –∏–Ω—Å—Ç–∞–≥—Ä–∞–º –Ω–∞ \u0001zumomoda –∏ –º—ã –ø—Ä–∏—à–ª—ë–º –ø—Ä–æ–º–æ-–∫–æ–¥ –Ω–∞ —Å–∫–∏–¥–∫—É –¥–æ—Å—Ç–∞–≤–∫–∞ —Å –ø—Ä–∏–º–µ—Ä–∫–æ–π –Ω–∞ –¥–æ–º –ø–æ –º–æ—Å–∫–≤–µ –∏ —Å–ø–± –æ—Ç–ø—Ä–∞–≤–∫–∞ –≤ –ª—é–±—ã–µ —Ä–µ–≥–∏–æ–Ω—ã —Ä–æ—Å—Å–∏–∏ –∏ —Å–Ω–≥ –∫—É—Ä—å–µ—Ä—Å–∫–æ–π —Å–ª—É–∂–±–æ–π —Å–¥—ç–∫\n",
      "------------------------>\n",
      "<Transformed>:\n",
      "–≤–∞—à–∏ –∏–¥–µ–∞–ª—å–Ω–æ–π —é–±–∫–∞ –∫—Ä–∞—Å–æ—Ç–∞ –∏–ª–∏ –Ω–∞–π–¥–µ—Ç–µ —Å–≤–æ–π —É–Ω–∏–∫–∞–ª—å–Ω—ã–π —Å—Ç–∏–ª—å –≤–º–µ—Å—Ç–µ —Å –ø–æ–¥–ø–∏—Å—ã–≤–∞–π—Å—è –≤ –∏–Ω—Å—Ç–∞–≥—Ä–∞–º –Ω–∞ –∫–æ–Ω—Ñ–µ—Ç—ã –∏ –º—ã –ø—Ä–∏—à–ª—ë–º –ø—Ä–æ–º–æ - –∫–æ–¥ –Ω–∞ —Å–∫–∏–¥–∫—É –¥–æ—Å—Ç–∞–≤–∫–∞ —Å –ø—Ä–∏–º–µ—Ä–∫–æ–π –Ω–∞ –¥–æ–º –ø–æ –º–æ—Å–∫–≤–µ –∏ —Å–ø–± –æ—Ç–ø—Ä–∞–≤–∫–∞ –≤ –ª—é–±—ã–µ —Ä–µ–≥–∏–æ–Ω—ã —Ä–æ—Å—Å–∏–∏ –∏ —Å–Ω–≥ –∫—É—Ä—å–µ—Ä—Å–∫–æ–π —Å–ª—É–∂–±–æ–π —Å–¥—ç–∫\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "<Original>:\n",
      "–æ—Ç–æ—Ä–≤–µ–º—Å—è –ø–æ - –ø–∏—Ç–µ—Ä—Å–∫–∏ \u0001 –∫–ª—É–± \u0001–∫—Ä–∞—Å–∞–≤–∏—Ü–∞ –∏ —á—É–¥–æ–≤–∏—â–µ \u0001 –±–ª–∞–≥–æ–¥–∞—Ä–∏—Ç –≤—Å–µ—Ö –ø—Ä–∏—à–µ–¥—à–∏—Ö —Å–µ–º–∏–¥–µ—Å—è—Ç–∏ –ø—è—Ç–∏ –≥–æ—Å—Ç–µ–π –Ω–∞—à–µ–≥–æ –≤–µ—á–µ—Ä–∞ 17 –º–∞—è \u0001 –±—É–¥–µ–º —Ä–∞–¥—ã –≤–∏–¥–µ—Ç—å –≤–∞—Å —Å–Ω–æ–≤–∞ –Ω–∞—Å–ª–∞–¥–∏–º—Å—è –∂–∏–∑–Ω—å—é –≤ —ç—Ç–æ—Ç –≤–µ—á–µ—Ä –∏ –ø–æ–ª—É—á–∏–º –≤–µ—Å—å —Å–ø–µ–∫—Ç—Ä —É–¥–æ–≤–æ–ª—å—Å—Ç–≤–∏–π –æ—Ç –Ω–µ–µ \u0001 24 –º–∞—è –Ω–µ —Å–∫—É—á–∞–π - –Ω–∞ –º–∞—Ä–∞—Ç–∞ –ø—Ä–∏—Ö–æ–¥–∏ \u0001 –¥–µ–Ω—å –≥–æ—Ä–æ–¥–∞ –≤–º–µ—Å—Ç–µ —Å –Ω–∞–º–∏ —ç—Ä–æ—Ç–∏—á–Ω–æ –æ—Ç–º–µ—á–∞–π \u0001 –ø–æ–∫–∞–∂–µ–º \u0001 —Å—Ç—Ä–∞—Å—Ç—å –∏ —è—Ä–∫–æ—Å—Ç—å –≤–∞—à–µ–π –Ω–µ—É–µ–º–Ω–æ–π —Å–µ–∫—Å—É–∞–ª—å–Ω–æ—Å—Ç–∏ \u0001 –≤ —ç—Ç–æ—Ç –≤–µ—Å–µ–Ω–Ω–∏–π –ø—Ä–∞–∑–¥–Ω–∏–∫ –Ω–∞—Å –æ–∂–∏–¥–∞—é—Ç –Ω–∞—Å—Ç–æ—è—â–∏–π —Å–∞–ª—é—Ç –∏–∑ –≤—Å–µ—Ö ¬´–º—É–∂—Å–∫–∏—Ö –ø—É—à–µ–∫¬ª \u0001 –≤—ã—Å—Ç—Ä–µ–ª–∏–≤–∞—è –º–µ—Ç–∫–∏–º–∏ –∑–∞–ª–ø–∞–º–∏ –≤–µ—Å—å —Ñ–µ–π–µ—Ä–≤–µ—Ä–∫ –æ—Ä–≥–∞–∑–º–∞ –∏ —É–¥–æ–≤–æ–ª—å—Å—Ç–≤–∏—è \u0001 \u0001 \u0001 \u0001 –Ω–∞—Å—Ç–æ—è—â–∞—è –±–∞–Ω—è –Ω–∞ –¥—Ä–æ–≤–∞—Ö —Å –≤–µ–Ω–∏–∫–∞–º–∏ \u0001 –±–∞—Å—Å–µ–π–Ω \u0001 –∏ –≤–µ—Å—å ¬´–∞–Ω—Ç—É—Ä–∞–∂ –≤–µ—Å–µ–ª–æ–≥–æ –∑–∞—Å—Ç–æ–ª—å—è¬ª \u0001 ¬´–ø–æ–¥–≤–∏–∂–Ω—ã–µ –∏–≥—Ä—ã¬ª \u0001 —Ç—É—Ä–Ω–∏—Ä –ø–æ ¬´—Å–µ–∫—Å—É–∞–ª—å–Ω–æ–º—É¬ª –¥–æ–º–∏–Ω–æ \u0001 –∞ —Å–∞–º–æ–µ –≥–ª–∞–≤–Ω–æ–µ —Ä–∞—Å–∫—Ä–µ–ø–æ—â–µ–Ω–Ω–æ–µ –∏ –ø—Ä–∏—è—Ç–Ω–æ–µ –æ–±—â–µ–Ω–∏–µ‚Ä¶ —Å –∫–∞–∂–¥–æ–π –º–∏–Ω—É—Ç–æ–π –ø—Ä–∏—Ç—è–≥–∏–≤–∞—é—â–µ–µ –≤—Å–µ –±–ª–∏–∂–µ –∏ –±–ª–∏–∂–µ‚Ä¶ –¥–ª—è —Ç–µ—Ö \u0001 –∫—Ç–æ —Ö–æ—á–µ—Ç —Ä–∞—Å—à–∏—Ä–∏—Ç—å –≥—Ä–∞–Ω–∏—Ü—ã –æ–±—ã–¥–µ–Ω–Ω–æ—Å—Ç–∏ \u0001 –∂–¥–µ–º –Ω–∞ –Ω–∞—à–µ–π –≤–µ—á–µ—Ä–∏–Ω–∫–µ \u0001 –≤ –≤–∞—à–µ–º —Ä–∞—Å–ø–æ—Ä—è–∂–µ–Ω–∏–∏ \u0001 2 —ç—Ç–∞–∂–∞ \u0001 1 —ç—Ç–∞–∂ –±–∞–Ω–Ω—ã–π –∫–æ–º–ø–ª–µ–∫—Å \u0001 2 —ç—Ç–∞–∂-—Ä–∞–∑–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π \u0001 –±–∞–Ω—è –Ω–∞ –¥—Ä–æ–≤–∞—Ö \u0001 –±–∞—Å—Å–µ–π–Ω \u0001 3 –∫–æ–º–Ω–∞—Ç—ã –æ—Ç–¥—ã—Ö–∞ \u0001 2 –º–∞—Å—Å–∞–∂–Ω—ã—Ö –º–µ—Å—Ç–∞ \u0001 –¥–ª—è –¥–µ–≤—É—à–µ–∫- –≤—Ö–æ–¥ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π –≤–∫–ª—é—á–µ–Ω —Ñ—É—Ä—à–µ—Ç—à–∞—à–ª—ã–∫ \u0001 –±—É—Ç–µ—Ä–±—Ä–æ–¥—ã \u0001 —Ñ—Ä—É–∫—Ç—ã \u0001 —Å–∞–ª–∞—Ç—ã \u0001–ø–∏–≤–æ \u0001 —à–∞–º–ø–∞–Ω—Å–∫–æ–µ \u0001 –±–µ–ª—ã–µ –∏ –∫—Ä–∞—Å–Ω—ã–µ –≤–∏–Ω–∞ \u0001 –±–∞–Ω–Ω—ã–µ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç–∏–ø—Ä–æ—Å—Ç—ã–Ω—è \u0001 —Ç–∞–ø–∫–∏ —Å—Ä–µ–¥—Å—Ç–≤–∞ –∑–∞—â–∏—Ç—ã \u0001 –¥–ª—è –º—É–∂—á–∏–Ω –±–µ–∑ –ø–∞—Ä—ã –≤—Ö–æ–¥ 2500 —Ä–≤ —Å—Ç–æ–∏–º–æ—Å—Ç—å –≤–∫–ª—é—á–µ–Ω —Ñ—É—Ä—à–µ—Ç—à–∞—à–ª—ã–∫ \u0001 –±—É—Ç–µ—Ä–±—Ä–æ–¥—ã \u0001 —Ñ—Ä—É–∫—Ç—ã \u0001 —Å–∞–ª–∞—Ç—ã \u0001 –ø–∏–≤–æ \u0001 —à–∞–º–ø–∞–Ω—Å–∫–æ–µ \u0001 –±–µ–ª—ã–µ –∏ –∫—Ä–∞—Å–Ω—ã–µ –≤–∏–Ω–∞ \u0001 –±–∞–Ω–Ω—ã–µ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç–∏–ø—Ä–æ—Å—Ç—ã–Ω—è \u0001 —Ç–∞–ø–∫–∏ —Å—Ä–µ–¥—Å—Ç–≤–∞ –∑–∞—â–∏—Ç—ã \u0001 –¥–ª—è –ø–∞—Ä—ã –º–∂-1000 —Ä —Å –ø–∞—Ä—ã –≤ —Å—Ç–æ–∏–º–æ—Å—Ç—å –≤–∫–ª—é—á–µ–Ω —Ñ—É—Ä—à–µ—Ç —à–∞—à–ª—ã–∫ \u0001 –±—É—Ç–µ—Ä–±—Ä–æ–¥—ã \u0001 —Ñ—Ä—É–∫—Ç—ã \u0001 —Å–∞–ª–∞—Ç—ã \u0001 –ø–∏–≤–æ \u0001 —à–∞–º–ø–∞–Ω—Å–∫–æ–µ \u0001 –±–µ–ª—ã–µ –∏ –∫—Ä–∞—Å–Ω—ã–µ –≤–∏–Ω–∞ \u0001 –±–∞–Ω–Ω—ã–µ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç–∏–ø—Ä–æ—Å—Ç—ã–Ω—è \u0001 —Ç–∞–ø–∫–∏ —Å—Ä–µ–¥—Å—Ç–≤–∞ –∑–∞—â–∏—Ç—ã \u0001 –º—ã –≤—Å–µ–≥–¥–∞ —Ä–∞–¥—ã –Ω–æ–≤—ã–º –∑–Ω–∞–∫–æ–º—Å—Ç–≤–∞–º —Å –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–º–∏ –ª—é–¥—å–º–∏ \u0001 –∑–∞–≥–ª—è–Ω–∏—Ç–µ –∫ –Ω–∞–º –≤ –∫–ª—É–± \u0001 –∏ –≤—ã –≤–Ω–æ–≤—å –ø–æ–≥—Ä—É–∑–∏—Ç–µ—Å—å –≤ —Ç—É \u0001 –Ω–µ–ø–µ—Ä–µ–¥–∞–≤–∞–µ–º—É—é –∞—Ç–º–æ—Å—Ñ–µ—Ä—É —á—É–≤—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ \u0001 –∏—Å–∫—Ä–∏—Å—Ç–æ–π —Ä–∞–¥–æ—Å—Ç–∏ –∏ —Ç–µ–ø–ª–∞ \u0001 –ø—Ä–µ–¥—á—É–≤—Å—Ç–≤–∏–µ –∏–∑—ã—Å–∫–∞–Ω–Ω—ã—Ö –Ω–∞—Å–ª–∞–∂–¥–µ–Ω–∏–π –Ω–∞–ø–æ–ª–Ω–∏—Ç –≤–∞—Å —Å–ª–∞–¥–∫–æ–π –∏—Å—Ç–æ–º–æ–π \u0001 –≤–µ—á–µ—Ä–∏–Ω–∫–∏ –ø—Ä–æ—Ö–æ–¥—è—Ç –µ–∂–µ–Ω–µ–¥–µ–ª—å–Ω–æ \u0001 –ø–æ –ø—è—Ç–Ω–∏—Ü–∞–º \u0001 19 \u000130-23 \u000130 \u0001 –≤ —Ü–µ–Ω—Ç—Ä–µ \u0001 –Ω–∞ –º–∞—Ä–∞—Ç–∞ \u0001 –≤—Ö–æ–¥ —Ç–æ–ª—å–∫–æ –ø–æ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –∑–∞–ø–∏—Å–∏ –ø–æ —Ç–µ–ª–µ—Ñ–æ–Ω—É \u0001 –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Å—Ç - –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–æ \u0001 –∑–∞–ø–∏—Å—å –ø–æ —Ç–µ–ª–µ—Ñ–æ–Ω–∞–º \u0001 +7-911-218-72-19 –Ω–∞—Ç–∞–ª–∏ \u0001 +7-981-767-85-44 –≤–ª–∞–¥–∏–º–∏—Ä –µ—Å–ª–∏ –≤—ã –ª—é–±–∏—Ç–µ –Ω–æ–≤—ã–µ –∑–Ω–∞–∫–æ–º—Å—Ç–≤–∞ \u0001 —Ö–æ—Ä–æ—à–∏–π –æ—Ç–¥—ã—Ö \u0001 –∏ –∑–Ω–∞–µ—Ç–µ —Ç–æ–ª–∫ –≤ –∂–∏–∑–Ω–∏ \u0001 —Å–≤–æ–±–æ–¥–Ω–æ–π –æ—Ç –ø—Ä–µ–¥—Ä–∞—Å—Å—É–¥–∫–æ–≤ \u0001 —Ç–æ–≥–¥–∞ —ç—Ç–∞ –≤–µ—á–µ—Ä–∏–Ω–∫–∞ - –¥–ª—è –≤–∞—Å \u0001 \u0001 \u0001 –∂–µ–ª–∞–µ–º –≤–∞–º —è—Ä–∫–∏—Ö —Å–µ–∫—Å—É–∞–ª—å–Ω—ã—Ö –≤–ø–µ—á–∞—Ç–ª–µ–Ω–∏–π –∏ –Ω–æ–≤—ã—Ö –ø—Ä–∏—è—Ç–Ω—ã—Ö –∑–Ω–∞–∫–æ–º—Å—Ç–≤ –≤ –Ω–∞—à–µ–º –∫–ª—É–±–µ \u0001 –≤–µ—á–µ—Ä–∏–Ω–∫–∞ —Å–æ—Å—Ç–æ–∏—Ç—Å—è \u0001 24 –º–∞—è —Å 19 —á 30 –º–∏–Ω –¥–æ 23 —á 30 –º–∏–Ω \u0001 –ø–æ –∞–¥—Ä–µ—Å—É \u0001 —É–ª \u0001 –º–∞—Ä–∞—Ç–∞ \u0001 –¥ \u0001 75 \u0001 –º \u0001 –∑–≤–µ–Ω–∏–≥–æ—Ä–æ–¥—Å–∫–∞—è attention \u0001 —Ç—Ä–µ—Ç—å—è –ø—Ä–∏–≤–∞—Ç –∫–æ–º–Ω–∞—Ç–∞–¥–∞–ª—å–Ω—è—è —Å–ø—Ä–∞–≤–∞ –æ—Ç –≤—Ö–æ–¥–∞ –Ω–∞ 2 —ç—Ç–∞–∂ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–∞ –¥–ª—è –ø–∞—Ä –º–∂ \u0001 –∂–µ–Ω—â–∏–Ω \u0001 –º—É–∂—á–∏–Ω \u0001 –Ω–µ –æ–±–ª–∞–¥–∞—é—â–∏—Ö –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π —Å—Ç–µ–ø–µ–Ω—å—é —Ä–∞—Å–∫—Ä–µ–ø–æ—â–µ–Ω–∏—è –∏ –∏—Å–ø—ã—Ç—ã–≤–∞—é—â–∏—Ö –Ω–µ–∫—É—é –Ω–µ–ª–æ–≤–∫–æ—Å—Ç—å –Ω–∞ –Ω–∞—á–∞–ª—å–Ω–æ–º —ç—Ç–∞–ø–µ –≥—Ä—É–ø–ø–æ–≤—ã—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏–π \u0001 –µ—Å–ª–∏ –≤—ã –≤—ã–±—Ä–∞–ª–∏ —Å–µ–±–µ –ø–∞—Ä—Ç–Ω–µ—Ä–æ–≤ \u0001 —Ç–æ –º–æ–∂–µ—Ç–µ –∑–∞–∫—Ä—ã—Ç—å—Å—è —Å –Ω–∏–º–∏ –Ω–∞ –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è –≤ —ç—Ç–æ–π –ø—Ä–∏–≤–∞—Ç –∫–æ–º–Ω–∞—Ç–µ \u0001 —Å–æ–≥–ª–∞—Å–æ–≤–∞–≤ —ç—Ç–æ —Å –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏–µ–π \u0001 –∂–µ–ª–∞–µ–º –ø—Ä–∏—è—Ç–Ω–æ–≥–æ –æ—Ç–¥—ã—Ö–∞ \u0001 +7 911 218-72-19 swing-plus \u0001ru 24 –º–∞—è –≤ 19 \u000130 ‚Äî 24 –º–∞—è –≤ 23 \u000130 —É–ª \u0001 –º–∞—Ä–∞—Ç–∞ \u0001 –¥ \u0001 75 \u0001 —Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥\n",
      "------------------------>\n",
      "<Transformed>:\n",
      "–æ—Ç–æ—Ä–≤–µ–º—Å—è –ø–æ - –∫—Ä–∞—Å–∞–≤–∏—Ü–∞ –∫–ª—É–± –∫—Ä–∞—Å–∞–≤–∏—Ü–∞ –∏ —á—É–¥–æ–≤–∏—â–µ –±–ª–∞–≥–æ–¥–∞—Ä–∏—Ç –≤—Å–µ—Ö –ø—Ä–∏—à–µ–¥—à–∏—Ö —Å–µ–º–∏–¥–µ—Å—è—Ç–∏ —Ç–æ—á–∫–∏ –≥–æ—Å—Ç–µ–π –Ω–∞—à–µ–≥–æ –≤–µ—á–µ—Ä–∞ 17 –º–∞—è –±—É–¥–µ–º —Ä–∞–¥—ã –≤–∏–¥–µ—Ç—å –≤–∞—Å —Å–Ω–æ–≤–∞ –Ω–∞—Å–ª–∞–¥–∏–º—Å—è –∂–∏–∑–Ω—å—é –≤ —ç—Ç–æ—Ç –≤–µ—á–µ—Ä –∏ –ø–æ–ª—É—á–∏–º –≤–µ—Å—å —Å–ø–µ–∫—Ç—Ä —É–¥–æ–≤–æ–ª—å—Å—Ç–≤–∏–π –æ—Ç –Ω–µ–µ 24 –º–∞—è –Ω–µ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å - –Ω–∞ –º–∞—Ä–∞—Ç–∞ –ø—Ä–∏–π—Ç–∏ –¥–µ–Ω—å –≥–æ—Ä–æ–¥–∞ –≤–º–µ—Å—Ç–µ —Å –Ω–∞–º–∏ —ç—Ä–æ—Ç–∏—á–Ω–æ –æ—Ç–º–µ—á–∞–π –ø–æ–∫–∞–∂–µ–º —Å—Ç—Ä–∞—Å—Ç—å –∏ –≤–∞—à–∏—Ö –≤–∞—à–µ–π –Ω–µ—É–µ–º–Ω–æ–π —Å–µ–∫—Å—É–∞–ª—å–Ω–æ—Å—Ç–∏ –≤ —ç—Ç–æ—Ç –≤–µ—Å–µ–Ω–Ω–∏–π\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "<Original>:\n",
      "–º–æ—Ä–æ–∂–µ–Ω–æ–µ \u0001 –ø—Ä–æ—Å—Ç–æ–π —Ä–µ—Ü–µ–ø—Ç –∏–∑ 2 –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç–æ–≤ / ice cream simple\n",
      "------------------------>\n",
      "<Transformed>:\n",
      "–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –ø—Ä–æ—Å—Ç–æ–π —Ä–µ—Ü–µ–ø—Ç –∏–∑ 2 ele—Åtronika / river wostok edition\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "test_texts = test_posts.text_clean\n",
    "for i in range(100, 110):\n",
    "    text = test_texts[test_posts.index[i]]\n",
    "    print('<Original>:')\n",
    "    print(text)\n",
    "    print('------------------------>')\n",
    "    print('<Transformed>:')\n",
    "    print(predict(model, text))\n",
    "    print('\\n------------------------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anonymyze texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = bert.to('cuda')\n",
    "\n",
    "def get_hidden_states_batch(encoded, model, layers):\n",
    "    \"\"\"Push input IDs through model. Stack and sum `layers` (last four by default).\n",
    "        Select only those subword token outputs that belong to our word of interest\n",
    "        and average them.\"\"\"\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        states = model(**encoded).hidden_states\n",
    " \n",
    "    batch_res = []\n",
    "    \n",
    "    for i in range(len(states[0])):\n",
    "        token_ids_words = encoded.word_ids(i)\n",
    "        output = torch.stack([states[layer][i] for layer in layers]).sum(0).squeeze().cpu()\n",
    "\n",
    "        res = []\n",
    "        labels_count = []\n",
    "\n",
    "        for idx, (outp, label) in enumerate(zip(output, token_ids_words)):\n",
    "            if label is None or token_ids_words[idx - 1] is None or token_ids_words[idx - 1] != token_ids_words[idx]:\n",
    "                res.append(outp)\n",
    "                labels_count.append(1)\n",
    "            else: \n",
    "                res[-1] += outp\n",
    "                labels_count[-1] += 1\n",
    "\n",
    "        res = torch.vstack(res)\n",
    "        res = res / torch.tensor(labels_count).float().unsqueeze(1)\n",
    "        \n",
    "        batch_res.append(res)\n",
    "    batch_res = nn.utils.rnn.pad_sequence(batch_res, batch_first=True, padding_value=0.0)\n",
    "    return batch_res\n",
    "\n",
    "def get_word_vectors_batch(sents, tokenizer, model, layers):\n",
    "    \"\"\"Get a word vector by first tokenizing the input sentence, getting all token idxs\n",
    "        that make up the word of interest, and then `get_hidden_states`.\"\"\"\n",
    "    \n",
    "    encoded = tokenizer.batch_encode_plus(sents, return_tensors=\"pt\", max_length=128, padding='max_length', truncation=True)\n",
    "    \n",
    "    device = torch.device('cuda')\n",
    "    encoded = encoded.to(device)\n",
    "\n",
    "    hidden_states = get_hidden_states_batch(encoded, model, layers)\n",
    "    return hidden_states\n",
    "\n",
    "def get_embedding_batch(docs, model=bert):\n",
    "    \"Get embedding for each word\"\n",
    "    layers = [-4, -3, -2, -1]\n",
    "    return get_word_vectors_batch(docs, tokenizer, model, layers)\n",
    "\n",
    "def predict_batch(model, sents, seed=None, batch_size=32, verbose=False, print_step=20):\n",
    "    model.eval()\n",
    "    rng = np.random.default_rng(seed)\n",
    "    \n",
    "    def build_two_sets(probs, k=5):\n",
    "        # return lexical set and semantic set\n",
    "        probs = np.array(probs)\n",
    "        l_set = rng.choice(probs.shape[0], k, p=probs, replace=True)\n",
    "        l_set_probs = probs[l_set]\n",
    "\n",
    "        marks = np.ones(probs.shape[0], dtype=bool)\n",
    "        marks[l_set] = False\n",
    "\n",
    "        whole_idxs = np.arange(probs.shape[0])\n",
    "        s_set = whole_idxs[marks]\n",
    "        s_set_probs = probs[marks]\n",
    "\n",
    "        return l_set, s_set, l_set_probs, s_set_probs\n",
    "\n",
    "    def choose_set(l_set, s_set, l_set_probs, s_set_probs, eps=80):\n",
    "        probs = [0, 0]\n",
    "        probs[0] = np.sum(l_set_probs) / (np.sum(l_set_probs) + np.sum(s_set_probs))\n",
    "        probs[1] = 1 - probs[0]\n",
    "        probs = exponential_mechanism(probs, eps, 1)\n",
    "        po = [(l_set, l_set_probs), (s_set, s_set_probs)]\n",
    "        indxs = [0, 1]\n",
    "        indx = int(rng.choice(indxs, 1, p=probs))\n",
    "        return po[indx]\n",
    "    \n",
    "    o_pred = []\n",
    "    \n",
    "    for i in range(0, len(sents), batch_size):\n",
    "        batch = sents[i:i+batch_size]\n",
    "        inp = get_embedding_batch(batch).to('cuda')\n",
    "        with torch.no_grad():\n",
    "            logits = model(inp).cpu()\n",
    "\n",
    "        predicted_probs = torch.softmax(logits, dim=-1)\n",
    "    \n",
    "        for sent in predicted_probs:\n",
    "            res = []\n",
    "            for probs in sent:\n",
    "                # build set\n",
    "                l_set, s_set, l_set_probs, s_set_probs = build_two_sets(probs, k=5)\n",
    "\n",
    "                # choose set\n",
    "                c_set, c_set_probs = choose_set(l_set, s_set, l_set_probs, s_set_probs)\n",
    "\n",
    "                # choose token\n",
    "                token_eps = 0.1\n",
    "                c_set_probs = exponential_mechanism(c_set_probs, token_eps, 1)\n",
    "                token_idx = int(rng.choice(c_set, 1, p=c_set_probs))\n",
    "\n",
    "                if token_idx == EOS_TOKEN_ID:\n",
    "                    break\n",
    "                res.append(token_idx)\n",
    "        #     o_pred = tokenizer.decode(res, skip_special_tokens=True)\n",
    "            o_pred.append(' '.join(idx2token[idx] for idx in res[1:]))\n",
    "        \n",
    "        if verbose and (i % (print_step * batch_size)) == 0:\n",
    "            print(i)\n",
    "\n",
    "    return o_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "640\n",
      "1280\n",
      "1920\n",
      "2560\n",
      "3200\n",
      "3840\n",
      "4480\n",
      "5120\n",
      "5760\n",
      "6400\n",
      "7040\n",
      "7680\n",
      "8320\n",
      "8960\n",
      "9600\n",
      "10240\n",
      "10880\n",
      "11520\n",
      "12160\n",
      "12800\n",
      "13440\n",
      "14080\n",
      "14720\n",
      "15360\n",
      "16000\n",
      "16640\n",
      "17280\n",
      "17920\n",
      "18560\n",
      "19200\n",
      "19840\n",
      "20480\n",
      "21120\n",
      "21760\n",
      "22400\n",
      "23040\n",
      "23680\n",
      "24320\n",
      "24960\n",
      "25600\n",
      "26240\n",
      "26880\n",
      "27520\n",
      "28160\n",
      "28800\n",
      "29440\n",
      "30080\n",
      "30720\n",
      "31360\n",
      "32000\n",
      "32640\n",
      "33280\n",
      "33920\n",
      "34560\n",
      "35200\n",
      "35840\n",
      "36480\n",
      "37120\n",
      "37760\n",
      "38400\n"
     ]
    }
   ],
   "source": [
    "train__ = train_posts.text_clean.tolist()\n",
    "model.eval()\n",
    "trans_train = predict_batch(model, train__, seed=42, verbose=True)\n",
    "\n",
    "train_posts['text_clean_anon'] = trans_train\n",
    "train_posts.to_csv('/home/jovyan/notebooks/vk/train_posts_anon.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "test__ = test_posts.text_clean.tolist()\n",
    "model.eval()\n",
    "trans_test = predict_batch(model, test__, seed=42, verbose=True)\n",
    "\n",
    "test_posts['text_clean_anon'] = trans_test\n",
    "test_posts.to_csv('/home/jovyan/notebooks/vk/test_posts_anon.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_posts_anon = pd.read_csv('/home/jovyan/notebooks/vk/train_posts_anon.csv')\n",
    "test_posts_anon = pd.read_csv('/home/jovyan/notebooks/vk/test_posts_anon.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashtags anonymization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SynTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = nyc_posts_authors_df.loc[nyc_posts_authors_df['hashtags'].str.len() > 50, 'hashtags'][:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#bike #bikes #bikelife #shopride #shoplife #myfavoritebikeshop #bikeclub #bikenyc #bikeny #croton #crotonaqueduct #gravelgrinder #gravel #gravelride #offroad'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18078"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "s = set(chain.from_iterable(docs.str.split()))\n",
    "len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_model = CountVectorizer(ngram_range=(1,1))\n",
    "X = count_model.fit_transform(docs)\n",
    "Xc = (X.T * X)\n",
    "Xc.setdiag(0)\n",
    "Xc = Xc.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5119884,\n",
       " matrix([[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 5, 5],\n",
       "         [0, 0, 0, ..., 5, 0, 5],\n",
       "         [0, 0, 0, ..., 5, 5, 0]]))"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xc.sum(), Xc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13364, 13364)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.4684e-05, 7.4684e-05, 7.4684e-05,  ..., 7.4684e-05, 7.4684e-05,\n",
       "         7.4684e-05],\n",
       "        [7.4582e-05, 7.4582e-05, 7.4582e-05,  ..., 7.4582e-05, 7.4582e-05,\n",
       "         7.4582e-05],\n",
       "        [7.3181e-05, 7.3181e-05, 7.3181e-05,  ..., 7.3181e-05, 7.3181e-05,\n",
       "         7.3181e-05],\n",
       "        ...,\n",
       "        [3.0429e-07, 3.0429e-07, 3.0429e-07,  ..., 3.0429e-07, 4.5161e-05,\n",
       "         4.5161e-05],\n",
       "        [3.0429e-07, 3.0429e-07, 3.0429e-07,  ..., 4.5161e-05, 3.0429e-07,\n",
       "         4.5161e-05],\n",
       "        [3.0429e-07, 3.0429e-07, 3.0429e-07,  ..., 4.5161e-05, 4.5161e-05,\n",
       "         3.0429e-07]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xc = torch.softmax(torch.from_numpy(Xc).float(), dim=-1)\n",
    "Xc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.0000), tensor(7.4684e-05))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xc[0].sum(), Xc[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "tfidf.fit(docs)\n",
    "doc_vecs = tfidf.transform(docs)\n",
    "doc_vecs = normalize(doc_vecs, norm='l1')\n",
    "words = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18063"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1405975, 5131770)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft = fasttext.load_facebook_model(datapath('/mnt/ess_storage/DN_1/storage/home/vpanov/cc.en.300.bin.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xc[count_model.vocabulary_['newyork']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13364, 13364)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vecs = [Xc[count_model.vocabulary_[word]].tolist()[0] for word in words]\n",
    "word_similarities = cosine_similarity(word_vecs, word_vecs)\n",
    "word_similarities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kgram_overlap(word1, word2, k):\n",
    "    a = set([word1[i:i+k] for i in range(0, len(word1) - k + 1)])\n",
    "    b = set([word2[i:i+k] for i in range(0, len(word2) - k + 1)])\n",
    "    inter = len(a.intersection(b))\n",
    "    return inter / (len(a) + len(b) - inter)\n",
    "\n",
    "def score(word1, word2):\n",
    "    idx1, idx2 = tfidf.vocabulary_[word1], tfidf.vocabulary_[word2]\n",
    "    return word_similarities[idx1, idx2] - 0.3 * kgram_overlap(word1, word2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "likes4like ['turkiye']\n"
     ]
    }
   ],
   "source": [
    "def exponential_gen(x, R, u, sensitivity=1, epsilon=25.4):\n",
    "    # Calculate the score for each element of R\n",
    "    scores = [u(x, r) for r in R]\n",
    "    \n",
    "    # Calculate the probability for each element, based on its score\n",
    "    probabilities = [np.exp(epsilon * score / (2 * sensitivity)) for score in scores]\n",
    "    \n",
    "    # Normalize the probabilties so they sum to 1\n",
    "    probabilities = probabilities / np.linalg.norm(probabilities, ord=1)\n",
    "\n",
    "    # Choose an element from R based on the probabilities\n",
    "    return np.random.choice(R, 1, p=probabilities)\n",
    "\n",
    "num = 7000\n",
    "print(words[num], exponential_gen(words[num], words, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13364, 13364)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def exponential(x, R, u, sensitivity=1, epsilon=25.4):\n",
    "    # Calculate the score for each element of R\n",
    "    scores = [u(x, r) for r in R]\n",
    "    \n",
    "    # Calculate the probability for each element, based on its score\n",
    "    probabilities = [np.exp(epsilon * score / (2 * sensitivity)) for score in scores]\n",
    "    \n",
    "    # Normalize the probabilties so they sum to 1\n",
    "    probabilities = probabilities / np.linalg.norm(probabilities, ord=1)\n",
    "    \n",
    "    return probabilities\n",
    "\n",
    "word_replace_probs = []\n",
    "\n",
    "for idx, word in enumerate(words):\n",
    "    if idx % 1000 == 0:\n",
    "        print(idx)\n",
    "    word_replace_probs.append(exponential(word, words, score))\n",
    "\n",
    "word_replace_probs = np.array(word_replace_probs)\n",
    "word_replace_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--ORIGINAL--\n",
      "#EATDRINKPARTY #miercolesplayero #bronx #BottleSpecials #bestiakitchenbx #LaBestiaDelBronx #HappyHour #PartyPeople #FoodPorn #salsa #playero #retro #4thofjulyparty #preindependenceday #preindependencedayparty\n",
      "--GENERATED SEQUENCE (WITHOUT WORD ORDER)--\n",
      "gaynightlife nbafinals2019 bachata miercolesplayero eatdrinkparty nbafinals2019 santiago 4thofjulyparty caucau 4thofjulyparty lentejitas nycdrinks ericktorres latino 4thofjulyparty\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "--ORIGINAL--\n",
      "#914 #newrochelle #newrochelleny #ionacollege #westchester #westchesterny #westchestereats #westchesternyeats #westchestercountyny #westchestercounty #westchesterfood #westchesterfoodie #tuckahoeny #tuckahoe #eastchester #eastchesterny #larchmont #larchmontny #larchmontvillage #pelhamny #yonkers #yonkersny #bronxville #bronxvilleny #burgersandbeer #bestwings\n",
      "--GENERATED SEQUENCE (WITHOUT WORD ORDER)--\n",
      "westchestereats burgersandfries jameson yonkersny jameson latenightfood larchmontny larchmont yonkers yonkers panini phillycheesesteak tuckahoe titosvodka pelhammanor larchmontny bestwingsever jamesonwhiskey bestwings yonkers newrochelle burgersandfries salad westchesternyeats jamesonwhiskey yonkers\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "--ORIGINAL--\n",
      "#effigies #forevergrounded #record #punk #1984 #theeffigies #records #vinyl #newyorkcity #recordsforsale #carrollgardens #redhook #chicago #punkrock #vinylporn #recordshop #hardcorepunk #vinyligclub #rock #almostreadyrecords #vinylforsale #webuyvinyl #buyselltrade #nyc #brooklyn #vinylrecord #enigma #lp\n",
      "--GENERATED SEQUENCE (WITHOUT WORD ORDER)--\n",
      "carlitosway stooges thebeatles almostready citoferminorchestra 33rpm 1970 effigies 1996 reissue thedictatorsnyc records mysaxophoneforchristmas modusoperandi doom thecity sealed almostchristmas miccitysons redhook redvinyl enigma effigies alifewithoutobstacles jamesbrown recordporn countryteasers ska\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "--ORIGINAL--\n",
      "#10yearchallenge #tgif #1 #happyhour #78loungenj #drinks #food #birthday #turnup #party #goodlife #nightlife #dj #like4like #goodtime #friends #family #followme #instaselfie #swag #follow #newjersey #friday #flashbackfriday\n",
      "--GENERATED SEQUENCE (WITHOUT WORD ORDER)--\n",
      "followme 78lifestyle amazonbooks tournament 78loungenj theheavyhitterdjs sipandpaint friends niceandsmooth family friends lawenforcement goodtime ncaa favorite followme goodlife nightlife 78lounge stpattysday theheavyhitterdjs budin dj djcoolv\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "--ORIGINAL--\n",
      "#birthdayparties #halloween #halloweenparty2019 #thecomplexnyc #birthday #bubbleball #bubbleballsoccer #halloweenpartylic #halloweenpartyastoria #bubbleballny #adults #bounce #astoria #lic #astoriaqueens #queens #birthdayparties #newyork #astoriaqueensny #astoriaqueens #queens #fun #kids #funkids #family #astoriasportscomplex #costumeparty #prizes #halloweenpartynyc #kidshalloweenparty #kidshalloweenparty2019 #halloweenpartyny #nychalloweenparty\n",
      "--GENERATED SEQUENCE (WITHOUT WORD ORDER)--\n",
      "birthdayparties funkids babysharkchallenge nerfgun halloweenpartynyc swag knockerballnyc halloweenpartynyc kids lic kidshalloweenparty2018 hocuspocustrivia animalshow magicshownyc costumeparty costumeparty funkids swimlessons funkids bubbleballsoccer bubbleballsoccer bubbleballsoccer halloweenpartyny astoriaqueensny bubbleballsoccer halloweenpartyny november bubblesoccer bubbleballsoccer indoors astoriaqueensny bubbleballnyc kidshalloweenparty2018\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for idx, doc in enumerate(docs[:5]):\n",
    "    print('--ORIGINAL--')\n",
    "    print(doc)\n",
    "    print('--GENERATED SEQUENCE (WITHOUT WORD ORDER)--')\n",
    "    words_count = len(doc.split())\n",
    "    words_ = np.random.choice(words, words_count, p=doc_vecs[idx].todense().tolist()[0])\n",
    "    for i in range(words_count):\n",
    "        word_idx = tfidf.vocabulary_[words_[i]]\n",
    "        words_[i] = np.random.choice(words, 1, p=word_replace_probs[word_idx])[0]\n",
    "    print(' '.join(words_))\n",
    "    print('-'*150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashtags magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = nyc_posts_authors_df.loc[nyc_posts_authors_df['hashtags'].str.len() > 50, 'hashtags'][:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3536335, 10212810)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtags = docs.str.split().tolist()\n",
    "\n",
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=5,\n",
    "                     vector_size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=10)\n",
    "w2v_model.build_vocab(hashtags, progress_per=1000)\n",
    "w2v_model.train(hashtags, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#tagforlikes', 0.747814416885376),\n",
       " ('#wedding', 0.7455034255981445),\n",
       " ('#like', 0.7374163866043091),\n",
       " ('#barmitzvah', 0.7349600791931152),\n",
       " ('#aniversary', 0.7308018803596497),\n",
       " ('#selfie', 0.7256142497062683),\n",
       " ('#foody', 0.7172373533248901),\n",
       " ('#picoftheday', 0.7157788276672363),\n",
       " ('#restaurants', 0.7054566740989685),\n",
       " ('#happy', 0.7051026225090027)]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('#newyork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#birthday', 0.9645171165466309),\n",
       " ('#aniversary', 0.9628538489341736),\n",
       " ('#restaurants', 0.9571098685264587),\n",
       " ('#barmitzvah', 0.954374372959137),\n",
       " ('#tagforlikes', 0.932794451713562),\n",
       " ('#sushi', 0.9251395463943481),\n",
       " ('#foody', 0.921452522277832),\n",
       " ('#like', 0.9046717882156372),\n",
       " ('#selfie', 0.885693371295929),\n",
       " ('#appetizer', 0.8824301958084106)]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('#wedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://blog.paperspace.com/implementing-levenshtein-distance-word-autocomplete-autocorrect/\n",
    "\n",
    "def levenshteinDistanceDP(token1, token2):\n",
    "    distances = numpy.zeros((len(token1) + 1, len(token2) + 1))\n",
    "\n",
    "    for t1 in range(len(token1) + 1):\n",
    "        distances[t1][0] = t1\n",
    "\n",
    "    for t2 in range(len(token2) + 1):\n",
    "        distances[0][t2] = t2\n",
    "        \n",
    "    a = 0\n",
    "    b = 0\n",
    "    c = 0\n",
    "    \n",
    "    for t1 in range(1, len(token1) + 1):\n",
    "        for t2 in range(1, len(token2) + 1):\n",
    "            if (token1[t1-1] == token2[t2-1]):\n",
    "                distances[t1][t2] = distances[t1 - 1][t2 - 1]\n",
    "            else:\n",
    "                a = distances[t1][t2 - 1]\n",
    "                b = distances[t1 - 1][t2]\n",
    "                c = distances[t1 - 1][t2 - 1]\n",
    "                \n",
    "                if (a <= b and a <= c):\n",
    "                    distances[t1][t2] = a + 1\n",
    "                elif (b <= a and b <= c):\n",
    "                    distances[t1][t2] = b + 1\n",
    "                else:\n",
    "                    distances[t1][t2] = c + 1\n",
    "\n",
    "#     printDistances(distances, len(token1), len(token2))\n",
    "    return distances[len(token1)][len(token2)]\n",
    "\n",
    "def kgram_overlap(word1, word2, k):\n",
    "    a = set([word1[i:i+k] for i in range(0, len(word1) - k + 1)])\n",
    "    b = set([word2[i:i+k] for i in range(0, len(word2) - k + 1)])\n",
    "    inter = len(a.intersection(b))\n",
    "    return inter / (len(a) + len(b) - inter + 1)\n",
    "\n",
    "def score(word1, word2):\n",
    "    return w2v_model.wv.similarity(word1, word2) - 0.5 * kgram_overlap(word1, word2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#newyork ['#happy']\n"
     ]
    }
   ],
   "source": [
    "# def get_replace(word, k=5, seed=None):\n",
    "#     repls = [(repl, score(word, repl)) for repl, _ in w2v_model.wv.most_similar(word, topn=k)]\n",
    "#     repls.sort(key=lambda x: x[1], reverse=True)\n",
    "#     rng = np.random.default_rng(seed)\n",
    "#     return rng.choice(repls, p=[score for _, score in repls])[0]\n",
    "\n",
    "# get_replace('#newyork')\n",
    "\n",
    "words = list(w2v_model.wv.key_to_index.keys())\n",
    "\n",
    "def exponential_gen(x, R, u, sensitivity=1, epsilon=25.4, seed=None):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    \n",
    "    # Calculate the score for each element of R\n",
    "    scores = [u(x, r) for r in R]\n",
    "    \n",
    "    # Calculate the probability for each element, based on its score\n",
    "    probabilities = [np.exp(epsilon * score / (2 * sensitivity)) for score in scores]\n",
    "    \n",
    "    # Normalize the probabilties so they sum to 1\n",
    "    probabilities = probabilities / np.linalg.norm(probabilities, ord=1)\n",
    "\n",
    "    # Choose an element from R based on the probabilities\n",
    "    return rng.choice(R, 1, p=probabilities)\n",
    "\n",
    "word = '#newyork'\n",
    "print(word, exponential_gen(word, words, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2242, 2242)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def exponential(x, R, u, sensitivity=1, epsilon=25.4, seed=None):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    # Calculate the score for each element of R\n",
    "    scores = [u(x, r) for r in R]\n",
    "    \n",
    "    # Calculate the probability for each element, based on its score\n",
    "    probabilities = [np.exp(epsilon * score / (2 * sensitivity)) for score in scores]\n",
    "    \n",
    "    # Normalize the probabilties so they sum to 1\n",
    "    probabilities = probabilities / np.linalg.norm(probabilities, ord=1)\n",
    "    \n",
    "    return probabilities\n",
    "\n",
    "word_replace_probs = []\n",
    "\n",
    "for idx, word in enumerate(words):\n",
    "    if idx % 1000 == 0:\n",
    "        print(idx)\n",
    "    word_replace_probs.append(exponential(word, words, score))\n",
    "\n",
    "word_replace_probs = np.array(word_replace_probs)\n",
    "word_replace_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--ORIGINAL--\n",
      "#bike #bikes #bikelife #shopride #shoplife #myfavoritebikeshop #bikeclub #bikenyc #bikeny #croton #crotonaqueduct #gravelgrinder #gravel #gravelride #offroad\n",
      "--GENERATED SEQUENCE (WITHOUT WORD ORDER)--\n",
      "#bike #cycling #movesale #shopride #shoplife #myfavoritebikeshop #bikeclub #bikenyc #bikeny #croton #crotonaqueduct #gravelgrinder #gravel #gravelride #offroad\n",
      "----------------------------------------------------------------------------------------------------\n",
      "--ORIGINAL--\n",
      "#76ers #raptors #blazers #nuggets #labatt #genesse #molson #nba #nbaplayoffs #nba2019 #basketball #hoops #nbafirstround #beerandhoops #brooklynsportsbar\n",
      "--GENERATED SEQUENCE (WITHOUT WORD ORDER)--\n",
      "#76ers #warriors #blazers #nuggets #labatt #genesse #molson #nbaleaguepass #nbaplayoffs #nba2019 #basketball #hoops #nbafirstround #nba2019 #brooklynsportsbar\n",
      "----------------------------------------------------------------------------------------------------\n",
      "--ORIGINAL--\n",
      "#comicartsbrooklyn #prattinstitute #gianthand #art #fun\n",
      "--GENERATED SEQUENCE (WITHOUT WORD ORDER)--\n",
      "#comicartsbrooklyn #prattinstitute #gianthand #art #ff\n",
      "----------------------------------------------------------------------------------------------------\n",
      "--ORIGINAL--\n",
      "#spendinglabordaylaboring #creativelylaboring #lovemyjob #howmisskatiespendsholidays\n",
      "--GENERATED SEQUENCE (WITHOUT WORD ORDER)--\n",
      "#spendinglabordaylaboring #creativelylaboring #lovemyjob #howmisskatiespendsholidays\n",
      "----------------------------------------------------------------------------------------------------\n",
      "--ORIGINAL--\n",
      "#rolex #custom #diamonds #nj #store #ship #unmated #value #textme\n",
      "--GENERATED SEQUENCE (WITHOUT WORD ORDER)--\n",
      "#rolex #quality #unmatched #nj #refrigeradora #rolex #unmated #unmatched #diamonds\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "\n",
    "for idx, doc in enumerate(docs[:5]):\n",
    "    print('--ORIGINAL--')\n",
    "    print(doc)\n",
    "    print('--GENERATED SEQUENCE (WITHOUT WORD ORDER)--')\n",
    "    words_ = doc.split()\n",
    "    for i in range(len(words_)):\n",
    "        if rng.random() < 0.5 and words_[i] in w2v_model.wv:\n",
    "            word_idx = w2v_model.wv.key_to_index[words_[i]]\n",
    "            words_[i] = rng.choice(words, 1, p=word_replace_probs[word_idx])[0]\n",
    "    print(' '.join(words_))\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attacker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/gkhayes/author_attribution CNN AA model\n",
    "\n",
    "https://github.com/yunitata/continuous-n-gram-AA code for CNN AA experiments reproduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = tokenizer.batch_decode(tokenizer(train_posts.text_clean.tolist(), return_tensors=\"pt\", max_length=128, padding='max_length', truncation=True).input_ids, skip_special_tokens=True)\n",
    "text_test = tokenizer.batch_decode(tokenizer(test_posts.text_clean.tolist(), return_tensors=\"pt\", max_length=128, padding='max_length', truncation=True).input_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_n_grams(excerpt_list, n, vocab_size, seq_size):\n",
    "    \"\"\"Create a list of n-gram sequences\n",
    "    \n",
    "    Args:\n",
    "    excerpt_list: list of strings. List of normalized text excerpts.\n",
    "    n: int. Length of n-grams.\n",
    "    vocab_size: int. Size of n-gram vocab (used in one-hot encoding)\n",
    "    seq_size: int. Size of n-gram sequences\n",
    "    \n",
    "    Returns:\n",
    "    n_gram_array: array. Numpy array of one-hot encoded n-grams.\n",
    "    \"\"\"\n",
    "    n_gram_list = []\n",
    "\n",
    "    for excerpt in excerpt_list:\n",
    "        # Remove spaces\n",
    "        excerpt = excerpt.replace(\" \", \"\")\n",
    "\n",
    "        # Extract n-grams\n",
    "        n_grams = [excerpt[i:i + n] for i in range(len(excerpt) - n + 1)]\n",
    "\n",
    "        # Convert to a single string with spaces between n-grams\n",
    "        new_string = \" \".join(n_grams)\n",
    "\n",
    "        # One hot encode\n",
    "        hot = one_hot(new_string, round(vocab_size*1.3))\n",
    "\n",
    "        # Pad hot if necessary\n",
    "        hot_len = len(hot)\n",
    "        if hot_len >= seq_size:\n",
    "            hot = hot[0:seq_size]\n",
    "        else:\n",
    "            diff = seq_size - hot_len\n",
    "            extra = [0]*diff\n",
    "            hot = hot + extra\n",
    "\n",
    "        n_gram_list.append(hot)\n",
    "    \n",
    "    n_gram_array = np.array(n_gram_list)\n",
    "    \n",
    "    return n_gram_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_size(excerpt_list, n, seq_size):\n",
    "    \"\"\"Calculate size of n-gram vocab\n",
    "    \n",
    "    Args:\n",
    "    excerpt_list: list of strings. List of normalized text excerpts.\n",
    "    n: int. Length of n-grams.\n",
    "    seq_size: int. Size of n-gram sequences\n",
    "    \n",
    "    Returns:\n",
    "    vocab_size: int. Size of n-gram vocab.\n",
    "    \"\"\"\n",
    "    n_gram_list = []\n",
    "\n",
    "    for excerpt in excerpt_list:\n",
    "        # Remove spaces\n",
    "        excerpt = excerpt.replace(\" \", \"\")\n",
    "\n",
    "        # Extract n-grams           \n",
    "        n_grams = [excerpt[i:i + n] for i in range(len(excerpt) - n + 1)]\n",
    "\n",
    "        # Create list of n-grams\n",
    "        gram_len = len(n_grams)\n",
    "        if gram_len >= seq_size:\n",
    "            n_grams = n_grams[0:seq_size]\n",
    "        else:\n",
    "            diff = seq_size - gram_len\n",
    "            extra = [0]*diff\n",
    "            n_grams = n_grams + extra\n",
    "        \n",
    "        n_gram_list.append(n_grams)\n",
    "    \n",
    "    # Flatten n-gram list\n",
    "    n_gram_list = list(np.array(n_gram_list).flat)\n",
    "    \n",
    "    # Calculate vocab size\n",
    "    n_gram_cnt = Counter(n_gram_list)\n",
    "    vocab_size = len(n_gram_cnt)\n",
    "    \n",
    "    return vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size for n = 3 is: 42643\n"
     ]
    }
   ],
   "source": [
    "i = 3\n",
    "vocab_size = get_vocab_size(text_train, i, 128)\n",
    "print('Vocab size for n =', i, 'is:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39035, 128)\n",
      "(1000, 128)\n"
     ]
    }
   ],
   "source": [
    "# Create n-gram lists\n",
    "gram3_train = create_n_grams(text_train, 3, vocab_size, 128)\n",
    "gram3_test = create_n_grams(text_test, 3, vocab_size, 128)\n",
    "\n",
    "print(np.shape(gram3_train))\n",
    "print(np.shape(gram3_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum encoding value for 3-grams is:  55434\n"
     ]
    }
   ],
   "source": [
    "max_3gram = np.max(gram3_train)\n",
    "\n",
    "print('Maximum encoding value for 3-grams is: ', max_3gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture in keras\n",
    "# Code reference: https://github.com/gkhayes/author_attribution\n",
    "def define_model(input_len, output_size, vocab_size, embedding_dim, verbose = True,\n",
    "                drop_out_pct = 0.25, conv_filters = 500, activation_fn = 'relu', pool_size = 2, learning = 0.0001):\n",
    "    \"\"\"Define n-gram CNN\n",
    "    \n",
    "    Args:\n",
    "    input_len: int. Length of input sequences.\n",
    "    output_size: int. Number of output classes.\n",
    "    vocab_size: int. Maximum value of n-gram encoding.\n",
    "    embedding_dim: int. Size of embedding layer.\n",
    "    verbose: bool. Whether or not to print model summary.\n",
    "    drop_out_pct: float. Drop-out rate.\n",
    "    conv_filters: int. Number of filters in the conv layer.\n",
    "    activation_fn: string. Activation function to use in the convolutional layer.\n",
    "    pool_size: int. Pool size for the max pooling layer.\n",
    "    learning: float. Learning rate for the model optimizer.\n",
    "    \n",
    "    Returns:\n",
    "    model: keras model object. \n",
    "    \"\"\"\n",
    "    # Channel 1\n",
    "    inputs1 = Input(shape = (input_len,))\n",
    "    embedding1 = Embedding(vocab_size, embedding_dim)(inputs1)\n",
    "    drop1 = Dropout(drop_out_pct)(embedding1)\n",
    "    conv1 = Conv1D(filters = conv_filters, kernel_size = 3, activation = activation_fn)(drop1)\n",
    "    pool1 = MaxPooling1D(pool_size = pool_size)(conv1)\n",
    "    flat1 = Flatten()(pool1)\n",
    "    \n",
    "    # Channel 2\n",
    "    inputs2 = Input(shape = (input_len,))\n",
    "    embedding2 = Embedding(vocab_size, embedding_dim)(inputs2)\n",
    "    drop2 = Dropout(drop_out_pct)(embedding2)\n",
    "    conv2 = Conv1D(filters = conv_filters, kernel_size = 4, activation = activation_fn)(drop2)\n",
    "    pool2 = MaxPooling1D(pool_size = pool_size)(conv2)\n",
    "    flat2 = Flatten()(pool2)\n",
    "\n",
    "    # Channel 3\n",
    "    inputs3 = Input(shape = (input_len,))\n",
    "    embedding3= Embedding(vocab_size, embedding_dim)(inputs3)\n",
    "    drop3 = Dropout(drop_out_pct)(embedding3)\n",
    "    conv3 = Conv1D(filters = conv_filters, kernel_size = 5, activation = activation_fn)(drop3)\n",
    "    pool3 = MaxPooling1D(pool_size = pool_size)(conv3)\n",
    "    flat3 = Flatten()(pool3)\n",
    "    \n",
    "    # Merge channels\n",
    "    merged = Concatenate()([flat1, flat2, flat3])\n",
    "    \n",
    "    # Create output layer\n",
    "    output = Dense(output_size, activation = 'softmax')(merged)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs = [inputs1, inputs2, inputs3], outputs = output)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = Adam(lr = learning), metrics=['accuracy'])\n",
    "    \n",
    "    if verbose:\n",
    "        print(model.summary())\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = preprocessing.LabelEncoder()\n",
    "\n",
    "author_train = lb.fit_transform(train_posts.owner_id.values)\n",
    "author_train_hot = pd.get_dummies(author_train).values\n",
    "author_test = lb.transform(test_posts.owner_id.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 128, 600)     33261000    ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 128, 600)     33261000    ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 128, 600)     33261000    ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 128, 600)     0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 128, 600)     0           ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 128, 600)     0           ['embedding_2[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 126, 500)     900500      ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 125, 500)     1200500     ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 124, 500)     1500500     ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 63, 500)      0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 62, 500)     0           ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 62, 500)     0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 31500)        0           ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 31000)        0           ['max_pooling1d_1[0][0]']        \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 31000)        0           ['max_pooling1d_2[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 93500)        0           ['flatten[0][0]',                \n",
      "                                                                  'flatten_1[0][0]',              \n",
      "                                                                  'flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 50)           4675050     ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 108,059,550\n",
      "Trainable params: 108,059,550\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create the 3-gram model\n",
    "gram3_model = define_model(128, len(train_posts.owner_id.unique()), max_3gram + 1, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gram3_model.fit([gram3_train, gram3_train, gram3_train], author_train_hot, epochs=15, batch_size=32, \n",
    "                verbose = 1, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and evaluate Model 1 (3-gram CNN)\n",
    "\n",
    "# t0 = time.time()\n",
    "\n",
    "# # Fit model\n",
    "# model1 = define_model(128, len(train_posts.authorid.unique()), max_3gram + 1, 300)\n",
    "# model1.fit([gram3_train, gram3_train, gram3_train], author_train_hot, epochs=10, batch_size=32, \n",
    "#            verbose = 1, validation_split = 0.2)\n",
    "t1 = time.time()\n",
    "\n",
    "# Predict values for test set\n",
    "author_pred1 = gram3_model.predict([gram3_test, gram3_test, gram3_test]).argmax(-1)\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "# Evaluate\n",
    "accuracy = balanced_accuracy_score(author_test, author_pred1)\n",
    "precision, recall, f1, support = score(author_test, author_pred1, average='weighted')\n",
    "confusion = confusion_matrix(author_test, author_pred1)\n",
    "    \n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Ave. Precision:\", precision)\n",
    "print(\"Ave. Recall:\", recall)\n",
    "print(\"Ave. F1 Score:\", f1)\n",
    "# print(\"Training Time:\", (t1 - t0), \"seconds\")\n",
    "print(\"Prediction Time:\", (t2 - t1), \"seconds\")\n",
    "print(\"Confusion Matrix:\\n\", confusion)\n",
    "\n",
    "with open('/home/jovyan/notebooks/vk/attacker_vk.txt', 'w') as f:\n",
    "    print(\"Accuracy:\", accuracy, file=f)\n",
    "    print(\"Ave. Precision:\", precision, file=f)\n",
    "    print(\"Ave. Recall:\", recall, file=f)\n",
    "    print(\"Ave. F1 Score:\", f1, file=f)\n",
    "    print(\"Confusion Matrix:\\n\", confusion, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_train = train_texts_anon['text_clean_anon']\n",
    "trans_test = test_texts_anon['text_clean_anon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_vocab_size = get_vocab_size(trans_train, 3, 128)\n",
    "trans_gram3_train = create_n_grams(trans_train, 3, trans_vocab_size, 128)\n",
    "trans_max_3gram = np.max(trans_gram3_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 3-gram model\n",
    "trans_gram3_model = define_model(128, len(train_posts.authorid.unique()), trans_max_3gram + 1, 600)\n",
    "trans_gram3_model.fit([trans_gram3_train, trans_gram3_train, trans_gram3_train], author_train_hot, epochs=15, batch_size=32, \n",
    "                verbose = 1, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_values = author_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = gram3_model.predict([gram3_test, gram3_test, gram3_test]).argmax(-1)\n",
    "\n",
    "trans_vocab_size = get_vocab_size(trans_train, 3, 128)\n",
    "trans_gram3_test = create_n_grams(trans_test, 3, trans_vocab_size, 128)\n",
    "trans_preds = trans_gram3_model.predict([trans_gram3_test, trans_gram3_test, trans_gram3_test]).argmax(-1)\n",
    "\n",
    "vocab_size = get_vocab_size(text_train, 3, 128)\n",
    "orig_trans_gram3_test = create_n_grams(trans_test, 3, vocab_size, 128)\n",
    "orig_trans_preds = gram3_model.predict([trans_gram3_test, trans_gram3_test, trans_gram3_test]).argmax(-1)\n",
    "\n",
    "print('Weighted scores')\n",
    "print('F1 score for original text:', f1_score(true_values, preds, average='weighted'))\n",
    "print('F1 score for transformed text with transformed model:', f1_score(true_values, trans_preds, average='weighted'))\n",
    "print('F1 score for transformed text with orig model:', f1_score(true_values, orig_trans_preds, average='weighted'))\n",
    "print()\n",
    "print('Macro averaged scores')\n",
    "print('F1 score for original text:', f1_score(true_values, preds, average='macro'))\n",
    "print('F1 score for transformed text with transformed model:', f1_score(true_values, trans_preds, average='macro'))\n",
    "print('F1 score for transformed text with orig model:', f1_score(true_values, orig_trans_preds, average='macro'))\n",
    "\n",
    "with open('/home/jovyan/notebooks/vk/attacker_tests_vk.txt', 'w') as f:\n",
    "    print('Weighted scores', file=f)\n",
    "    print('F1 score for original text:', f1_score(true_values, preds, average='weighted'), file=f)\n",
    "    print('F1 score for transformed text with transformed model:', f1_score(true_values, trans_preds, average='weighted'), file=f)\n",
    "    print('F1 score for transformed text with orig model:', f1_score(true_values, orig_trans_preds, average='weighted'), file=f)\n",
    "    print('', file=f)\n",
    "    print('Macro averaged scores', file=f)\n",
    "    print('F1 score for original text:', f1_score(true_values, preds, average='macro'), file=f)\n",
    "    print('F1 score for transformed text with transformed model:', f1_score(true_values, trans_preds, average='macro'), file=f)\n",
    "    print('F1 score for transformed text with orig model:', f1_score(true_values, orig_trans_preds, average='macro'), file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('checkpoints_vk_distilbert/best2.txt', 'r', encoding='utf-8') as f:\n",
    "    best_model_path = f.readlines()[0]\n",
    "data = None\n",
    "model = LitERAE.load_from_checkpoint(checkpoint_path=best_model_path, data=data, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "vk_sentiment_pos_df = pd.read_csv('/home/jovyan/notebooks/twitter/positive.csv', usecols=['tdate', 'tname', 'ttext', 'ttype'])\n",
    "vk_sentiment_pos_df['tdate'] = vk_sentiment_pos_df['tdate'].apply(datetime.fromtimestamp)\n",
    "\n",
    "vk_sentiment_neg_df = pd.read_csv('/home/jovyan/notebooks/twitter/negative.csv', usecols=['tdate', 'tname', 'ttext', 'ttype'])\n",
    "vk_sentiment_neg_df['tdate'] = vk_sentiment_neg_df['tdate'].apply(datetime.fromtimestamp)\n",
    "\n",
    "vk_sentiment_df = pd.concat([vk_sentiment_pos_df, vk_sentiment_neg_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_cond = (vk_sentiment_df['tdate'] >= datetime(2013, 12, 6)) & (vk_sentiment_df['tdate'] < datetime(2013, 12, 7))\n",
    "vk_sentiment_df = vk_sentiment_df[subset_cond]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lb = LabelEncoder()\n",
    "vk_sentiment_df['target'] = lb.fit_transform(vk_sentiment_df['ttype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "vk_sentiment_df['text'] = vk_sentiment_df['ttext'].apply(apply_clean)\n",
    "vk_sentiment_df[['only_text', 'hashtags']] = vk_sentiment_df.apply(get_text_and_hashtags, axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "vk_sentiment_df['anonymized_text'] = predict_batch(model, vk_sentiment_df['only_text'].tolist(), seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentiment, test_sentiment = train_test_split(vk_sentiment_df, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentiment.to_csv('/home/jovyan/notebooks/twitter/train_sentiment.csv')\n",
    "test_sentiment.to_csv('/home/jovyan/notebooks/twitter/test_sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentiment = pd.read_csv('/home/jovyan/notebooks/twitter/train_sentiment.csv')\n",
    "test_sentiment = pd.read_csv('/home/jovyan/notebooks/twitter/test_sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd4d794c25b443ea0ed10110080c2c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)okenizer_config.json:   0%|          | 0.00/39.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f809d7e8a0a64f81809e757ff138796b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/953 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a134e6d8f434e8d84cedd43d02304ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)solve/main/vocab.txt:   0%|          | 0.00/872k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2360d6c3d804ba8869373d622c95e84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentiment_model_name = 'nlptown/bert-base-multilingual-uncased-sentiment'\n",
    "\n",
    "num_labels = train_sentiment.target.nunique()\n",
    "\n",
    "sentiment_tokenizer = AutoTokenizer.from_pretrained(sentiment_model_name)\n",
    "\n",
    "def sentiment_model_init(model_name=sentiment_model_name, num_labels=num_labels):\n",
    "    return AutoModelForSequenceClassification.from_pretrained(sentiment_model_name, num_labels=num_labels, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, targets, tokenizer):\n",
    "        self.inputs = tokenizer(texts, return_tensors=\"pt\", max_length=128, padding='max_length', truncation=True)\n",
    "        self.outputs = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.outputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.inputs['input_ids'][idx],\n",
    "            'token_type_ids': self.inputs['token_type_ids'][idx],\n",
    "            'attention_mask': self.inputs['attention_mask'][idx],\n",
    "            'targets': self.outputs[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_BATCH_SIZE = 8\n",
    "\n",
    "class LitSentiment(pl.LightningModule):\n",
    "    def __init__(self, model_init, train, test, learning_rate=1e-4):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # We hardcode dataset specific stuff here.\n",
    "        self.train_dataset = train\n",
    "        self.test_dataset = test\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = model_init()\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        return self.model(**inputs)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = {a: b for a, b in batch.items() if a != 'targets'}\n",
    "        y = batch['targets']\n",
    "        outputs = self(**x)\n",
    "        logits = outputs.logits\n",
    "        loss = self.loss_func(logits, y)\n",
    "        \n",
    "        f1 = f1_score(y.cpu(), logits.cpu().argmax(dim=-1), average='macro')\n",
    "        \n",
    "        self.log(f'train_loss', loss)\n",
    "        self.log(f'avg_train_loss', loss, on_step=False, on_epoch=True)\n",
    "        self.log(f'train_f1', f1)\n",
    "        self.log(f'avg_train_f1', f1, on_step=False, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = {a: b for a, b in batch.items() if a != 'targets'}\n",
    "        y = batch['targets']\n",
    "        outputs = self(**x)\n",
    "        logits = outputs.logits\n",
    "        loss = self.loss_func(logits, y)\n",
    "        \n",
    "        f1 = f1_score(y.cpu(), logits.cpu().argmax(dim=-1), average='macro')\n",
    "        \n",
    "        self.log(f'val_loss', loss)\n",
    "        self.log(f'avg_val_loss', loss, on_step=False, on_epoch=True)\n",
    "        self.log(f'val_f1', f1)\n",
    "        self.log(f'avg_val_f1', f1, on_step=False, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x = {a: b for a, b in batch.items() if a != 'targets'}\n",
    "        y = batch['targets']\n",
    "        outputs = self(**x)\n",
    "        logits = outputs.logits\n",
    "        loss = self.loss_func(logits, y)\n",
    "        \n",
    "        f1 = f1_score(y.cpu(), logits.cpu().argmax(dim=-1), average='macro')\n",
    "        \n",
    "        self.log(f'test_loss', loss)\n",
    "        self.log(f'avg_test_loss', loss, on_step=False, on_epoch=True)\n",
    "        self.log(f'test_f1', f1)\n",
    "        self.log(f'avg_test_f1', f1, on_step=False, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def on_train_batch_end(self, outputs, batch, batch_idx):\n",
    "        metrics = self.trainer.callback_metrics\n",
    "#         logger.info(f'Batch train loss {metrics}')\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        metrics = self.trainer.callback_metrics\n",
    "        print(f'Train loss: {metrics[\"avg_train_loss\"]}')\n",
    "        print(f'Train f1: {metrics[\"avg_train_f1\"]}')\n",
    "\n",
    "    def on_validation_batch_end(self, outputs, batch, batch_idx):\n",
    "        metrics = self.trainer.callback_metrics\n",
    "#         logger.info(f'Batch validation loss {metrics}')\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        metrics = self.trainer.callback_metrics\n",
    "        print(f'Val loss: {metrics[\"avg_val_loss\"]}')\n",
    "        print(f'Val f1: {metrics[\"avg_val_f1\"]}')\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        metrics = self.trainer.callback_metrics\n",
    "        print(f'Test loss: {metrics[\"avg_test_loss\"]}')\n",
    "        print(f'Test f1: {metrics[\"avg_test_f1\"]}')\n",
    "\n",
    "    ####################\n",
    "    # DATA RELATED HOOKS\n",
    "    ####################\n",
    "\n",
    "#     def prepare_data(self):\n",
    "#         self.data = nn.utils.rnn.pad_sequence(self.data)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            train_size = int(0.9 * len(self.train_dataset))\n",
    "            val_size = len(self.train_dataset) - train_size\n",
    "            self.data_train, self.data_val = random_split(self.train_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.data_test = self.test_dataset\n",
    "\n",
    "        self.loss_func = nn.CrossEntropyLoss() # forgot to add ignore_index for BERT\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.data_train, batch_size=BERT_BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.data_val, batch_size=BERT_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.data_test, batch_size=BERT_BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "chechpoint_path_sentiment = \"checkpoints_ru_sentiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlptown/bert-base-multilingual-uncased-sentiment and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([5, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([5]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                          | Params\n",
      "------------------------------------------------------------\n",
      "0 | model     | BertForSequenceClassification | 167 M \n",
      "1 | loss_func | CrossEntropyLoss              | 0     \n",
      "------------------------------------------------------------\n",
      "167 M     Trainable params\n",
      "0         Non-trainable params\n",
      "167 M     Total params\n",
      "669.432   Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8918ab3bcff4d1ea7ad3a09a144b25c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.32891377806663513\n",
      "Val f1: 0.6737083312425782\n",
      "Train loss: 0.35420048236846924\n",
      "Train f1: 0.6251215586409684\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.3041144907474518\n",
      "Val f1: 0.7129141786676035\n",
      "Train loss: 0.28593504428863525\n",
      "Train f1: 0.6917142796868747\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.3173080384731293\n",
      "Val f1: 0.6830140939729983\n",
      "Train loss: 0.20676544308662415\n",
      "Train f1: 0.8051538357044588\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.4246973991394043\n",
      "Val f1: 0.6742362660170882\n",
      "Train loss: 0.13005143404006958\n",
      "Train f1: 0.8785016894633761\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5860643982887268\n",
      "Val f1: 0.6058249208934143\n",
      "Train loss: 0.0751800686120987\n",
      "Train f1: 0.9329830337696376\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SentimentDataset(train_sentiment.text.tolist(), train_sentiment.target.tolist(), sentiment_tokenizer)\n",
    "test_dataset = SentimentDataset(test_sentiment.text.tolist(), test_sentiment.target.tolist(), sentiment_tokenizer)\n",
    "sentiment_pl = LitSentiment(sentiment_model_init, train_dataset, test_dataset, 2e-5)\n",
    "sentiment_pl.train()\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(dirpath=chechpoint_path_sentiment, save_top_k=2, monitor=\"val_loss\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=5,\n",
    "    num_nodes=1,\n",
    "    num_sanity_val_steps=0,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "trainer.fit(sentiment_pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /home/jovyan/notebooks/anonymization/checkpoints_ru_sentiment/epoch=1-step=1972.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /home/jovyan/notebooks/anonymization/checkpoints_ru_sentiment/epoch=1-step=1972.ckpt\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c6af584d5946aaa8e5053d39982321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.30249857902526855\n",
      "Test f1: 0.6790496749428991\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "       Test metric             DataLoader 0\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "       avg_test_f1          0.6790496749428991\n",
      "      avg_test_loss         0.30249857902526855\n",
      "         test_f1            0.6790496749428991\n",
      "        test_loss           0.30249857902526855\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n"
     ]
    }
   ],
   "source": [
    "sentiment_pl.stage = 'test'\n",
    "sentiment_pl.eval()\n",
    "\n",
    "results = trainer.test(sentiment_pl, ckpt_path='best')\n",
    "with open('/home/jovyan/notebooks/vk/sentiment_original.txt', 'w') as f:\n",
    "    print(results, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlptown/bert-base-multilingual-uncased-sentiment and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([5, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([5]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /home/jovyan/notebooks/anonymization/lightning_logs\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/jovyan/notebooks/anonymization/checkpoints_ru_sentiment exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                          | Params\n",
      "------------------------------------------------------------\n",
      "0 | model     | BertForSequenceClassification | 167 M \n",
      "1 | loss_func | CrossEntropyLoss              | 0     \n",
      "------------------------------------------------------------\n",
      "167 M     Trainable params\n",
      "0         Non-trainable params\n",
      "167 M     Total params\n",
      "669.432   Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5817757f7e346548ce8acecd9806f12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.34638315439224243\n",
      "Val f1: 0.6643685082041251\n",
      "Train loss: 0.3769320845603943\n",
      "Train f1: 0.6378467341674641\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.32824310660362244\n",
      "Val f1: 0.6748858447488589\n",
      "Train loss: 0.35599154233932495\n",
      "Train f1: 0.6356911125751514\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.3394582271575928\n",
      "Val f1: 0.6785789552912845\n",
      "Train loss: 0.31508028507232666\n",
      "Train f1: 0.6521806198537979\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.4136177897453308\n",
      "Val f1: 0.6738311916394112\n",
      "Train loss: 0.24240928888320923\n",
      "Train f1: 0.7623966605443407\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.44836264848709106\n",
      "Val f1: 0.6729723853011527\n",
      "Train loss: 0.14990410208702087\n",
      "Train f1: 0.8664648723517047\n"
     ]
    }
   ],
   "source": [
    "anon_train_dataset = SentimentDataset(train_sentiment.anonymized_text.tolist(), train_sentiment.target.tolist(), sentiment_tokenizer)\n",
    "anon_test_dataset = SentimentDataset(test_sentiment.anonymized_text.tolist(), test_sentiment.target.tolist(), sentiment_tokenizer)\n",
    "sentiment_pl = LitSentiment(sentiment_model_init, anon_train_dataset, anon_test_dataset, 2e-5)\n",
    "sentiment_pl.train()\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(dirpath=chechpoint_path_sentiment, save_top_k=2, monitor=\"val_loss\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=5,\n",
    "    num_nodes=1,\n",
    "    num_sanity_val_steps=0,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "trainer.fit(sentiment_pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /home/jovyan/notebooks/anonymization/checkpoints_ru_sentiment/epoch=1-step=1972-v2.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /home/jovyan/notebooks/anonymization/checkpoints_ru_sentiment/epoch=1-step=1972-v2.ckpt\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e68c4b86dbae4126b9680da9bf20ec18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.3402031660079956\n",
      "Test f1: 0.6381162984859087\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "       Test metric             DataLoader 0\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "       avg_test_f1          0.6381162984859087\n",
      "      avg_test_loss         0.3402031660079956\n",
      "         test_f1            0.6381162984859087\n",
      "        test_loss           0.3402031660079956\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n"
     ]
    }
   ],
   "source": [
    "sentiment_pl.stage = 'test'\n",
    "sentiment_pl.eval()\n",
    "\n",
    "results = trainer.test(sentiment_pl, ckpt_path='best')\n",
    "with open('/home/jovyan/notebooks/vk/sentiment_anonymized.txt', 'w') as f:\n",
    "    print(results, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
