{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Может получиться найти модель, которая умеет лучше определять автора текста (0.26 слишком мало) | Done\n",
    "\n",
    "Хештеги:\n",
    "Берем берт, вычисляем эмбеддинги для всех предложений, усредняем по хештегам. Для каждого предложения находим эмбеддинг каждого токена. Замену ищем из ближайших по косинусному расстоянию, но также не близких по расстоянию левенштайна.\n",
    "\n",
    "Посчитать метрики берта на задачах MLM и NSP для оригинальных текстов и анонимизированных.\n",
    "Посчитать метрики на задаче оценки тональности. Датасет для инсты может Миша предоставить, есть в инете также по твиттеру.\n",
    "\n",
    "Как оставить упоминания других пользователей? Заменить их в тексте на <MASK> например, потом вставить на место токена упоминание пользователя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "eng_stopwords = stopwords.words('english')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from gensim.models import fasttext as ft, Word2Vec\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "import fasttext\n",
    "\n",
    "from cleantext import clean\n",
    "\n",
    "from transformers import (\n",
    "    AutoModel, \n",
    "    AutoModelForMaskedLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from transformers import pipeline\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, Embedding\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint \n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import re\n",
    "from itertools import chain, islice\n",
    "import logging\n",
    "import os\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "from utils import apply_clean, get_text_and_hashtags\n",
    "\n",
    "CORES = 10\n",
    "\n",
    "SEED = 42\n",
    "TRAIN_DOC_COUNT = 10000\n",
    "TEST_DOC_COUNT = 1000\n",
    "AUTHOR_COUNT = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>post_id</th>\n",
       "      <th>owner_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-02 18:58:00</td>\n",
       "      <td>11205</td>\n",
       "      <td>84</td>\n",
       "      <td>Крутейшие Неоновые маски. Успей заказать первы...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-02-03 20:55:00</td>\n",
       "      <td>12084</td>\n",
       "      <td>84</td>\n",
       "      <td>Рекомендую! Отличная вещь. Видеорегистратор Xi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-02-04 15:57:00</td>\n",
       "      <td>12086</td>\n",
       "      <td>84</td>\n",
       "      <td>Крутейший набор для рукоделия.     &lt;a href=\"/a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-02-05 11:16:00</td>\n",
       "      <td>12087</td>\n",
       "      <td>84</td>\n",
       "      <td>Кошелек игровая консоль PlayStation ретро.  &lt;i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-02-06 19:17:14</td>\n",
       "      <td>12091</td>\n",
       "      <td>84</td>\n",
       "      <td>Предоставлю отчёт по Гос номеру автомобиля: ДТ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date  post_id  owner_id  \\\n",
       "0  2019-01-02 18:58:00    11205        84   \n",
       "1  2019-02-03 20:55:00    12084        84   \n",
       "2  2019-02-04 15:57:00    12086        84   \n",
       "3  2019-02-05 11:16:00    12087        84   \n",
       "4  2019-02-06 19:17:14    12091        84   \n",
       "\n",
       "                                                text  \n",
       "0  Крутейшие Неоновые маски. Успей заказать первы...  \n",
       "1  Рекомендую! Отличная вещь. Видеорегистратор Xi...  \n",
       "2  Крутейший набор для рукоделия.     <a href=\"/a...  \n",
       "3  Кошелек игровая консоль PlayStation ретро.  <i...  \n",
       "4  Предоставлю отчёт по Гос номеру автомобиля: ДТ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_df = pd.read_csv('/mnt/ess_storage/DN_1/storage/home/vpanov/vk/vk_spb_posts_2019.csv')\n",
    "posts_df = posts_df.dropna(axis=0)\n",
    "posts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "482067"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(posts_df['owner_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227620       1175\n",
       "371727932    1174\n",
       "181072430    1172\n",
       "1378978      1171\n",
       "36010941     1170\n",
       "             ... \n",
       "309131878    1031\n",
       "478126947    1030\n",
       "176091683    1026\n",
       "289137251    1026\n",
       "393151306    1024\n",
       "Name: owner_id, Length: 100, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offset = 500\n",
    "posts_df['owner_id'].value_counts()[offset:offset+AUTHOR_COUNT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>post_id</th>\n",
       "      <th>owner_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>989187</th>\n",
       "      <td>2019-10-28 00:00:00</td>\n",
       "      <td>49664</td>\n",
       "      <td>134312775</td>\n",
       "      <td>Ты говоришь лететь за мёдом \\nв далёкий космос...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1287802</th>\n",
       "      <td>2019-03-23 00:00:00</td>\n",
       "      <td>26829</td>\n",
       "      <td>161384085</td>\n",
       "      <td>Смотрите \"Наставление за шаг от смерти! Сохран...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8066156</th>\n",
       "      <td>2019-04-13 05:31:00</td>\n",
       "      <td>4000</td>\n",
       "      <td>106327572</td>\n",
       "      <td>Я набрал 141800 очков на 2329 уровне. Присоеди...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1777838</th>\n",
       "      <td>2019-03-23 00:00:00</td>\n",
       "      <td>31216</td>\n",
       "      <td>338518394</td>\n",
       "      <td>После этого Рецепта Вы Полюбите Печень. Курина...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502411</th>\n",
       "      <td>2019-09-09 00:00:00</td>\n",
       "      <td>5862</td>\n",
       "      <td>36010941</td>\n",
       "      <td>Ракета НОЛЬ 2609 СССР\\nVintage soviet USSR wat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        date  post_id   owner_id  \\\n",
       "989187   2019-10-28 00:00:00    49664  134312775   \n",
       "1287802  2019-03-23 00:00:00    26829  161384085   \n",
       "8066156  2019-04-13 05:31:00     4000  106327572   \n",
       "1777838  2019-03-23 00:00:00    31216  338518394   \n",
       "502411   2019-09-09 00:00:00     5862   36010941   \n",
       "\n",
       "                                                      text  \n",
       "989187   Ты говоришь лететь за мёдом \\nв далёкий космос...  \n",
       "1287802  Смотрите \"Наставление за шаг от смерти! Сохран...  \n",
       "8066156  Я набрал 141800 очков на 2329 уровне. Присоеди...  \n",
       "1777838  После этого Рецепта Вы Полюбите Печень. Курина...  \n",
       "502411   Ракета НОЛЬ 2609 СССР\\nVintage soviet USSR wat...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_productive_authors = posts_df['owner_id'].value_counts()[offset:offset+AUTHOR_COUNT].index.values\n",
    "posts_authors_df = posts_df[posts_df.owner_id.isin(most_productive_authors)].sample(frac=1.0, random_state=SEED)\n",
    "posts_authors_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_authors_df[['only_text', 'hashtags']] = posts_authors_df.apply(get_text_and_hashtags, axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_authors_df['text_clean'] = posts_authors_df.only_text.apply(lambda x: apply_clean(x, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_space(text):\n",
    "    return re.sub(r'([,.!\\'\\\"@<>\\\\\\[\\]\\(\\)#\\$%^&\\*;:])', ' \\1', text)\n",
    "\n",
    "posts_authors_df['text2'] = posts_authors_df['text_clean'].apply(input_space)\n",
    "\n",
    "count_threshold = 5\n",
    "\n",
    "v = posts_authors_df['text2'].str.split().tolist()\n",
    "c = Counter(chain.from_iterable(v))\n",
    "posts_authors_df['text_clean'] = [' '.join([j for j in i if c[j] > count_threshold]) for i in v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_authors = 50\n",
    "least_long = 50\n",
    "max_posts_per_user = 800\n",
    "\n",
    "long_posts = posts_authors_df[posts_authors_df.text_clean.str.len() > least_long]\n",
    "authors_posts_count = long_posts.owner_id.value_counts()\n",
    "authors = authors_posts_count[:max_authors].index.tolist()\n",
    "min_posts = authors_posts_count.values[max_authors - 1]\n",
    "median_posts = int(authors_posts_count.median())\n",
    "\n",
    "train_posts = []\n",
    "test_posts = []\n",
    "\n",
    "for i in authors:\n",
    "    author_i_posts = posts_authors_df[(posts_authors_df.text_clean.str.len() > least_long) & (posts_authors_df.owner_id == i)]\n",
    "    l = len(author_i_posts)\n",
    "    train_posts.append(author_i_posts[:min(max_posts_per_user, l - 20)])\n",
    "    test_posts.append(author_i_posts[l - 20:])\n",
    "\n",
    "train_posts = pd.concat(train_posts).sample(frac=1.0, random_state=SEED)\n",
    "test_posts = pd.concat(test_posts).sample(frac=1.0, random_state=SEED)\n",
    "\n",
    "train_posts.to_csv('/home/jovyan/notebooks/vk/train_posts.csv', index=None)\n",
    "test_posts.to_csv('/home/jovyan/notebooks/vk/test_posts.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_posts = pd.read_csv('/home/jovyan/notebooks/vk/train_posts.csv')\n",
    "test_posts = pd.read_csv('/home/jovyan/notebooks/vk/test_posts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text anonymization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SynTF method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_SYNSETS = False\n",
    "TEXT_LENGTH = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3620\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3621\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3622\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-09737ae7c995>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moriginal_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnyc_posts_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnyc_posts_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3503\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3504\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3505\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3506\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3507\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3621\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3622\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3623\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3624\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3625\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "original_docs = nyc_posts_df.loc[nyc_posts_df['text'].str.len() > 100, 'text'][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'@\\w+', ' ', text.lower())\n",
    "    text = re.sub(r'[^a-zA-Z0-9]', ' ', text)\n",
    "    words = text.split()\n",
    "    words = filter(lambda x: x not in eng_stopwords, words)\n",
    "    return ' '.join(lemmatizer.lemmatize(x) for x in words)\n",
    "\n",
    "def get_synonyms(uniq_words):\n",
    "    all_synonyms = set()\n",
    "    for word in uniq_words:\n",
    "        synonyms = wordnet.synsets(word)\n",
    "        all_synonyms.update(chain.from_iterable([word.lemma_names() for word in synonyms]))\n",
    "    return all_synonyms\n",
    "\n",
    "docs = original_docs.apply(preprocess).tolist()\n",
    "if USE_SYNSETS:\n",
    "    uniq_words = set(chain.from_iterable([doc.split() for doc in docs]))\n",
    "    synonyms = ' '.join(get_synonyms(uniq_words))\n",
    "    docs_with_synonyms = [*docs, synonyms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "if USE_SYNSETS:\n",
    "    tfidf.fit(docs_with_synonyms)\n",
    "else:\n",
    "    tfidf.fit(docs)\n",
    "doc_vecs = tfidf.transform(docs)\n",
    "doc_vecs = normalize(doc_vecs, norm='l1')\n",
    "words = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7458"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = fasttext.load_facebook_model(datapath('/mnt/ess_storage/DN_1/storage/home/vpanov/cc.en.300.bin.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7458, 7458)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vecs = [ft.wv[word] for word in words]\n",
    "word_similarities = cosine_similarity(word_vecs, word_vecs)\n",
    "word_similarities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kgram_overlap(word1, word2, k):\n",
    "    a = set([word1[i:i+k] for i in range(0, len(word1) - k + 1)])\n",
    "    b = set([word2[i:i+k] for i in range(0, len(word2) - k + 1)])\n",
    "    inter = len(a.intersection(b))\n",
    "    return inter / (len(a) + len(b) - inter)\n",
    "\n",
    "def score(word1, word2):\n",
    "    idx1, idx2 = tfidf.vocabulary_[word1], tfidf.vocabulary_[word2]\n",
    "    return word_similarities[idx1, idx2] - 0.3 * kgram_overlap(word1, word2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://programming-dp.com/notebooks/ch9.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unforgettable ['unedited']\n"
     ]
    }
   ],
   "source": [
    "def exponential_gen(x, R, u, sensitivity=1, epsilon=25.4):\n",
    "    # Calculate the score for each element of R\n",
    "    scores = [u(x, r) for r in R]\n",
    "    \n",
    "    # Calculate the probability for each element, based on its score\n",
    "    probabilities = [np.exp(epsilon * score / (2 * sensitivity)) for score in scores]\n",
    "    \n",
    "    # Normalize the probabilties so they sum to 1\n",
    "    probabilities = probabilities / np.linalg.norm(probabilities, ord=1)\n",
    "\n",
    "    # Choose an element from R based on the probabilities\n",
    "    return np.random.choice(R, 1, p=probabilities)\n",
    "\n",
    "num = 7000\n",
    "print(words[num], exponential_gen(words[num], words, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3342ecb536a0454d9452dbfbf1e3cd3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=7458.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7458, 7458)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def exponential(x, R, u, sensitivity=1, epsilon=25.4):\n",
    "    # Calculate the score for each element of R\n",
    "    scores = [u(x, r) for r in R]\n",
    "    \n",
    "    # Calculate the probability for each element, based on its score\n",
    "    probabilities = [np.exp(epsilon * score / (2 * sensitivity)) for score in scores]\n",
    "    \n",
    "    # Normalize the probabilties so they sum to 1\n",
    "    probabilities = probabilities / np.linalg.norm(probabilities, ord=1)\n",
    "    \n",
    "    return probabilities\n",
    "\n",
    "word_replace_probs = []\n",
    "\n",
    "for word in tqdm(words):\n",
    "    word_replace_probs.append(exponential(word, words, score))\n",
    "\n",
    "word_replace_probs = np.array(word_replace_probs)\n",
    "word_replace_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--ORIGINAL--\n",
      "Right or Left?\n",
      "\n",
      "Buffalo Chicken slice on the right and Chicken, Bacon(beef), Ranch on the left from @saucny, all new halal pizza spot in New Hyde Park.\n",
      "--PREPROCESSED--\n",
      "right left buffalo chicken slice right chicken bacon beef ranch left new halal pizza spot new hyde park\n",
      "--GENERATED SEQUENCE (WITHOUT WORD ORDER)--\n",
      "freshwater bend fish horn lyndhurst cabbage placed rice priestley proof goat rippon bacon inside pork nearby spot though\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "--ORIGINAL--\n",
      "Poor Cyndy!  This superstar massage therapist had to endure an hour of Amy's special playlist with The Sky treatment to break a 4 day cuckoo migraine.  Wow, this treatment works!  But, Cyndy will never ever want to listen to Pearl Jam, DMB or REM again.\n",
      "--PREPROCESSED--\n",
      "poor cyndy superstar massage therapist endure hour amy special playlist sky treatment break 4 day cuckoo migraine wow treatment work cyndy never ever want listen pearl jam dmb rem\n",
      "--GENERATED SEQUENCE (WITHOUT WORD ORDER)--\n",
      "spray elizabeth falling karen occasion atlanta visited soft revamped heidi superstar else crush tech scott blue elizabeth nice folklore kitt edwin programing tmn defiantly waking rosie drummer twelve product\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "--ORIGINAL--\n",
      "he hit the booty like a drum, yumyum.😀 why he wanna stutter and he know he wanna butter my buns and have all of my sons. got them young and old like yang hyun suk and bang yong guk and my baby knows how to cook, red suit,  korean look.  startin all them trends in the mercedes benz playin that kpop, that jrock, i'm playin with that c🐈🐓k he likes a chick that plays wu tang that's how we bang, loves to talk slang knows how to freestyle knows how to get buckwild. he's a little bit of bruce lee doin karate, he a hottie he pulls out the shottie and we keep it sexual, also intellectual they askin what the dragon  do! 🐉🔥🔥🔥                   🍎🍏          😘⛲😜💱📳🌃💚💫\n",
      "--PREPROCESSED--\n",
      "hit booty like drum yumyum wanna stutter know wanna butter bun son got young old like yang hyun suk bang yong guk baby know cook red suit korean look startin trend mercedes benz playin kpop jrock playin c k like chick play wu tang bang love talk slang know freestyle know get buckwild little bit bruce lee doin karate hottie pull shottie keep sexual also intellectual askin dragon\n",
      "--GENERATED SEQUENCE (WITHOUT WORD ORDER)--\n",
      "jen strangely booty dj bruce pulling ndido guk intellectual infant whine garlic alright ssi hackensack slang let nicole heavy blvd heat samantha stutter look giant husband jummah ure bagel everywhere bjj echt enrichment second mater charming hell trans rts adversity push uch sobre new chunky true zou pointed verlo streammiter rhd conozcas sportin girl nenhum farm ah medical seriously tof little firsties sky naomi suk cheryl bit key\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "--ORIGINAL--\n",
      "First cocktail of the day — Three Hearts — featuring Kansas distilled spirit. And our bartender just finished writing her first play.\n",
      "--PREPROCESSED--\n",
      "first cocktail day three heart featuring kansa distilled spirit bartender finished writing first play\n",
      "--GENERATED SEQUENCE (WITHOUT WORD ORDER)--\n",
      "twice fourth summer timing kansa finishing allowing second seven includes snack clase unfinished named\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "--ORIGINAL--\n",
      "Just a week away we are welcoming all the youth from ages 14-18 !!! Let us start this year with God !\n",
      "--PREPROCESSED--\n",
      "week away welcoming youth age 14 18 let u start year god\n",
      "--GENERATED SEQUENCE (WITHOUT WORD ORDER)--\n",
      "34 23 welcoming god joyous hating pursuing adult month 30 true announcing\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for idx, doc in enumerate(docs[:5]):\n",
    "    print('--ORIGINAL--')\n",
    "    print(original_docs[original_docs.index[idx]])\n",
    "    print('--PREPROCESSED--')\n",
    "    print(doc)\n",
    "    print('--GENERATED SEQUENCE (WITHOUT WORD ORDER)--')\n",
    "    words_count = len(doc.split())\n",
    "    words_ = np.random.choice(words, words_count, p=doc_vecs[idx].todense().tolist()[0])\n",
    "    for i in range(words_count):\n",
    "        word_idx = tfidf.vocabulary_[words_[i]]\n",
    "        words_[i] = np.random.choice(words, 1, p=word_replace_probs[word_idx])[0]\n",
    "    print(' '.join(words_))\n",
    "    print('-'*150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ER-AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-multilingual-cased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = 'distilbert-base-multilingual-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "bert = AutoModel.from_pretrained(model_name, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 146, 11850, 24109, 15703, 119, 12689, 72894, 11268, 119, 102] [CLS] I like pigs. And apples. [SEP]\n",
      "[101, 177, 11850, 24109, 15703, 119, 10111, 72894, 11268, 119, 102] [CLS] i like pigs. and apples. [SEP]\n",
      "[101, 177, 11850, 24109, 15703, 119, 10111, 72894, 11268, 119, 102] [CLS] i like pigs. and apples. [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode('I like pigs. And apples.'), tokenizer.decode(tokenizer.encode('I like pigs. And apples.')))\n",
    "print(tokenizer.encode('i like pigs. and apples.'), tokenizer.decode(tokenizer.encode('i like pigs. and apples.')))\n",
    "print(tokenizer.encode('i like pigs . and apples .'), tokenizer.decode(tokenizer.encode('i like pigs . and apples .')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(doc):\n",
    "    doc = tokenizer.decode(tokenizer.encode(doc, max_length=128, padding='max_length', truncation=True))\n",
    "#     doc = tokenizer.decode(tokenizer.encode(doc))\n",
    "    doc = re.sub(r'([\\.,\\'’\\\"\\-!\\?\\(\\)])', r' \\1 ', doc)\n",
    "    doc = re.sub('\\s', ' ', doc)\n",
    "    return doc.split()\n",
    "\n",
    "unique_tokens = set(chain.from_iterable([*train_posts.text_clean.apply(get_words).tolist(),\n",
    "                                       *test_posts.text_clean.apply(get_words).tolist()]))\n",
    "\n",
    "token2idx = {token: idx for idx, token in enumerate(unique_tokens)}\n",
    "idx2token = {idx: token for idx, token in enumerate(unique_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_TOKEN_ID = token2idx['[CLS]']\n",
    "EOS_TOKEN_ID = token2idx['[SEP]']\n",
    "PAD_TOKEN_ID = token2idx['[PAD]']\n",
    "\n",
    "VOCAB_SIZE = len(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "del unique_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44463"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_idx(sent: str, word: str):\n",
    "    return sent.split(\" \").index(word)\n",
    "\n",
    "def get_hidden_states(encoded, token_ids_words, model, layers):\n",
    "    \"\"\"Push input IDs through model. Stack and sum `layers` (last four by default).\n",
    "        Select only those subword token outputs that belong to our word of interest\n",
    "        and average them.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded)\n",
    " \n",
    "    # Get all hidden states\n",
    "    states = output.hidden_states\n",
    "    # Stack and sum all requested layers\n",
    "    output = torch.stack([states[i] for i in layers]).sum(0).squeeze()\n",
    "    \n",
    "    res = []\n",
    "    labels_count = []\n",
    "    \n",
    "    for idx, (outp, label) in enumerate(zip(output, token_ids_words)):\n",
    "        if label is None or token_ids_words[idx - 1] is None or token_ids_words[idx - 1] != token_ids_words[idx]:\n",
    "            res.append(outp)\n",
    "            labels_count.append(1)\n",
    "        else: \n",
    "            res[-1] += outp\n",
    "            labels_count[-1] += 1\n",
    "    \n",
    "    res = torch.vstack(res)\n",
    "    res = res / torch.tensor(labels_count).float().unsqueeze(1)\n",
    " \n",
    "    return res\n",
    "\n",
    "\n",
    "def get_word_vectors(sent, tokenizer, model, layers):\n",
    "    \"\"\"Get a word vector by first tokenizing the input sentence, getting all token idxs\n",
    "        that make up the word of interest, and then `get_hidden_states`.\"\"\"\n",
    "    encoded = tokenizer.encode_plus(sent, return_tensors=\"pt\", max_length=128, padding='max_length', truncation=True)\n",
    "    return get_hidden_states(encoded, encoded.word_ids(), model, layers)\n",
    "\n",
    "\n",
    "def exmpl(layers=None):\n",
    "    # Use last four layers by default\n",
    "    layers = [-4, -3, -2, -1] if layers is None else layers\n",
    "\n",
    "    sent = train_posts.text_clean[train_posts.index[23]]\n",
    "\n",
    "    word_embedding = get_word_vectors(sent, tokenizer, bert, layers)\n",
    "\n",
    "    return word_embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([121, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exmpl().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vectors(sent, tokenizer, model, layers):\n",
    "    \"\"\"Get a word vector by first tokenizing the input sentence, getting all token idxs\n",
    "        that make up the word of interest, and then `get_hidden_states`.\"\"\"\n",
    "    \n",
    "    encoded = tokenizer.encode_plus(sent, return_tensors=\"pt\", max_length=128, padding='max_length', truncation=True)\n",
    "    \n",
    "    input_ids = list(map(lambda x: token2idx[x], get_words(sent)))\n",
    "    input_ids = torch.tensor(input_ids).unsqueeze(0)\n",
    "    return get_hidden_states(encoded, encoded.word_ids(), model, layers).cpu(), input_ids\n",
    "\n",
    "def get_embedding(doc, model=bert):\n",
    "    \"Get embedding for each word\"\n",
    "    layers = [-4, -3, -2, -1]\n",
    "    return get_word_vectors(doc, tokenizer, model, layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "input_ids = []\n",
    "\n",
    "token_embeddings = torch.zeros((VOCAB_SIZE, 768))\n",
    "token_count = torch.zeros((VOCAB_SIZE,))\n",
    "\n",
    "for idx, doc in enumerate(train_posts.text_clean):\n",
    "#     try:\n",
    "    embedding, ids = get_embedding(doc)\n",
    "    embeddings.append(embedding)\n",
    "    input_ids.append(ids)\n",
    "    token_embeddings.scatter_add_(0, ids[0].unsqueeze(1).expand(embedding.shape), embedding)\n",
    "    token_count.scatter_add_(0, ids[0], torch.ones_like(ids[0], dtype=torch.float))\n",
    "#     except Exception as e:\n",
    "#         print(idx)\n",
    "#         print(get_words(doc))\n",
    "#         print(e)\n",
    "\n",
    "token_embeddings = torch.nan_to_num(torch.div(token_embeddings, token_count.unsqueeze(1).expand(token_embeddings.shape)))\n",
    "F.normalize(token_embeddings, dim=-1, out=token_embeddings)\n",
    "del token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([44463, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del token_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n"
     ]
    }
   ],
   "source": [
    "token_similarities = torch.zeros((VOCAB_SIZE, VOCAB_SIZE), dtype=torch.float16)\n",
    "\n",
    "batch_size = 100000\n",
    "for i in range(VOCAB_SIZE):\n",
    "    if i % 10000 == 0:\n",
    "        print(i)\n",
    "        \n",
    "    vals = torch.matmul(token_embeddings[i].cuda(), token_embeddings[0:i+1].T.cuda()).cpu()\n",
    "    torch.clip(vals, max=0.85, out=vals)\n",
    "    vals = vals.to(torch.float16)\n",
    "    token_similarities[i, 0:i+1] = vals\n",
    "    token_similarities[0:i+1, i] = vals\n",
    "\n",
    "def get_token_sim(i, j, token_similarities=token_similarities):\n",
    "    return token_similarities[i, j].to(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.utilities.memory.garbage_collection_cuda()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('checkpoints_vk_distilbert/log.txt', 'a', encoding='utf-8') as f:\n",
    "    print('token_similarities ok', file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.clip(token_similarities, max=0.85, out=token_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([44463, 44463])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_similarities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NYDataset(Dataset):\n",
    "    def __init__(self, embeddings, input_ids):\n",
    "        self.embeddings = embeddings\n",
    "        self.input_ids = list(map(lambda x: x.squeeze(), input_ids))\n",
    "        \n",
    "        self.embeddings = nn.utils.rnn.pad_sequence(self.embeddings, batch_first=True, padding_value=0)\n",
    "        self.input_ids = nn.utils.rnn.pad_sequence(self.input_ids, batch_first=True, padding_value=PAD_TOKEN_ID)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.input_ids[idx].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 44463])\n"
     ]
    }
   ],
   "source": [
    "gen = torch.randn(128, 64, VOCAB_SIZE)\n",
    "print(gen[:, 0, :].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(900735.2500)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = F.log_softmax(torch.randn(128, 64, VOCAB_SIZE), -1)\n",
    "orig = torch.randint(0, VOCAB_SIZE, (64, 128))\n",
    "\n",
    "gen_doc = gen[:, 0, :]\n",
    "orig_doc = orig[0, :]\n",
    "\n",
    "def doc_embed_loss(gen_doc, orig_doc, k=5):\n",
    "    topk_values, topk_indices = torch.topk(gen_doc, k, dim=-1)\n",
    "    rand_indices = torch.randint(0, VOCAB_SIZE, (gen_doc.shape[0], k))\n",
    "    doc_loss = 0\n",
    "    for i in range(gen_doc.shape[0]):\n",
    "        rand_values = torch.index_select(gen_doc, -1, rand_indices[i])\n",
    "        doc_loss += (topk_values * get_token_sim(orig_doc[i].item(), topk_indices[i])).sum()\n",
    "        doc_loss += (rand_values * get_token_sim(orig_doc[i].item(), rand_indices[i])).sum()\n",
    "    \n",
    "    return -doc_loss\n",
    "\n",
    "doc_embed_loss(gen_doc, orig_doc)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5218"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_doc[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.2026)\n",
      "tensor(58259076.)\n",
      "tensor(29129340.)\n"
     ]
    }
   ],
   "source": [
    "gen = torch.randn(64, 128, VOCAB_SIZE)\n",
    "orig = torch.randint(0, VOCAB_SIZE, (64, 1, 128))\n",
    "\n",
    "# LOSS FUNCTIONS\n",
    "\n",
    "def recon_loss(inp, targ):\n",
    "    \"Loss of first stage\"\n",
    "    return F.cross_entropy(inp.view(-1, VOCAB_SIZE), targ.reshape(-1)) # forgot set ignore_index as PAD_TOKEN_ID\n",
    "\n",
    "def doc_embed_loss(gen_doc, orig_doc, k=5):\n",
    "    topk_values, topk_indices = torch.topk(gen_doc, k, dim=-1)\n",
    "    rand_indices = torch.randint(0, VOCAB_SIZE, (gen_doc.shape[0], k))\n",
    "    doc_loss = 0\n",
    "    \n",
    "    for i in range(gen_doc.shape[0]):\n",
    "        rand_values = torch.index_select(gen_doc, -1, rand_indices[i])\n",
    "        doc_loss += (topk_values * get_token_sim(orig_doc[i].item(), topk_indices[i])).sum()\n",
    "        doc_loss += (rand_values * get_token_sim(orig_doc[i].item(), rand_indices[i])).sum()\n",
    "\n",
    "    return doc_loss.to(torch.float)\n",
    "\n",
    "def embed_loss(inp, targ, k=5):\n",
    "    loss = 0\n",
    "    inp = F.log_softmax(inp, dim=-1)\n",
    "    for i in range(inp.shape[0]):\n",
    "        loss += doc_embed_loss(inp[i], targ[i][0], k=k)\n",
    "    return -loss\n",
    "\n",
    "def total_loss(inp, targ, alpha=1, beta=0.5, k=5):\n",
    "    \"Loss of second stage\"\n",
    "    return alpha * recon_loss(inp, targ) + beta * embed_loss(inp, targ, k)\n",
    "\n",
    "print(recon_loss(gen, orig))\n",
    "print(embed_loss(gen, orig))\n",
    "print(total_loss(gen, orig))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch-lightning.readthedocs.io/en/latest/notebooks/lightning_examples/datamodules.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(samples):\n",
    "    x = [sample[0] for sample in samples]\n",
    "    y = [sample[1].squeeze() for sample in samples]\n",
    "    \n",
    "    x = nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value=0.0)\n",
    "    y = nn.utils.rnn.pad_sequence(y, batch_first=True, padding_value=PAD_TOKEN_ID)\n",
    "    \n",
    "    if x.shape[1] < y.shape[1]:\n",
    "        y = y[:, :x.shape[1]]\n",
    "    \n",
    "#     print(x.shape, y.shape)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "class LitERAE(pl.LightningModule):\n",
    "    def __init__(self, data, emb_size=768, hidden_size=512, num_layers=2, act_type=None, learning_rate=1e-3, batch_size=64):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # We hardcode dataset specific stuff here.\n",
    "        self.data = data\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Build model\n",
    "        self.gru_1 = nn.GRU(emb_size, hidden_size, num_layers, bidirectional=True, batch_first=True)\n",
    "        if act_type == None:\n",
    "            self.act_1 = nn.Identity()\n",
    "        if act_type == 'ReLU':\n",
    "            self.act_1 = nn.ReLU()\n",
    "        self.linear_1 = nn.Linear(hidden_size * 2, emb_size)\n",
    "        self.gru_2 = nn.GRU(emb_size, hidden_size, num_layers, bidirectional=True, batch_first=True)\n",
    "        self.linear_2 = nn.Linear(hidden_size * 2, VOCAB_SIZE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, hidden = self.gru_1(x)\n",
    "        x = self.act_1(x)\n",
    "        x = self.linear_1(x)\n",
    "        x, _ = self.gru_2(x, hidden)\n",
    "        x = self.act_1(x)\n",
    "        x = self.linear_2(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_func(logits, y)\n",
    "        \n",
    "        self.log(f'train_loss', loss)\n",
    "        self.log(f'avg_train_loss', loss, on_step=False, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_func(logits, y)\n",
    "        \n",
    "        self.log(f'val_loss', loss)\n",
    "        self.log(f'avg_val_loss', loss, on_step=False, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        lr_scheduler = {\"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2, verbose=True), \"monitor\": \"avg_val_loss\"}\n",
    "        return {'optimizer': optimizer, 'lr_shceduler': lr_scheduler}\n",
    "\n",
    "    def on_train_batch_end(self, outputs, batch, batch_idx):\n",
    "        metrics = self.trainer.callback_metrics\n",
    "#         logger.info(f'Batch train loss {metrics}')\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        metrics = self.trainer.callback_metrics\n",
    "        print(f'Train loss: {metrics[\"avg_train_loss\"]}')\n",
    "\n",
    "    def on_validation_batch_end(self, outputs, batch, batch_idx):\n",
    "        metrics = self.trainer.callback_metrics\n",
    "#         logger.info(f'Batch validation loss {metrics}')\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        metrics = self.trainer.callback_metrics\n",
    "        print(f'Val loss: {metrics[\"avg_val_loss\"]}')\n",
    "\n",
    "    ####################\n",
    "    # DATA RELATED HOOKS\n",
    "    ####################\n",
    "\n",
    "#     def prepare_data(self):\n",
    "#         self.data = nn.utils.rnn.pad_sequence(self.data)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            train_size = int(0.9 * len(self.data))\n",
    "            val_size = len(self.data) - train_size\n",
    "            self.data_train, self.data_val = random_split(self.data, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "            self.loss_func = recon_loss\n",
    "        \n",
    "        if stage == 'fit_2':\n",
    "            train_size = int(0.9 * self.data.shape[1])\n",
    "            val_size = self.data.shape[1] - train_size\n",
    "            self.data_train, self.data_val = random_split(self.data, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "            self.loss_func = total_loss\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "#         if stage == \"test\" or stage is None:\n",
    "#             self.data_test\n",
    "#             self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.data_train, batch_size=self.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.data_val, batch_size=self.batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "#     def test_dataloader(self):\n",
    "#         return DataLoader(self.mnist_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "chechpoint_path = \"checkpoints_vk_distilbert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "params = {\n",
    "    'hidden_size': 700,\n",
    "    'num_layers': 2,\n",
    "    'learning_rate': 1e-3,\n",
    "    'act_type': 'ReLU'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type   | Params\n",
      "------------------------------------\n",
      "0 | gru_1    | GRU    | 15.0 M\n",
      "1 | act_1    | ReLU   | 0     \n",
      "2 | linear_1 | Linear | 1.1 M \n",
      "3 | gru_2    | GRU    | 15.0 M\n",
      "4 | linear_2 | Linear | 62.3 M\n",
      "------------------------------------\n",
      "93.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "93.4 M    Total params\n",
      "373.494   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5658c18ebab426a8cf6fdf0b6c70e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 1.845604658126831\n",
      "Train loss: 2.1288845539093018\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 1.2394801378250122\n",
      "Train loss: 1.517362356185913\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50313128abbb43afb7eeb1f4b7c32888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5857846736907959\n",
      "Train loss: 0.5560625791549683\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.499859094619751\n",
      "Train loss: 0.36761829257011414\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.45253172516822815\n",
      "Train loss: 0.2541213631629944\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.4258239269256592\n",
      "Train loss: 0.17898763716220856\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.4147576093673706\n",
      "Train loss: 0.12884068489074707\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.3999043405056\n",
      "Train loss: 0.09576758742332458\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.4012964367866516\n",
      "Train loss: 0.07497640699148178\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.4011717438697815\n",
      "Train loss: 0.06294993311166763\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.4036440849304199\n",
      "Train loss: 0.05500193312764168\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.4000810384750366\n",
      "Train loss: 0.049020130187273026\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.4064443111419678\n",
      "Train loss: 0.04413075000047684\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=15` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.4098970592021942\n",
      "Train loss: 0.042632874101400375\n"
     ]
    }
   ],
   "source": [
    "data = NYDataset(embeddings, input_ids)\n",
    "model = LitERAE(data, batch_size=BATCH_SIZE, **params)\n",
    "model.train()\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(dirpath=chechpoint_path, save_top_k=2, monitor=\"val_loss\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=15,\n",
    "    num_nodes=1,\n",
    "    num_sanity_val_steps=0,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = checkpoint_callback.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{chechpoint_path}/best1.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data\n",
    "del model\n",
    "del trainer\n",
    "pl.utilities.memory.garbage_collection_cuda()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun 14 14:37:14 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.64.00    Driver Version: 440.64.00    CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:5E:00.0 Off |                    0 |\n",
      "| N/A   64C    P0    38W /  70W |   3078MiB / 15109MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = NYDataset(embeddings, input_ids)\n",
    "model = LitERAE.load_from_checkpoint(checkpoint_path=best_model_path, data=data, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type   | Params\n",
      "------------------------------------\n",
      "0 | gru_1    | GRU    | 15.0 M\n",
      "1 | act_1    | ReLU   | 0     \n",
      "2 | linear_1 | Linear | 1.1 M \n",
      "3 | gru_2    | GRU    | 15.0 M\n",
      "4 | linear_2 | Linear | 62.3 M\n",
      "------------------------------------\n",
      "93.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "93.4 M    Total params\n",
      "373.494   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "087e6fa59dc94804a26729118ec4f5ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.369366854429245\n",
      "Train loss: 0.06973734498023987\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.37771719694137573\n",
      "Train loss: 0.042418014258146286\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.3689057528972626\n",
      "Train loss: 0.03361129015684128\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.3715161681175232\n",
      "Train loss: 0.028621839359402657\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.37552621960639954\n",
      "Train loss: 0.025716310366988182\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.3814913332462311\n",
      "Train loss: 0.023617105558514595\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.3801876902580261\n",
      "Train loss: 0.021150799468159676\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.3817344307899475\n",
      "Train loss: 0.020335646346211433\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.38352853059768677\n",
      "Train loss: 0.01897362992167473\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.3885502815246582\n",
      "Train loss: 0.017361585050821304\n"
     ]
    }
   ],
   "source": [
    "model.stage = 'fit_2'\n",
    "model.train()\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(dirpath=chechpoint_path, save_top_k=2, monitor=\"val_loss\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    num_nodes=1,\n",
    "    num_sanity_val_steps=0,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = checkpoint_callback.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{chechpoint_path}/best2.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data\n",
    "del model\n",
    "del trainer\n",
    "# torch.cuda.empty_cache()\n",
    "pl.utilities.memory.garbage_collection_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('checkpoints_vk_distilbert/best2.txt', 'r', encoding='utf-8') as f:\n",
    "    best_model_path = f.readlines()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = NYDataset(embeddings, input_ids)\n",
    "model = LitERAE.load_from_checkpoint(checkpoint_path=best_model_path, data=data, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_mechanism(pho, epsilon, delta):\n",
    "    pho = np.array(pho)\n",
    "    temp = np.exp(epsilon / (2 * delta) * pho)\n",
    "    return temp / np.sum(temp)\n",
    "\n",
    "\n",
    "def predict(model, sent, seed=None):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    \n",
    "    inp = get_embedding(sent)[0].unsqueeze(0).cuda()\n",
    "    with torch.no_grad():\n",
    "        logits = model(inp)[0].cpu()\n",
    "    \n",
    "    predicted_probs = torch.softmax(logits, dim=-1)\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    def build_two_sets(probs, k=5):\n",
    "        # return lexical set and semantic set\n",
    "        probs = np.array(probs)\n",
    "        l_set = rng.choice(probs.shape[0], k, p=probs, replace=True)\n",
    "        l_set_probs = probs[l_set]\n",
    "\n",
    "        marks = np.ones(probs.shape[0], dtype=bool)\n",
    "        marks[l_set] = False\n",
    "\n",
    "        whole_idxs = np.arange(probs.shape[0])\n",
    "        s_set = whole_idxs[marks]\n",
    "        s_set_probs = probs[marks]\n",
    "\n",
    "        return l_set, s_set, l_set_probs, s_set_probs\n",
    "\n",
    "    def choose_set(l_set, s_set, l_set_probs, s_set_probs, eps=180):\n",
    "        probs = [0, 0]\n",
    "        probs[0] = np.sum(l_set_probs) / (np.sum(l_set_probs) + np.sum(s_set_probs))\n",
    "        probs[1] = 1 - probs[0]\n",
    "        probs = exponential_mechanism(probs, eps, 1)\n",
    "        po = [(l_set, l_set_probs), (s_set, s_set_probs)]\n",
    "        indxs = [0, 1]\n",
    "        indx = int(rng.choice(indxs, 1, p=probs))\n",
    "        return po[indx]\n",
    "\n",
    "    for probs in predicted_probs:\n",
    "        # build set\n",
    "        l_set, s_set, l_set_probs, s_set_probs = build_two_sets(probs, k=5)\n",
    "\n",
    "        # choose set\n",
    "        c_set, c_set_probs = choose_set(l_set, s_set, l_set_probs, s_set_probs)\n",
    "\n",
    "        # choose token\n",
    "        token_eps = 0.1\n",
    "        c_set_probs = exponential_mechanism(c_set_probs, token_eps, 1)\n",
    "        token_idx = int(rng.choice(c_set, 1, p=c_set_probs))\n",
    "\n",
    "        if token_idx == EOS_TOKEN_ID:\n",
    "            break\n",
    "        res.append(token_idx)\n",
    "#     o_pred = ' '.join([self.params['idx2word'][idx] for idx in res])\n",
    "#     o_pred = tokenizer.decode(res, skip_special_tokens=True)\n",
    "    o_pred = ' '.join(idx2token[idx] for idx in res[1:])\n",
    "    return o_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Original>:\n",
      "ура \u0001 ура \u0001 получили новинки линия \u0001 22 112 размер \u0001 50 52 54 56 58 60 цена \u0001 1100 р\n",
      "--------------------------------->\n",
      "<Transformed>:\n",
      "ура ура получили новинки карта 22 150 размер 50 52 54 58 58 60 цена 500 р\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "text = test_posts.text_clean[test_posts.index[1]]\n",
    "print('<Original>:')\n",
    "print(text)\n",
    "print('--------------------------------->')\n",
    "print('<Transformed>:')\n",
    "print(predict(model, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Original>:\n",
      "доброе утро \u0001 легкого понедельника \u0001 осень \u0001 утро \u0001 дом \u0001 \u0001 горечь \u0001 кофе \u0001 \u0001 дождь \u0001 тоска \u0001 душа \u0001 взгляд \u0001 компьютер \u0001 вздох \u0001 надежда \u0001 грусть \u0001 сомненье \u0001 интернет \u0001 кнопка \u0001 \u0001 экран \u0001 привет \u0001 вера \u0001 сердце \u0001 стук \u0001 мгновенье \u0001 блеск \u0001 глаза \u0001 миг \u0001 солнце \u0001 лучик \u0001 одночасье \u0001 радость \u0001 смех \u0001 улыбка \u0001 счастье \u0001 ника\n",
      "------------------------>\n",
      "<Transformed>:\n",
      "доброе утро легкого располагается осень утро море проруби кофе дождь поляна blue взгляд ольга впечатлений надежду смех солнечного сад dial подарок привет веру дмитрий атмосфера аромат блеск глаза ст солнце dial edition радости страсть [PAD] счастье edition\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "<Original>:\n",
      "estel - салон красоты у вас дома выбрать совместные покупки спб \u0001клюкvа \u0001 наши раздачи\n",
      "------------------------>\n",
      "<Transformed>:\n",
      "estel - салат красоту у вас 13 выбрать совместные покупки спб клюкvа наши раздачи\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "<Original>:\n",
      "my november \u0001 \u0001 \u0001мне не дано жить нормальной жизнью \u0001 тогда приходится придумать свою \u0001 \u0001 \u0001 сара \u0001\n",
      "------------------------>\n",
      "<Transformed>:\n",
      "my ориентировочно мне не дает жить нормально жизнью тогда правда приключений свою eyes\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "<Original>:\n",
      "испытай удачу и выиграй в ежедневной лотерее \u0001злой сказки \u0001 первый кликнувший получит дополнительную попытку\n",
      "------------------------>\n",
      "<Transformed>:\n",
      "испытай удачу и подписывайся в праздник россии ждать сказки первый кликнувший получит дополнительную попытку\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "<Original>:\n",
      "однажды хемингуэй заключил спор \u0001 что напишет рассказ \u0001 состоящий всего из шести слов \u0001 способный любого читателя \u0001 писателю удалось выиграть спор \u0001 детские ботиночки \u0001 sale \u0001 baby \u0001 never \u0001 \u0001 \u0001 фредерик браун сочинил страшную историю из когда-либо написанных \u0001 «последний человек на земле сидел в комнате \u0001 в дверь \u0001 \u0001 \u0001» \u0001 \u0001 \u0001 американский писатель о \u0001генри выиграл конкурс на самый короткий рассказ \u0001 который имеет все составляющие традиционного рассказа — \u0001 и \u0001 и над \u0001 посмотреть много ли осталось бензина \u0001 было двадцать три года» \u0001 \u0001 \u0001 \u0001 англичане тоже конкурс на самый краткий рассказ \u0001 но по условиям конкурса \u0001 в нем должны быть королева \u0001 бог \u0001 секс \u0001 тайна \u0001 первое место присудили автору такого рассказа \u0001 «о \u0001 боже \u0001 — воскликнула королева \u0001 — я беременна и не знаю от кого \u0001» \u0001 \u0001 \u0001 классический пример спартанцев относится к царя филиппа ii \u0001 многие греческие города \u0001 вам сдаться немедленно \u0001 потому что если моя армия войдёт в ваши земли \u0001 я ваши сады \u0001 людей и город» \u0001 на это ответили одним словом \u0001 \u0001 \u0001 \u0001 \u0001 виктор гюго отправил рукопись романа с письмом \u0001 ответ был не менее \u0001 « \u0001» \u0001 \u0001 \u0001 b на самую короткую победила одна пожилая француженка \u0001 которая написала \u0001 у меня было лицо и юбка \u0001 а теперь —\n",
      "------------------------>\n",
      "<Transformed>:\n",
      "первый ездит напоминаю разговор что автору речь созданный всего из монсеррат слов сэс любые подчеркнул приготовленным удалось мiр спор детские символы man процесс never алексей лавров попробовать концерта видов из когда - либо отыграл « немного человек на земле пожаловать в алиса в 2010 » лучших медаль о футбольные благодаря процесс на самый 1974 речь который имеет все убийцы следы ламинирование [UNK] и и над посмотреть много ли\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "<Original>:\n",
      "я набрала 92000 очков на уровне \u0001 а сколько сможешь набрать ты?\n",
      "------------------------>\n",
      "<Transformed>:\n",
      "я набрала 92000 очков на уровне а сколько сможешь набрать ты ?\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "<Original>:\n",
      "александр 1 марта 1909 - 9 августа 1966 \u0001 \u0001 \u0001 пусть все стихи \u0001 что написал я за год \u0001 перед тобой сейчас покорно лягут \u0001 взамен корзины разноцветных ягод \u0001 которых я собрать тебе не смог \u0001 и все \u0001 мною \u0001 пусть свое вино \u0001 чтобы оно волною пыль с твоих ног \u0001 1958 \u0001 \u0001 \u0001 деревья \u0001который год у моего окна с сосна \u0001 не замечая ничего вокруг \u0001 как мы с тобою \u0001 мой друг \u0001 \u0001 \u0001 \u0001 пожелание хотел бы пожелать в радости \u0001 в разлуки или горе вечно помнить первое \u0001 забывая о последней ссоре \u0001 \u0001 1958\n",
      "------------------------>\n",
      "<Transformed>:\n",
      "александр 2 марта стойк - 9 августа запятые пусть все гармонии что написал я за год перед великим сегодня разве россии верх санаторий прощения ждала ибо я покупая тебе не смог и все тебя плохо свое полный чтобы оно исполняет памятника с нужна 1960 1958 улыбка который год у моей окна с времен не отмечали еще вокруг как мы с каждую мой друг мозг сказал бы пожаловать в радости в сковороде или белых в\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "<Original>:\n",
      "ваша идеальная юбка \u0001 красная или найдете свой уникальный стиль вместе с подписывайся в инстаграм на \u0001zumomoda и мы пришлём промо-код на скидку доставка с примеркой на дом по москве и спб отправка в любые регионы россии и снг курьерской службой сдэк\n",
      "------------------------>\n",
      "<Transformed>:\n",
      "ваши идеальной юбка красота или найдете свой уникальный стиль вместе с подписывайся в инстаграм на конфеты и мы пришлём промо - код на скидку доставка с примеркой на дом по москве и спб отправка в любые регионы россии и снг курьерской службой сдэк\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "<Original>:\n",
      "оторвемся по - питерски \u0001 клуб \u0001красавица и чудовище \u0001 благодарит всех пришедших семидесяти пяти гостей нашего вечера 17 мая \u0001 будем рады видеть вас снова насладимся жизнью в этот вечер и получим весь спектр удовольствий от нее \u0001 24 мая не скучай - на марата приходи \u0001 день города вместе с нами эротично отмечай \u0001 покажем \u0001 страсть и яркость вашей неуемной сексуальности \u0001 в этот весенний праздник нас ожидают настоящий салют из всех «мужских пушек» \u0001 выстреливая меткими залпами весь фейерверк оргазма и удовольствия \u0001 \u0001 \u0001 \u0001 настоящая баня на дровах с вениками \u0001 бассейн \u0001 и весь «антураж веселого застолья» \u0001 «подвижные игры» \u0001 турнир по «сексуальному» домино \u0001 а самое главное раскрепощенное и приятное общение… с каждой минутой притягивающее все ближе и ближе… для тех \u0001 кто хочет расширить границы обыденности \u0001 ждем на нашей вечеринке \u0001 в вашем распоряжении \u0001 2 этажа \u0001 1 этаж банный комплекс \u0001 2 этаж-развлекательный \u0001 баня на дровах \u0001 бассейн \u0001 3 комнаты отдыха \u0001 2 массажных места \u0001 для девушек- вход бесплатный включен фуршетшашлык \u0001 бутерброды \u0001 фрукты \u0001 салаты \u0001пиво \u0001 шампанское \u0001 белые и красные вина \u0001 банные принадлежностипростыня \u0001 тапки средства защиты \u0001 для мужчин без пары вход 2500 рв стоимость включен фуршетшашлык \u0001 бутерброды \u0001 фрукты \u0001 салаты \u0001 пиво \u0001 шампанское \u0001 белые и красные вина \u0001 банные принадлежностипростыня \u0001 тапки средства защиты \u0001 для пары мж-1000 р с пары в стоимость включен фуршет шашлык \u0001 бутерброды \u0001 фрукты \u0001 салаты \u0001 пиво \u0001 шампанское \u0001 белые и красные вина \u0001 банные принадлежностипростыня \u0001 тапки средства защиты \u0001 мы всегда рады новым знакомствам с интересными людьми \u0001 загляните к нам в клуб \u0001 и вы вновь погрузитесь в ту \u0001 непередаваемую атмосферу чувственности \u0001 искристой радости и тепла \u0001 предчувствие изысканных наслаждений наполнит вас сладкой истомой \u0001 вечеринки проходят еженедельно \u0001 по пятницам \u0001 19 \u000130-23 \u000130 \u0001 в центре \u0001 на марата \u0001 вход только по предварительной записи по телефону \u0001 количество мест - ограничено \u0001 запись по телефонам \u0001 +7-911-218-72-19 натали \u0001 +7-981-767-85-44 владимир если вы любите новые знакомства \u0001 хороший отдых \u0001 и знаете толк в жизни \u0001 свободной от предрассудков \u0001 тогда эта вечеринка - для вас \u0001 \u0001 \u0001 желаем вам ярких сексуальных впечатлений и новых приятных знакомств в нашем клубе \u0001 вечеринка состоится \u0001 24 мая с 19 ч 30 мин до 23 ч 30 мин \u0001 по адресу \u0001 ул \u0001 марата \u0001 д \u0001 75 \u0001 м \u0001 звенигородская attention \u0001 третья приват комнатадальняя справа от входа на 2 этаж предназначена для пар мж \u0001 женщин \u0001 мужчин \u0001 не обладающих достаточной степенью раскрепощения и испытывающих некую неловкость на начальном этапе групповых отношений \u0001 если вы выбрали себе партнеров \u0001 то можете закрыться с ними на некоторое время в этой приват комнате \u0001 согласовав это с администрацией \u0001 желаем приятного отдыха \u0001 +7 911 218-72-19 swing-plus \u0001ru 24 мая в 19 \u000130 — 24 мая в 23 \u000130 ул \u0001 марата \u0001 д \u0001 75 \u0001 санкт-петербург\n",
      "------------------------>\n",
      "<Transformed>:\n",
      "оторвемся по - красавица клуб красавица и чудовище благодарит всех пришедших семидесяти точки гостей нашего вечера 17 мая будем рады видеть вас снова насладимся жизнью в этот вечер и получим весь спектр удовольствий от нее 24 мая не пожаловать - на марата прийти день города вместе с нами эротично отмечай покажем страсть и ваших вашей неуемной сексуальности в этот весенний\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "<Original>:\n",
      "мороженое \u0001 простой рецепт из 2 ингредиентов / ice cream simple\n",
      "------------------------>\n",
      "<Transformed>:\n",
      "медицинские простой рецепт из 2 eleсtronika / river wostok edition\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "test_texts = test_posts.text_clean\n",
    "for i in range(100, 110):\n",
    "    text = test_texts[test_posts.index[i]]\n",
    "    print('<Original>:')\n",
    "    print(text)\n",
    "    print('------------------------>')\n",
    "    print('<Transformed>:')\n",
    "    print(predict(model, text))\n",
    "    print('\\n------------------------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anonymyze texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = bert.to('cuda')\n",
    "\n",
    "def get_hidden_states_batch(encoded, model, layers):\n",
    "    \"\"\"Push input IDs through model. Stack and sum `layers` (last four by default).\n",
    "        Select only those subword token outputs that belong to our word of interest\n",
    "        and average them.\"\"\"\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        states = model(**encoded).hidden_states\n",
    " \n",
    "    batch_res = []\n",
    "    \n",
    "    for i in range(len(states[0])):\n",
    "        token_ids_words = encoded.word_ids(i)\n",
    "        output = torch.stack([states[layer][i] for layer in layers]).sum(0).squeeze().cpu()\n",
    "\n",
    "        res = []\n",
    "        labels_count = []\n",
    "\n",
    "        for idx, (outp, label) in enumerate(zip(output, token_ids_words)):\n",
    "            if label is None or token_ids_words[idx - 1] is None or token_ids_words[idx - 1] != token_ids_words[idx]:\n",
    "                res.append(outp)\n",
    "                labels_count.append(1)\n",
    "            else: \n",
    "                res[-1] += outp\n",
    "                labels_count[-1] += 1\n",
    "\n",
    "        res = torch.vstack(res)\n",
    "        res = res / torch.tensor(labels_count).float().unsqueeze(1)\n",
    "        \n",
    "        batch_res.append(res)\n",
    "    batch_res = nn.utils.rnn.pad_sequence(batch_res, batch_first=True, padding_value=0.0)\n",
    "    return batch_res\n",
    "\n",
    "def get_word_vectors_batch(sents, tokenizer, model, layers):\n",
    "    \"\"\"Get a word vector by first tokenizing the input sentence, getting all token idxs\n",
    "        that make up the word of interest, and then `get_hidden_states`.\"\"\"\n",
    "    \n",
    "    encoded = tokenizer.batch_encode_plus(sents, return_tensors=\"pt\", max_length=128, padding='max_length', truncation=True)\n",
    "    \n",
    "    device = torch.device('cuda')\n",
    "    encoded = encoded.to(device)\n",
    "\n",
    "    hidden_states = get_hidden_states_batch(encoded, model, layers)\n",
    "    return hidden_states\n",
    "\n",
    "def get_embedding_batch(docs, model=bert):\n",
    "    \"Get embedding for each word\"\n",
    "    layers = [-4, -3, -2, -1]\n",
    "    return get_word_vectors_batch(docs, tokenizer, model, layers)\n",
    "\n",
    "def predict_batch(model, sents, seed=None, batch_size=32, verbose=False, print_step=20):\n",
    "    model.eval()\n",
    "    rng = np.random.default_rng(seed)\n",
    "    \n",
    "    def build_two_sets(probs, k=5):\n",
    "        # return lexical set and semantic set\n",
    "        probs = np.array(probs)\n",
    "        l_set = rng.choice(probs.shape[0], k, p=probs, replace=True)\n",
    "        l_set_probs = probs[l_set]\n",
    "\n",
    "        marks = np.ones(probs.shape[0], dtype=bool)\n",
    "        marks[l_set] = False\n",
    "\n",
    "        whole_idxs = np.arange(probs.shape[0])\n",
    "        s_set = whole_idxs[marks]\n",
    "        s_set_probs = probs[marks]\n",
    "\n",
    "        return l_set, s_set, l_set_probs, s_set_probs\n",
    "\n",
    "    def choose_set(l_set, s_set, l_set_probs, s_set_probs, eps=80):\n",
    "        probs = [0, 0]\n",
    "        probs[0] = np.sum(l_set_probs) / (np.sum(l_set_probs) + np.sum(s_set_probs))\n",
    "        probs[1] = 1 - probs[0]\n",
    "        probs = exponential_mechanism(probs, eps, 1)\n",
    "        po = [(l_set, l_set_probs), (s_set, s_set_probs)]\n",
    "        indxs = [0, 1]\n",
    "        indx = int(rng.choice(indxs, 1, p=probs))\n",
    "        return po[indx]\n",
    "    \n",
    "    o_pred = []\n",
    "    \n",
    "    for i in range(0, len(sents), batch_size):\n",
    "        batch = sents[i:i+batch_size]\n",
    "        inp = get_embedding_batch(batch).to('cuda')\n",
    "        with torch.no_grad():\n",
    "            logits = model(inp).cpu()\n",
    "\n",
    "        predicted_probs = torch.softmax(logits, dim=-1)\n",
    "    \n",
    "        for sent in predicted_probs:\n",
    "            res = []\n",
    "            for probs in sent:\n",
    "                # build set\n",
    "                l_set, s_set, l_set_probs, s_set_probs = build_two_sets(probs, k=5)\n",
    "\n",
    "                # choose set\n",
    "                c_set, c_set_probs = choose_set(l_set, s_set, l_set_probs, s_set_probs)\n",
    "\n",
    "                # choose token\n",
    "                token_eps = 0.1\n",
    "                c_set_probs = exponential_mechanism(c_set_probs, token_eps, 1)\n",
    "                token_idx = int(rng.choice(c_set, 1, p=c_set_probs))\n",
    "\n",
    "                if token_idx == EOS_TOKEN_ID:\n",
    "                    break\n",
    "                res.append(token_idx)\n",
    "        #     o_pred = tokenizer.decode(res, skip_special_tokens=True)\n",
    "            o_pred.append(' '.join(idx2token[idx] for idx in res[1:]))\n",
    "        \n",
    "        if verbose and (i % (print_step * batch_size)) == 0:\n",
    "            print(i)\n",
    "\n",
    "    return o_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "640\n",
      "1280\n",
      "1920\n",
      "2560\n",
      "3200\n",
      "3840\n",
      "4480\n",
      "5120\n",
      "5760\n",
      "6400\n",
      "7040\n",
      "7680\n",
      "8320\n",
      "8960\n",
      "9600\n",
      "10240\n",
      "10880\n",
      "11520\n",
      "12160\n",
      "12800\n",
      "13440\n",
      "14080\n",
      "14720\n",
      "15360\n",
      "16000\n",
      "16640\n",
      "17280\n",
      "17920\n",
      "18560\n",
      "19200\n",
      "19840\n",
      "20480\n",
      "21120\n",
      "21760\n",
      "22400\n",
      "23040\n",
      "23680\n",
      "24320\n",
      "24960\n",
      "25600\n",
      "26240\n",
      "26880\n",
      "27520\n",
      "28160\n",
      "28800\n",
      "29440\n",
      "30080\n",
      "30720\n",
      "31360\n",
      "32000\n",
      "32640\n",
      "33280\n",
      "33920\n",
      "34560\n",
      "35200\n",
      "35840\n",
      "36480\n",
      "37120\n",
      "37760\n",
      "38400\n"
     ]
    }
   ],
   "source": [
    "train__ = train_posts.text_clean.tolist()\n",
    "model.eval()\n",
    "trans_train = predict_batch(model, train__, seed=42, verbose=True)\n",
    "\n",
    "train_posts['text_clean_anon'] = trans_train\n",
    "train_posts.to_csv('/home/jovyan/notebooks/vk/train_posts_anon.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "test__ = test_posts.text_clean.tolist()\n",
    "model.eval()\n",
    "trans_test = predict_batch(model, test__, seed=42, verbose=True)\n",
    "\n",
    "test_posts['text_clean_anon'] = trans_test\n",
    "test_posts.to_csv('/home/jovyan/notebooks/vk/test_posts_anon.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_posts_anon = pd.read_csv('/home/jovyan/notebooks/vk/train_posts_anon.csv')\n",
    "test_posts_anon = pd.read_csv('/home/jovyan/notebooks/vk/test_posts_anon.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashtags anonymization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SynTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = nyc_posts_authors_df.loc[nyc_posts_authors_df['hashtags'].str.len() > 50, 'hashtags'][:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#bike #bikes #bikelife #shopride #shoplife #myfavoritebikeshop #bikeclub #bikenyc #bikeny #croton #crotonaqueduct #gravelgrinder #gravel #gravelride #offroad'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18078"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "s = set(chain.from_iterable(docs.str.split()))\n",
    "len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_model = CountVectorizer(ngram_range=(1,1))\n",
    "X = count_model.fit_transform(docs)\n",
    "Xc = (X.T * X)\n",
    "Xc.setdiag(0)\n",
    "Xc = Xc.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5119884,\n",
       " matrix([[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 5, 5],\n",
       "         [0, 0, 0, ..., 5, 0, 5],\n",
       "         [0, 0, 0, ..., 5, 5, 0]]))"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xc.sum(), Xc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13364, 13364)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.4684e-05, 7.4684e-05, 7.4684e-05,  ..., 7.4684e-05, 7.4684e-05,\n",
       "         7.4684e-05],\n",
       "        [7.4582e-05, 7.4582e-05, 7.4582e-05,  ..., 7.4582e-05, 7.4582e-05,\n",
       "         7.4582e-05],\n",
       "        [7.3181e-05, 7.3181e-05, 7.3181e-05,  ..., 7.3181e-05, 7.3181e-05,\n",
       "         7.3181e-05],\n",
       "        ...,\n",
       "        [3.0429e-07, 3.0429e-07, 3.0429e-07,  ..., 3.0429e-07, 4.5161e-05,\n",
       "         4.5161e-05],\n",
       "        [3.0429e-07, 3.0429e-07, 3.0429e-07,  ..., 4.5161e-05, 3.0429e-07,\n",
       "         4.5161e-05],\n",
       "        [3.0429e-07, 3.0429e-07, 3.0429e-07,  ..., 4.5161e-05, 4.5161e-05,\n",
       "         3.0429e-07]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xc = torch.softmax(torch.from_numpy(Xc).float(), dim=-1)\n",
    "Xc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.0000), tensor(7.4684e-05))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xc[0].sum(), Xc[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "tfidf.fit(docs)\n",
    "doc_vecs = tfidf.transform(docs)\n",
    "doc_vecs = normalize(doc_vecs, norm='l1')\n",
    "words = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18063"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1405975, 5131770)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft = fasttext.load_facebook_model(datapath('/mnt/ess_storage/DN_1/storage/home/vpanov/cc.en.300.bin.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xc[count_model.vocabulary_['newyork']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13364, 13364)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vecs = [Xc[count_model.vocabulary_[word]].tolist()[0] for word in words]\n",
    "word_similarities = cosine_similarity(word_vecs, word_vecs)\n",
    "word_similarities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kgram_overlap(word1, word2, k):\n",
    "    a = set([word1[i:i+k] for i in range(0, len(word1) - k + 1)])\n",
    "    b = set([word2[i:i+k] for i in range(0, len(word2) - k + 1)])\n",
    "    inter = len(a.intersection(b))\n",
    "    return inter / (len(a) + len(b) - inter)\n",
    "\n",
    "def score(word1, word2):\n",
    "    idx1, idx2 = tfidf.vocabulary_[word1], tfidf.vocabulary_[word2]\n",
    "    return word_similarities[idx1, idx2] - 0.3 * kgram_overlap(word1, word2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "likes4like ['turkiye']\n"
     ]
    }
   ],
   "source": [
    "def exponential_gen(x, R, u, sensitivity=1, epsilon=25.4):\n",
    "    # Calculate the score for each element of R\n",
    "    scores = [u(x, r) for r in R]\n",
    "    \n",
    "    # Calculate the probability for each element, based on its score\n",
    "    probabilities = [np.exp(epsilon * score / (2 * sensitivity)) for score in scores]\n",
    "    \n",
    "    # Normalize the probabilties so they sum to 1\n",
    "    probabilities = probabilities / np.linalg.norm(probabilities, ord=1)\n",
    "\n",
    "    # Choose an element from R based on the probabilities\n",
    "    return np.random.choice(R, 1, p=probabilities)\n",
    "\n",
    "num = 7000\n",
    "print(words[num], exponential_gen(words[num], words, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13364, 13364)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def exponential(x, R, u, sensitivity=1, epsilon=25.4):\n",
    "    # Calculate the score for each element of R\n",
    "    scores = [u(x, r) for r in R]\n",
    "    \n",
    "    # Calculate the probability for each element, based on its score\n",
    "    probabilities = [np.exp(epsilon * score / (2 * sensitivity)) for score in scores]\n",
    "    \n",
    "    # Normalize the probabilties so they sum to 1\n",
    "    probabilities = probabilities / np.linalg.norm(probabilities, ord=1)\n",
    "    \n",
    "    return probabilities\n",
    "\n",
    "word_replace_probs = []\n",
    "\n",
    "for idx, word in enumerate(words):\n",
    "    if idx % 1000 == 0:\n",
    "        print(idx)\n",
    "    word_replace_probs.append(exponential(word, words, score))\n",
    "\n",
    "word_replace_probs = np.array(word_replace_probs)\n",
    "word_replace_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--ORIGINAL--\n",
      "#EATDRINKPARTY #miercolesplayero #bronx #BottleSpecials #bestiakitchenbx #LaBestiaDelBronx #HappyHour #PartyPeople #FoodPorn #salsa #playero #retro #4thofjulyparty #preindependenceday #preindependencedayparty\n",
      "--GENERATED SEQUENCE (WITHOUT WORD ORDER)--\n",
      "gaynightlife nbafinals2019 bachata miercolesplayero eatdrinkparty nbafinals2019 santiago 4thofjulyparty caucau 4thofjulyparty lentejitas nycdrinks ericktorres latino 4thofjulyparty\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "--ORIGINAL--\n",
      "#914 #newrochelle #newrochelleny #ionacollege #westchester #westchesterny #westchestereats #westchesternyeats #westchestercountyny #westchestercounty #westchesterfood #westchesterfoodie #tuckahoeny #tuckahoe #eastchester #eastchesterny #larchmont #larchmontny #larchmontvillage #pelhamny #yonkers #yonkersny #bronxville #bronxvilleny #burgersandbeer #bestwings\n",
      "--GENERATED SEQUENCE (WITHOUT WORD ORDER)--\n",
      "westchestereats burgersandfries jameson yonkersny jameson latenightfood larchmontny larchmont yonkers yonkers panini phillycheesesteak tuckahoe titosvodka pelhammanor larchmontny bestwingsever jamesonwhiskey bestwings yonkers newrochelle burgersandfries salad westchesternyeats jamesonwhiskey yonkers\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "--ORIGINAL--\n",
      "#effigies #forevergrounded #record #punk #1984 #theeffigies #records #vinyl #newyorkcity #recordsforsale #carrollgardens #redhook #chicago #punkrock #vinylporn #recordshop #hardcorepunk #vinyligclub #rock #almostreadyrecords #vinylforsale #webuyvinyl #buyselltrade #nyc #brooklyn #vinylrecord #enigma #lp\n",
      "--GENERATED SEQUENCE (WITHOUT WORD ORDER)--\n",
      "carlitosway stooges thebeatles almostready citoferminorchestra 33rpm 1970 effigies 1996 reissue thedictatorsnyc records mysaxophoneforchristmas modusoperandi doom thecity sealed almostchristmas miccitysons redhook redvinyl enigma effigies alifewithoutobstacles jamesbrown recordporn countryteasers ska\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "--ORIGINAL--\n",
      "#10yearchallenge #tgif #1 #happyhour #78loungenj #drinks #food #birthday #turnup #party #goodlife #nightlife #dj #like4like #goodtime #friends #family #followme #instaselfie #swag #follow #newjersey #friday #flashbackfriday\n",
      "--GENERATED SEQUENCE (WITHOUT WORD ORDER)--\n",
      "followme 78lifestyle amazonbooks tournament 78loungenj theheavyhitterdjs sipandpaint friends niceandsmooth family friends lawenforcement goodtime ncaa favorite followme goodlife nightlife 78lounge stpattysday theheavyhitterdjs budin dj djcoolv\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "--ORIGINAL--\n",
      "#birthdayparties #halloween #halloweenparty2019 #thecomplexnyc #birthday #bubbleball #bubbleballsoccer #halloweenpartylic #halloweenpartyastoria #bubbleballny #adults #bounce #astoria #lic #astoriaqueens #queens #birthdayparties #newyork #astoriaqueensny #astoriaqueens #queens #fun #kids #funkids #family #astoriasportscomplex #costumeparty #prizes #halloweenpartynyc #kidshalloweenparty #kidshalloweenparty2019 #halloweenpartyny #nychalloweenparty\n",
      "--GENERATED SEQUENCE (WITHOUT WORD ORDER)--\n",
      "birthdayparties funkids babysharkchallenge nerfgun halloweenpartynyc swag knockerballnyc halloweenpartynyc kids lic kidshalloweenparty2018 hocuspocustrivia animalshow magicshownyc costumeparty costumeparty funkids swimlessons funkids bubbleballsoccer bubbleballsoccer bubbleballsoccer halloweenpartyny astoriaqueensny bubbleballsoccer halloweenpartyny november bubblesoccer bubbleballsoccer indoors astoriaqueensny bubbleballnyc kidshalloweenparty2018\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for idx, doc in enumerate(docs[:5]):\n",
    "    print('--ORIGINAL--')\n",
    "    print(doc)\n",
    "    print('--GENERATED SEQUENCE (WITHOUT WORD ORDER)--')\n",
    "    words_count = len(doc.split())\n",
    "    words_ = np.random.choice(words, words_count, p=doc_vecs[idx].todense().tolist()[0])\n",
    "    for i in range(words_count):\n",
    "        word_idx = tfidf.vocabulary_[words_[i]]\n",
    "        words_[i] = np.random.choice(words, 1, p=word_replace_probs[word_idx])[0]\n",
    "    print(' '.join(words_))\n",
    "    print('-'*150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashtags magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = nyc_posts_authors_df.loc[nyc_posts_authors_df['hashtags'].str.len() > 50, 'hashtags'][:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3536335, 10212810)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtags = docs.str.split().tolist()\n",
    "\n",
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=5,\n",
    "                     vector_size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=10)\n",
    "w2v_model.build_vocab(hashtags, progress_per=1000)\n",
    "w2v_model.train(hashtags, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#tagforlikes', 0.747814416885376),\n",
       " ('#wedding', 0.7455034255981445),\n",
       " ('#like', 0.7374163866043091),\n",
       " ('#barmitzvah', 0.7349600791931152),\n",
       " ('#aniversary', 0.7308018803596497),\n",
       " ('#selfie', 0.7256142497062683),\n",
       " ('#foody', 0.7172373533248901),\n",
       " ('#picoftheday', 0.7157788276672363),\n",
       " ('#restaurants', 0.7054566740989685),\n",
       " ('#happy', 0.7051026225090027)]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('#newyork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#birthday', 0.9645171165466309),\n",
       " ('#aniversary', 0.9628538489341736),\n",
       " ('#restaurants', 0.9571098685264587),\n",
       " ('#barmitzvah', 0.954374372959137),\n",
       " ('#tagforlikes', 0.932794451713562),\n",
       " ('#sushi', 0.9251395463943481),\n",
       " ('#foody', 0.921452522277832),\n",
       " ('#like', 0.9046717882156372),\n",
       " ('#selfie', 0.885693371295929),\n",
       " ('#appetizer', 0.8824301958084106)]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('#wedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://blog.paperspace.com/implementing-levenshtein-distance-word-autocomplete-autocorrect/\n",
    "\n",
    "def levenshteinDistanceDP(token1, token2):\n",
    "    distances = numpy.zeros((len(token1) + 1, len(token2) + 1))\n",
    "\n",
    "    for t1 in range(len(token1) + 1):\n",
    "        distances[t1][0] = t1\n",
    "\n",
    "    for t2 in range(len(token2) + 1):\n",
    "        distances[0][t2] = t2\n",
    "        \n",
    "    a = 0\n",
    "    b = 0\n",
    "    c = 0\n",
    "    \n",
    "    for t1 in range(1, len(token1) + 1):\n",
    "        for t2 in range(1, len(token2) + 1):\n",
    "            if (token1[t1-1] == token2[t2-1]):\n",
    "                distances[t1][t2] = distances[t1 - 1][t2 - 1]\n",
    "            else:\n",
    "                a = distances[t1][t2 - 1]\n",
    "                b = distances[t1 - 1][t2]\n",
    "                c = distances[t1 - 1][t2 - 1]\n",
    "                \n",
    "                if (a <= b and a <= c):\n",
    "                    distances[t1][t2] = a + 1\n",
    "                elif (b <= a and b <= c):\n",
    "                    distances[t1][t2] = b + 1\n",
    "                else:\n",
    "                    distances[t1][t2] = c + 1\n",
    "\n",
    "#     printDistances(distances, len(token1), len(token2))\n",
    "    return distances[len(token1)][len(token2)]\n",
    "\n",
    "def kgram_overlap(word1, word2, k):\n",
    "    a = set([word1[i:i+k] for i in range(0, len(word1) - k + 1)])\n",
    "    b = set([word2[i:i+k] for i in range(0, len(word2) - k + 1)])\n",
    "    inter = len(a.intersection(b))\n",
    "    return inter / (len(a) + len(b) - inter + 1)\n",
    "\n",
    "def score(word1, word2):\n",
    "    return w2v_model.wv.similarity(word1, word2) - 0.5 * kgram_overlap(word1, word2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#newyork ['#happy']\n"
     ]
    }
   ],
   "source": [
    "# def get_replace(word, k=5, seed=None):\n",
    "#     repls = [(repl, score(word, repl)) for repl, _ in w2v_model.wv.most_similar(word, topn=k)]\n",
    "#     repls.sort(key=lambda x: x[1], reverse=True)\n",
    "#     rng = np.random.default_rng(seed)\n",
    "#     return rng.choice(repls, p=[score for _, score in repls])[0]\n",
    "\n",
    "# get_replace('#newyork')\n",
    "\n",
    "words = list(w2v_model.wv.key_to_index.keys())\n",
    "\n",
    "def exponential_gen(x, R, u, sensitivity=1, epsilon=25.4, seed=None):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    \n",
    "    # Calculate the score for each element of R\n",
    "    scores = [u(x, r) for r in R]\n",
    "    \n",
    "    # Calculate the probability for each element, based on its score\n",
    "    probabilities = [np.exp(epsilon * score / (2 * sensitivity)) for score in scores]\n",
    "    \n",
    "    # Normalize the probabilties so they sum to 1\n",
    "    probabilities = probabilities / np.linalg.norm(probabilities, ord=1)\n",
    "\n",
    "    # Choose an element from R based on the probabilities\n",
    "    return rng.choice(R, 1, p=probabilities)\n",
    "\n",
    "word = '#newyork'\n",
    "print(word, exponential_gen(word, words, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2242, 2242)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def exponential(x, R, u, sensitivity=1, epsilon=25.4, seed=None):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    # Calculate the score for each element of R\n",
    "    scores = [u(x, r) for r in R]\n",
    "    \n",
    "    # Calculate the probability for each element, based on its score\n",
    "    probabilities = [np.exp(epsilon * score / (2 * sensitivity)) for score in scores]\n",
    "    \n",
    "    # Normalize the probabilties so they sum to 1\n",
    "    probabilities = probabilities / np.linalg.norm(probabilities, ord=1)\n",
    "    \n",
    "    return probabilities\n",
    "\n",
    "word_replace_probs = []\n",
    "\n",
    "for idx, word in enumerate(words):\n",
    "    if idx % 1000 == 0:\n",
    "        print(idx)\n",
    "    word_replace_probs.append(exponential(word, words, score))\n",
    "\n",
    "word_replace_probs = np.array(word_replace_probs)\n",
    "word_replace_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--ORIGINAL--\n",
      "#bike #bikes #bikelife #shopride #shoplife #myfavoritebikeshop #bikeclub #bikenyc #bikeny #croton #crotonaqueduct #gravelgrinder #gravel #gravelride #offroad\n",
      "--GENERATED SEQUENCE (WITHOUT WORD ORDER)--\n",
      "#bike #cycling #movesale #shopride #shoplife #myfavoritebikeshop #bikeclub #bikenyc #bikeny #croton #crotonaqueduct #gravelgrinder #gravel #gravelride #offroad\n",
      "----------------------------------------------------------------------------------------------------\n",
      "--ORIGINAL--\n",
      "#76ers #raptors #blazers #nuggets #labatt #genesse #molson #nba #nbaplayoffs #nba2019 #basketball #hoops #nbafirstround #beerandhoops #brooklynsportsbar\n",
      "--GENERATED SEQUENCE (WITHOUT WORD ORDER)--\n",
      "#76ers #warriors #blazers #nuggets #labatt #genesse #molson #nbaleaguepass #nbaplayoffs #nba2019 #basketball #hoops #nbafirstround #nba2019 #brooklynsportsbar\n",
      "----------------------------------------------------------------------------------------------------\n",
      "--ORIGINAL--\n",
      "#comicartsbrooklyn #prattinstitute #gianthand #art #fun\n",
      "--GENERATED SEQUENCE (WITHOUT WORD ORDER)--\n",
      "#comicartsbrooklyn #prattinstitute #gianthand #art #ff\n",
      "----------------------------------------------------------------------------------------------------\n",
      "--ORIGINAL--\n",
      "#spendinglabordaylaboring #creativelylaboring #lovemyjob #howmisskatiespendsholidays\n",
      "--GENERATED SEQUENCE (WITHOUT WORD ORDER)--\n",
      "#spendinglabordaylaboring #creativelylaboring #lovemyjob #howmisskatiespendsholidays\n",
      "----------------------------------------------------------------------------------------------------\n",
      "--ORIGINAL--\n",
      "#rolex #custom #diamonds #nj #store #ship #unmated #value #textme\n",
      "--GENERATED SEQUENCE (WITHOUT WORD ORDER)--\n",
      "#rolex #quality #unmatched #nj #refrigeradora #rolex #unmated #unmatched #diamonds\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "\n",
    "for idx, doc in enumerate(docs[:5]):\n",
    "    print('--ORIGINAL--')\n",
    "    print(doc)\n",
    "    print('--GENERATED SEQUENCE (WITHOUT WORD ORDER)--')\n",
    "    words_ = doc.split()\n",
    "    for i in range(len(words_)):\n",
    "        if rng.random() < 0.5 and words_[i] in w2v_model.wv:\n",
    "            word_idx = w2v_model.wv.key_to_index[words_[i]]\n",
    "            words_[i] = rng.choice(words, 1, p=word_replace_probs[word_idx])[0]\n",
    "    print(' '.join(words_))\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attacker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/gkhayes/author_attribution CNN AA model\n",
    "\n",
    "https://github.com/yunitata/continuous-n-gram-AA code for CNN AA experiments reproduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = tokenizer.batch_decode(tokenizer(train_posts.text_clean.tolist(), return_tensors=\"pt\", max_length=128, padding='max_length', truncation=True).input_ids, skip_special_tokens=True)\n",
    "text_test = tokenizer.batch_decode(tokenizer(test_posts.text_clean.tolist(), return_tensors=\"pt\", max_length=128, padding='max_length', truncation=True).input_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_n_grams(excerpt_list, n, vocab_size, seq_size):\n",
    "    \"\"\"Create a list of n-gram sequences\n",
    "    \n",
    "    Args:\n",
    "    excerpt_list: list of strings. List of normalized text excerpts.\n",
    "    n: int. Length of n-grams.\n",
    "    vocab_size: int. Size of n-gram vocab (used in one-hot encoding)\n",
    "    seq_size: int. Size of n-gram sequences\n",
    "    \n",
    "    Returns:\n",
    "    n_gram_array: array. Numpy array of one-hot encoded n-grams.\n",
    "    \"\"\"\n",
    "    n_gram_list = []\n",
    "\n",
    "    for excerpt in excerpt_list:\n",
    "        # Remove spaces\n",
    "        excerpt = excerpt.replace(\" \", \"\")\n",
    "\n",
    "        # Extract n-grams\n",
    "        n_grams = [excerpt[i:i + n] for i in range(len(excerpt) - n + 1)]\n",
    "\n",
    "        # Convert to a single string with spaces between n-grams\n",
    "        new_string = \" \".join(n_grams)\n",
    "\n",
    "        # One hot encode\n",
    "        hot = one_hot(new_string, round(vocab_size*1.3))\n",
    "\n",
    "        # Pad hot if necessary\n",
    "        hot_len = len(hot)\n",
    "        if hot_len >= seq_size:\n",
    "            hot = hot[0:seq_size]\n",
    "        else:\n",
    "            diff = seq_size - hot_len\n",
    "            extra = [0]*diff\n",
    "            hot = hot + extra\n",
    "\n",
    "        n_gram_list.append(hot)\n",
    "    \n",
    "    n_gram_array = np.array(n_gram_list)\n",
    "    \n",
    "    return n_gram_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_size(excerpt_list, n, seq_size):\n",
    "    \"\"\"Calculate size of n-gram vocab\n",
    "    \n",
    "    Args:\n",
    "    excerpt_list: list of strings. List of normalized text excerpts.\n",
    "    n: int. Length of n-grams.\n",
    "    seq_size: int. Size of n-gram sequences\n",
    "    \n",
    "    Returns:\n",
    "    vocab_size: int. Size of n-gram vocab.\n",
    "    \"\"\"\n",
    "    n_gram_list = []\n",
    "\n",
    "    for excerpt in excerpt_list:\n",
    "        # Remove spaces\n",
    "        excerpt = excerpt.replace(\" \", \"\")\n",
    "\n",
    "        # Extract n-grams           \n",
    "        n_grams = [excerpt[i:i + n] for i in range(len(excerpt) - n + 1)]\n",
    "\n",
    "        # Create list of n-grams\n",
    "        gram_len = len(n_grams)\n",
    "        if gram_len >= seq_size:\n",
    "            n_grams = n_grams[0:seq_size]\n",
    "        else:\n",
    "            diff = seq_size - gram_len\n",
    "            extra = [0]*diff\n",
    "            n_grams = n_grams + extra\n",
    "        \n",
    "        n_gram_list.append(n_grams)\n",
    "    \n",
    "    # Flatten n-gram list\n",
    "    n_gram_list = list(np.array(n_gram_list).flat)\n",
    "    \n",
    "    # Calculate vocab size\n",
    "    n_gram_cnt = Counter(n_gram_list)\n",
    "    vocab_size = len(n_gram_cnt)\n",
    "    \n",
    "    return vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size for n = 3 is: 42643\n"
     ]
    }
   ],
   "source": [
    "i = 3\n",
    "vocab_size = get_vocab_size(text_train, i, 128)\n",
    "print('Vocab size for n =', i, 'is:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39035, 128)\n",
      "(1000, 128)\n"
     ]
    }
   ],
   "source": [
    "# Create n-gram lists\n",
    "gram3_train = create_n_grams(text_train, 3, vocab_size, 128)\n",
    "gram3_test = create_n_grams(text_test, 3, vocab_size, 128)\n",
    "\n",
    "print(np.shape(gram3_train))\n",
    "print(np.shape(gram3_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum encoding value for 3-grams is:  55434\n"
     ]
    }
   ],
   "source": [
    "max_3gram = np.max(gram3_train)\n",
    "\n",
    "print('Maximum encoding value for 3-grams is: ', max_3gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture in keras\n",
    "# Code reference: https://github.com/gkhayes/author_attribution\n",
    "def define_model(input_len, output_size, vocab_size, embedding_dim, verbose = True,\n",
    "                drop_out_pct = 0.25, conv_filters = 500, activation_fn = 'relu', pool_size = 2, learning = 0.0001):\n",
    "    \"\"\"Define n-gram CNN\n",
    "    \n",
    "    Args:\n",
    "    input_len: int. Length of input sequences.\n",
    "    output_size: int. Number of output classes.\n",
    "    vocab_size: int. Maximum value of n-gram encoding.\n",
    "    embedding_dim: int. Size of embedding layer.\n",
    "    verbose: bool. Whether or not to print model summary.\n",
    "    drop_out_pct: float. Drop-out rate.\n",
    "    conv_filters: int. Number of filters in the conv layer.\n",
    "    activation_fn: string. Activation function to use in the convolutional layer.\n",
    "    pool_size: int. Pool size for the max pooling layer.\n",
    "    learning: float. Learning rate for the model optimizer.\n",
    "    \n",
    "    Returns:\n",
    "    model: keras model object. \n",
    "    \"\"\"\n",
    "    # Channel 1\n",
    "    inputs1 = Input(shape = (input_len,))\n",
    "    embedding1 = Embedding(vocab_size, embedding_dim)(inputs1)\n",
    "    drop1 = Dropout(drop_out_pct)(embedding1)\n",
    "    conv1 = Conv1D(filters = conv_filters, kernel_size = 3, activation = activation_fn)(drop1)\n",
    "    pool1 = MaxPooling1D(pool_size = pool_size)(conv1)\n",
    "    flat1 = Flatten()(pool1)\n",
    "    \n",
    "    # Channel 2\n",
    "    inputs2 = Input(shape = (input_len,))\n",
    "    embedding2 = Embedding(vocab_size, embedding_dim)(inputs2)\n",
    "    drop2 = Dropout(drop_out_pct)(embedding2)\n",
    "    conv2 = Conv1D(filters = conv_filters, kernel_size = 4, activation = activation_fn)(drop2)\n",
    "    pool2 = MaxPooling1D(pool_size = pool_size)(conv2)\n",
    "    flat2 = Flatten()(pool2)\n",
    "\n",
    "    # Channel 3\n",
    "    inputs3 = Input(shape = (input_len,))\n",
    "    embedding3= Embedding(vocab_size, embedding_dim)(inputs3)\n",
    "    drop3 = Dropout(drop_out_pct)(embedding3)\n",
    "    conv3 = Conv1D(filters = conv_filters, kernel_size = 5, activation = activation_fn)(drop3)\n",
    "    pool3 = MaxPooling1D(pool_size = pool_size)(conv3)\n",
    "    flat3 = Flatten()(pool3)\n",
    "    \n",
    "    # Merge channels\n",
    "    merged = Concatenate()([flat1, flat2, flat3])\n",
    "    \n",
    "    # Create output layer\n",
    "    output = Dense(output_size, activation = 'softmax')(merged)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs = [inputs1, inputs2, inputs3], outputs = output)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = Adam(lr = learning), metrics=['accuracy'])\n",
    "    \n",
    "    if verbose:\n",
    "        print(model.summary())\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = preprocessing.LabelEncoder()\n",
    "\n",
    "author_train = lb.fit_transform(train_posts.owner_id.values)\n",
    "author_train_hot = pd.get_dummies(author_train).values\n",
    "author_test = lb.transform(test_posts.owner_id.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 128, 600)     33261000    ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 128, 600)     33261000    ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 128, 600)     33261000    ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 128, 600)     0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 128, 600)     0           ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 128, 600)     0           ['embedding_2[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 126, 500)     900500      ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 125, 500)     1200500     ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 124, 500)     1500500     ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 63, 500)      0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 62, 500)     0           ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 62, 500)     0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 31500)        0           ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 31000)        0           ['max_pooling1d_1[0][0]']        \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 31000)        0           ['max_pooling1d_2[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 93500)        0           ['flatten[0][0]',                \n",
      "                                                                  'flatten_1[0][0]',              \n",
      "                                                                  'flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 50)           4675050     ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 108,059,550\n",
      "Trainable params: 108,059,550\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create the 3-gram model\n",
    "gram3_model = define_model(128, len(train_posts.owner_id.unique()), max_3gram + 1, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gram3_model.fit([gram3_train, gram3_train, gram3_train], author_train_hot, epochs=15, batch_size=32, \n",
    "                verbose = 1, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and evaluate Model 1 (3-gram CNN)\n",
    "\n",
    "# t0 = time.time()\n",
    "\n",
    "# # Fit model\n",
    "# model1 = define_model(128, len(train_posts.authorid.unique()), max_3gram + 1, 300)\n",
    "# model1.fit([gram3_train, gram3_train, gram3_train], author_train_hot, epochs=10, batch_size=32, \n",
    "#            verbose = 1, validation_split = 0.2)\n",
    "t1 = time.time()\n",
    "\n",
    "# Predict values for test set\n",
    "author_pred1 = gram3_model.predict([gram3_test, gram3_test, gram3_test]).argmax(-1)\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "# Evaluate\n",
    "accuracy = balanced_accuracy_score(author_test, author_pred1)\n",
    "precision, recall, f1, support = score(author_test, author_pred1, average='weighted')\n",
    "confusion = confusion_matrix(author_test, author_pred1)\n",
    "    \n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Ave. Precision:\", precision)\n",
    "print(\"Ave. Recall:\", recall)\n",
    "print(\"Ave. F1 Score:\", f1)\n",
    "# print(\"Training Time:\", (t1 - t0), \"seconds\")\n",
    "print(\"Prediction Time:\", (t2 - t1), \"seconds\")\n",
    "print(\"Confusion Matrix:\\n\", confusion)\n",
    "\n",
    "with open('/home/jovyan/notebooks/vk/attacker_vk.txt', 'w') as f:\n",
    "    print(\"Accuracy:\", accuracy, file=f)\n",
    "    print(\"Ave. Precision:\", precision, file=f)\n",
    "    print(\"Ave. Recall:\", recall, file=f)\n",
    "    print(\"Ave. F1 Score:\", f1, file=f)\n",
    "    print(\"Confusion Matrix:\\n\", confusion, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_train = train_texts_anon['text_clean_anon']\n",
    "trans_test = test_texts_anon['text_clean_anon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_vocab_size = get_vocab_size(trans_train, 3, 128)\n",
    "trans_gram3_train = create_n_grams(trans_train, 3, trans_vocab_size, 128)\n",
    "trans_max_3gram = np.max(trans_gram3_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 3-gram model\n",
    "trans_gram3_model = define_model(128, len(train_posts.authorid.unique()), trans_max_3gram + 1, 600)\n",
    "trans_gram3_model.fit([trans_gram3_train, trans_gram3_train, trans_gram3_train], author_train_hot, epochs=15, batch_size=32, \n",
    "                verbose = 1, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_values = author_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = gram3_model.predict([gram3_test, gram3_test, gram3_test]).argmax(-1)\n",
    "\n",
    "trans_vocab_size = get_vocab_size(trans_train, 3, 128)\n",
    "trans_gram3_test = create_n_grams(trans_test, 3, trans_vocab_size, 128)\n",
    "trans_preds = trans_gram3_model.predict([trans_gram3_test, trans_gram3_test, trans_gram3_test]).argmax(-1)\n",
    "\n",
    "vocab_size = get_vocab_size(text_train, 3, 128)\n",
    "orig_trans_gram3_test = create_n_grams(trans_test, 3, vocab_size, 128)\n",
    "orig_trans_preds = gram3_model.predict([trans_gram3_test, trans_gram3_test, trans_gram3_test]).argmax(-1)\n",
    "\n",
    "print('Weighted scores')\n",
    "print('F1 score for original text:', f1_score(true_values, preds, average='weighted'))\n",
    "print('F1 score for transformed text with transformed model:', f1_score(true_values, trans_preds, average='weighted'))\n",
    "print('F1 score for transformed text with orig model:', f1_score(true_values, orig_trans_preds, average='weighted'))\n",
    "print()\n",
    "print('Macro averaged scores')\n",
    "print('F1 score for original text:', f1_score(true_values, preds, average='macro'))\n",
    "print('F1 score for transformed text with transformed model:', f1_score(true_values, trans_preds, average='macro'))\n",
    "print('F1 score for transformed text with orig model:', f1_score(true_values, orig_trans_preds, average='macro'))\n",
    "\n",
    "with open('/home/jovyan/notebooks/vk/attacker_tests_vk.txt', 'w') as f:\n",
    "    print('Weighted scores', file=f)\n",
    "    print('F1 score for original text:', f1_score(true_values, preds, average='weighted'), file=f)\n",
    "    print('F1 score for transformed text with transformed model:', f1_score(true_values, trans_preds, average='weighted'), file=f)\n",
    "    print('F1 score for transformed text with orig model:', f1_score(true_values, orig_trans_preds, average='weighted'), file=f)\n",
    "    print('', file=f)\n",
    "    print('Macro averaged scores', file=f)\n",
    "    print('F1 score for original text:', f1_score(true_values, preds, average='macro'), file=f)\n",
    "    print('F1 score for transformed text with transformed model:', f1_score(true_values, trans_preds, average='macro'), file=f)\n",
    "    print('F1 score for transformed text with orig model:', f1_score(true_values, orig_trans_preds, average='macro'), file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('checkpoints_vk_distilbert/best2.txt', 'r', encoding='utf-8') as f:\n",
    "    best_model_path = f.readlines()[0]\n",
    "data = None\n",
    "model = LitERAE.load_from_checkpoint(checkpoint_path=best_model_path, data=data, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "vk_sentiment_pos_df = pd.read_csv('/home/jovyan/notebooks/twitter/positive.csv', usecols=['tdate', 'tname', 'ttext', 'ttype'])\n",
    "vk_sentiment_pos_df['tdate'] = vk_sentiment_pos_df['tdate'].apply(datetime.fromtimestamp)\n",
    "\n",
    "vk_sentiment_neg_df = pd.read_csv('/home/jovyan/notebooks/twitter/negative.csv', usecols=['tdate', 'tname', 'ttext', 'ttype'])\n",
    "vk_sentiment_neg_df['tdate'] = vk_sentiment_neg_df['tdate'].apply(datetime.fromtimestamp)\n",
    "\n",
    "vk_sentiment_df = pd.concat([vk_sentiment_pos_df, vk_sentiment_neg_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_cond = (vk_sentiment_df['tdate'] >= datetime(2013, 12, 6)) & (vk_sentiment_df['tdate'] < datetime(2013, 12, 7))\n",
    "vk_sentiment_df = vk_sentiment_df[subset_cond]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lb = LabelEncoder()\n",
    "vk_sentiment_df['target'] = lb.fit_transform(vk_sentiment_df['ttype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "vk_sentiment_df['text'] = vk_sentiment_df['ttext'].apply(apply_clean)\n",
    "vk_sentiment_df[['only_text', 'hashtags']] = vk_sentiment_df.apply(get_text_and_hashtags, axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "vk_sentiment_df['anonymized_text'] = predict_batch(model, vk_sentiment_df['only_text'].tolist(), seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentiment, test_sentiment = train_test_split(vk_sentiment_df, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentiment.to_csv('/home/jovyan/notebooks/twitter/train_sentiment.csv')\n",
    "test_sentiment.to_csv('/home/jovyan/notebooks/twitter/test_sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentiment = pd.read_csv('/home/jovyan/notebooks/twitter/train_sentiment.csv')\n",
    "test_sentiment = pd.read_csv('/home/jovyan/notebooks/twitter/test_sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd4d794c25b443ea0ed10110080c2c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/39.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f809d7e8a0a64f81809e757ff138796b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/953 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a134e6d8f434e8d84cedd43d02304ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/872k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2360d6c3d804ba8869373d622c95e84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentiment_model_name = 'nlptown/bert-base-multilingual-uncased-sentiment'\n",
    "\n",
    "num_labels = train_sentiment.target.nunique()\n",
    "\n",
    "sentiment_tokenizer = AutoTokenizer.from_pretrained(sentiment_model_name)\n",
    "\n",
    "def sentiment_model_init(model_name=sentiment_model_name, num_labels=num_labels):\n",
    "    return AutoModelForSequenceClassification.from_pretrained(sentiment_model_name, num_labels=num_labels, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, targets, tokenizer):\n",
    "        self.inputs = tokenizer(texts, return_tensors=\"pt\", max_length=128, padding='max_length', truncation=True)\n",
    "        self.outputs = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.outputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.inputs['input_ids'][idx],\n",
    "            'token_type_ids': self.inputs['token_type_ids'][idx],\n",
    "            'attention_mask': self.inputs['attention_mask'][idx],\n",
    "            'targets': self.outputs[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_BATCH_SIZE = 8\n",
    "\n",
    "class LitSentiment(pl.LightningModule):\n",
    "    def __init__(self, model_init, train, test, learning_rate=1e-4):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # We hardcode dataset specific stuff here.\n",
    "        self.train_dataset = train\n",
    "        self.test_dataset = test\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = model_init()\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        return self.model(**inputs)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = {a: b for a, b in batch.items() if a != 'targets'}\n",
    "        y = batch['targets']\n",
    "        outputs = self(**x)\n",
    "        logits = outputs.logits\n",
    "        loss = self.loss_func(logits, y)\n",
    "        \n",
    "        f1 = f1_score(y.cpu(), logits.cpu().argmax(dim=-1), average='macro')\n",
    "        \n",
    "        self.log(f'train_loss', loss)\n",
    "        self.log(f'avg_train_loss', loss, on_step=False, on_epoch=True)\n",
    "        self.log(f'train_f1', f1)\n",
    "        self.log(f'avg_train_f1', f1, on_step=False, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = {a: b for a, b in batch.items() if a != 'targets'}\n",
    "        y = batch['targets']\n",
    "        outputs = self(**x)\n",
    "        logits = outputs.logits\n",
    "        loss = self.loss_func(logits, y)\n",
    "        \n",
    "        f1 = f1_score(y.cpu(), logits.cpu().argmax(dim=-1), average='macro')\n",
    "        \n",
    "        self.log(f'val_loss', loss)\n",
    "        self.log(f'avg_val_loss', loss, on_step=False, on_epoch=True)\n",
    "        self.log(f'val_f1', f1)\n",
    "        self.log(f'avg_val_f1', f1, on_step=False, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x = {a: b for a, b in batch.items() if a != 'targets'}\n",
    "        y = batch['targets']\n",
    "        outputs = self(**x)\n",
    "        logits = outputs.logits\n",
    "        loss = self.loss_func(logits, y)\n",
    "        \n",
    "        f1 = f1_score(y.cpu(), logits.cpu().argmax(dim=-1), average='macro')\n",
    "        \n",
    "        self.log(f'test_loss', loss)\n",
    "        self.log(f'avg_test_loss', loss, on_step=False, on_epoch=True)\n",
    "        self.log(f'test_f1', f1)\n",
    "        self.log(f'avg_test_f1', f1, on_step=False, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def on_train_batch_end(self, outputs, batch, batch_idx):\n",
    "        metrics = self.trainer.callback_metrics\n",
    "#         logger.info(f'Batch train loss {metrics}')\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        metrics = self.trainer.callback_metrics\n",
    "        print(f'Train loss: {metrics[\"avg_train_loss\"]}')\n",
    "        print(f'Train f1: {metrics[\"avg_train_f1\"]}')\n",
    "\n",
    "    def on_validation_batch_end(self, outputs, batch, batch_idx):\n",
    "        metrics = self.trainer.callback_metrics\n",
    "#         logger.info(f'Batch validation loss {metrics}')\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        metrics = self.trainer.callback_metrics\n",
    "        print(f'Val loss: {metrics[\"avg_val_loss\"]}')\n",
    "        print(f'Val f1: {metrics[\"avg_val_f1\"]}')\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        metrics = self.trainer.callback_metrics\n",
    "        print(f'Test loss: {metrics[\"avg_test_loss\"]}')\n",
    "        print(f'Test f1: {metrics[\"avg_test_f1\"]}')\n",
    "\n",
    "    ####################\n",
    "    # DATA RELATED HOOKS\n",
    "    ####################\n",
    "\n",
    "#     def prepare_data(self):\n",
    "#         self.data = nn.utils.rnn.pad_sequence(self.data)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            train_size = int(0.9 * len(self.train_dataset))\n",
    "            val_size = len(self.train_dataset) - train_size\n",
    "            self.data_train, self.data_val = random_split(self.train_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.data_test = self.test_dataset\n",
    "\n",
    "        self.loss_func = nn.CrossEntropyLoss() # forgot to add ignore_index for BERT\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.data_train, batch_size=BERT_BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.data_val, batch_size=BERT_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.data_test, batch_size=BERT_BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "chechpoint_path_sentiment = \"checkpoints_ru_sentiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlptown/bert-base-multilingual-uncased-sentiment and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([5, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([5]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                          | Params\n",
      "------------------------------------------------------------\n",
      "0 | model     | BertForSequenceClassification | 167 M \n",
      "1 | loss_func | CrossEntropyLoss              | 0     \n",
      "------------------------------------------------------------\n",
      "167 M     Trainable params\n",
      "0         Non-trainable params\n",
      "167 M     Total params\n",
      "669.432   Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8918ab3bcff4d1ea7ad3a09a144b25c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.32891377806663513\n",
      "Val f1: 0.6737083312425782\n",
      "Train loss: 0.35420048236846924\n",
      "Train f1: 0.6251215586409684\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.3041144907474518\n",
      "Val f1: 0.7129141786676035\n",
      "Train loss: 0.28593504428863525\n",
      "Train f1: 0.6917142796868747\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.3173080384731293\n",
      "Val f1: 0.6830140939729983\n",
      "Train loss: 0.20676544308662415\n",
      "Train f1: 0.8051538357044588\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.4246973991394043\n",
      "Val f1: 0.6742362660170882\n",
      "Train loss: 0.13005143404006958\n",
      "Train f1: 0.8785016894633761\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5860643982887268\n",
      "Val f1: 0.6058249208934143\n",
      "Train loss: 0.0751800686120987\n",
      "Train f1: 0.9329830337696376\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SentimentDataset(train_sentiment.text.tolist(), train_sentiment.target.tolist(), sentiment_tokenizer)\n",
    "test_dataset = SentimentDataset(test_sentiment.text.tolist(), test_sentiment.target.tolist(), sentiment_tokenizer)\n",
    "sentiment_pl = LitSentiment(sentiment_model_init, train_dataset, test_dataset, 2e-5)\n",
    "sentiment_pl.train()\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(dirpath=chechpoint_path_sentiment, save_top_k=2, monitor=\"val_loss\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=5,\n",
    "    num_nodes=1,\n",
    "    num_sanity_val_steps=0,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "trainer.fit(sentiment_pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /home/jovyan/notebooks/anonymization/checkpoints_ru_sentiment/epoch=1-step=1972.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /home/jovyan/notebooks/anonymization/checkpoints_ru_sentiment/epoch=1-step=1972.ckpt\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c6af584d5946aaa8e5053d39982321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.30249857902526855\n",
      "Test f1: 0.6790496749428991\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       avg_test_f1          0.6790496749428991\n",
      "      avg_test_loss         0.30249857902526855\n",
      "         test_f1            0.6790496749428991\n",
      "        test_loss           0.30249857902526855\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "sentiment_pl.stage = 'test'\n",
    "sentiment_pl.eval()\n",
    "\n",
    "results = trainer.test(sentiment_pl, ckpt_path='best')\n",
    "with open('/home/jovyan/notebooks/vk/sentiment_original.txt', 'w') as f:\n",
    "    print(results, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlptown/bert-base-multilingual-uncased-sentiment and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([5, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([5]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /home/jovyan/notebooks/anonymization/lightning_logs\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/jovyan/notebooks/anonymization/checkpoints_ru_sentiment exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                          | Params\n",
      "------------------------------------------------------------\n",
      "0 | model     | BertForSequenceClassification | 167 M \n",
      "1 | loss_func | CrossEntropyLoss              | 0     \n",
      "------------------------------------------------------------\n",
      "167 M     Trainable params\n",
      "0         Non-trainable params\n",
      "167 M     Total params\n",
      "669.432   Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5817757f7e346548ce8acecd9806f12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.34638315439224243\n",
      "Val f1: 0.6643685082041251\n",
      "Train loss: 0.3769320845603943\n",
      "Train f1: 0.6378467341674641\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.32824310660362244\n",
      "Val f1: 0.6748858447488589\n",
      "Train loss: 0.35599154233932495\n",
      "Train f1: 0.6356911125751514\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.3394582271575928\n",
      "Val f1: 0.6785789552912845\n",
      "Train loss: 0.31508028507232666\n",
      "Train f1: 0.6521806198537979\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.4136177897453308\n",
      "Val f1: 0.6738311916394112\n",
      "Train loss: 0.24240928888320923\n",
      "Train f1: 0.7623966605443407\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.44836264848709106\n",
      "Val f1: 0.6729723853011527\n",
      "Train loss: 0.14990410208702087\n",
      "Train f1: 0.8664648723517047\n"
     ]
    }
   ],
   "source": [
    "anon_train_dataset = SentimentDataset(train_sentiment.anonymized_text.tolist(), train_sentiment.target.tolist(), sentiment_tokenizer)\n",
    "anon_test_dataset = SentimentDataset(test_sentiment.anonymized_text.tolist(), test_sentiment.target.tolist(), sentiment_tokenizer)\n",
    "sentiment_pl = LitSentiment(sentiment_model_init, anon_train_dataset, anon_test_dataset, 2e-5)\n",
    "sentiment_pl.train()\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(dirpath=chechpoint_path_sentiment, save_top_k=2, monitor=\"val_loss\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=5,\n",
    "    num_nodes=1,\n",
    "    num_sanity_val_steps=0,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "trainer.fit(sentiment_pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /home/jovyan/notebooks/anonymization/checkpoints_ru_sentiment/epoch=1-step=1972-v2.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /home/jovyan/notebooks/anonymization/checkpoints_ru_sentiment/epoch=1-step=1972-v2.ckpt\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e68c4b86dbae4126b9680da9bf20ec18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.3402031660079956\n",
      "Test f1: 0.6381162984859087\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       avg_test_f1          0.6381162984859087\n",
      "      avg_test_loss         0.3402031660079956\n",
      "         test_f1            0.6381162984859087\n",
      "        test_loss           0.3402031660079956\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "sentiment_pl.stage = 'test'\n",
    "sentiment_pl.eval()\n",
    "\n",
    "results = trainer.test(sentiment_pl, ckpt_path='best')\n",
    "with open('/home/jovyan/notebooks/vk/sentiment_anonymized.txt', 'w') as f:\n",
    "    print(results, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
